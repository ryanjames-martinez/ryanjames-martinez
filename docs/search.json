[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Turorial & Projects",
    "section": "",
    "text": "Ordinal Regression Analysis\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMultidimensional Scaling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/MDS/mds.html",
    "href": "projects/MDS/mds.html",
    "title": "Multidimensional Scaling",
    "section": "",
    "text": "Dimension-reduction techniques transform complex, high-dimensional datasets into simpler two or three-dimensional visual representations. These methods ensure that the relative distances between data points are maintained, making it easier to analyze and interpret the spatial relationships and patterns inherent in the data.\nMultidimensional Scaling (MDS) is a dimension-reduction technique designed to project high-dimensional data to two or three dimensions while preserving relative distances between observations. The fundamental aim is to place items so that the distances in the low-dimensional space as closely as possible match the given dissimilarities, usually measured on a qualitative or quantitative scale.\nMDS is a powerful statistical technique for analyzing similarity or dissimilarity data, allowing researchers to visualize the level of similarity of individual cases of a dataset by representing data points in a low-dimensional space. This technique is particularly useful in fields such as psychology, ecology, and market research, where it can reveal underlying structures in complex data sets (Borg & Groenen, 2005; Kruskal & Wish, 1978) and where measuring the perceived differences between data can be pivotal.\nThis statistical technique not only helps uncover hidden relationships in data but also facilitates the creation of insightful visual representations."
  },
  {
    "objectID": "projects/MDS/mds.html#metric-multidimensional-scaling",
    "href": "projects/MDS/mds.html#metric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Metric Multidimensional Scaling",
    "text": "Metric Multidimensional Scaling\nGiven a number of objects with their dissimilarities recorded, metric Multidimensional Scaling (mMDS) aims to arrange these objects as points in a space such that the spatial distances between the points match the given dissimilarities. Each dissimilarity value, denoted by \\(\\delta_{rs}\\) is transformed by a continuous, monotonic function \\(f\\) to determine the distance \\(d_{rs}\\) between points in this space, where \\[d_{rs} = f(\\delta_{rs})\\] This method has its roots in classical scaling, a technique developed in the 1930s by Young and Householder (1938). They demonstrated that starting from a matrix of distances among all points in a Euclidean space, it’s possible to compute coordinates for these points that preserve these distances. The approach gained widespread recognition when Torgerson (1952) popularized it by adapting the method to use dissimilarities instead of actual distances, thus broadening its application."
  },
  {
    "objectID": "projects/MDS/mds.html#principal-coordinate-analysis",
    "href": "projects/MDS/mds.html#principal-coordinate-analysis",
    "title": "Multidimensional Scaling",
    "section": "Principal Coordinate Analysis",
    "text": "Principal Coordinate Analysis\nPrincipal Coordinate Analysis (PCoA), often interchangeable with metric MDS in many contexts, also focuses on preserving the actual distances between pairs of objects in the dimension-reduced space. The approach starts with a distance matrix and applies eigenvalue decomposition to derive the principal coordinates. PCoA can handle any symmetric distance matrix and thus provides a flexible method for dimensionality reduction. Primary distinction to Principal Component Analysis (PCA) is that PCoA uses distance matrix of the items while PCA utilizes covariance matrix of the exploratory variables.\nTo grasp the link between PCoA and PCA, it’s essential to revisit some foundational concepts and introduce new insights from these methods:\n\nPrincipal Components as Linear Combinations:  \nEach principal component is a linear combination of the original variables, represented by the formula: \\[Z_i = a_{i1}X_1 + a_{i2}X_2 + \\dots + a_{ip}X_p\\] This showcases how principal components simplify the data by transforming the original variables into new axes that capture the majority of the data’s variance.\nCovariance and Similarity Matrices:  \nWhen the variables have zero mean, the covariance matrix \\(C\\) is calculated as: \\[\\mathbf{C} = \\frac{\\mathbf{X}^T\\mathbf{X}}{n-1}\\] PCA uses this covariance matrix to derive eigenvalues and eigenvectors that define the principal components. In contrast, PCoA uses a matrix \\(\\mathbf{S} = \\mathbf{XX}^T\\), which represents similarities or dissimilarities, making it suitable for data that doesn’t naturally fit into a covariance analysis framework.\nCalculating and Interpreting Components:  \nThe principal components are obtained through: \\[\\mathbf{Z} = \\mathbf{XA}'\\] where \\(\\mathbf{Z}\\) includes the principal component scores, and \\(\\mathbf{A}'\\) is the matrix of eigenvectors’ transpose. This transformation allows for the original data matrix \\(\\mathbf{X}\\) to be reconstructed from \\(\\mathbf{Z}\\) and \\(\\mathbf{A}\\): \\[\\mathbf{X} = \\mathbf{ZA}\\] Unlike PCA, PCoA often scales components differently, affecting how they’re interpreted compared to PCA."
  },
  {
    "objectID": "projects/MDS/mds.html#nonmetric-multidimensional-scaling",
    "href": "projects/MDS/mds.html#nonmetric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Nonmetric Multidimensional Scaling",
    "text": "Nonmetric Multidimensional Scaling\nThe non-metric Multidimensional Scaling (nMDS) method was initially conceptualized by Shepard in the early 1960s and was further refined by Kruskal shortly thereafter. This approach involves arranging a set of \\(n\\) objects based on their dissimilarities, \\(\\delta_{rs}\\) into a configuration of \\(n\\) points within typically Euclidean space. Each object corresponds to a point such that the spatial layout reflects the dissimilarities as accurately as possible.\nNMDS relaxes the requirement of preserving the exact distances and focuses instead on preserving the rank order of distances. This method is particularly useful when the distances do not adhere strictly to a metric scale or when the exact magnitudes of distances are less important than the order of these distances. The primary aim here is to maintain the ordinal relationship of the distances, such as ensuring that if object A is closer to object B than to object C in high-dimensional space, the same should be true in the reduced space. This involves an iterative process where the configuration is adjusted to minimize a stress function shown in the diagnositc section."
  },
  {
    "objectID": "projects/MDS/mds.html#algorithm",
    "href": "projects/MDS/mds.html#algorithm",
    "title": "Multidimensional Scaling",
    "section": "Algorithm",
    "text": "Algorithm\nThe algorithm for recovering coordinates from distances between pairs of points is as follows:\n\nForm matrix \\(\\mathcal{A} = -\\frac{1}{2} \\delta_{rs}^2\\)\nForm matrix \\(\\mathcal{B} = \\mathcal{HAH}\\), where \\(\\mathcal{H}\\) is the centering matrix \\(\\mathcal{H} = \\mathcal{I}- n^{-1} \\mathbf{1}_n\\mathbf{1}_n^T\\), where \\(\\mathcal{I}\\) is the identity matrix, and \\(\\mathbf{1}_n\\) is a vector of ones.\nFind the spectral decomposition of \\(\\mathcal{B}, \\mathcal{B} = \\mathcal{V} \\Lambda \\mathcal{V}^T\\), where \\(\\Lambda\\) is the diagonal matrix formed from eigenvalues of \\(\\mathcal{B}\\), and \\(\\mathcal{V}\\) is the matrix of corresponding eigenvectors.\nIf the points were originally in a \\(p\\)-dimensional space, the first \\(p\\) eigenvalues of \\(\\mathcal{B}\\) are nonzero and the remaining \\(n-p\\) are zero. Discard these from \\(\\Lambda\\) (rename as \\(\\Lambda_1\\), and discard the corresponding eigenvalues from \\(\\mathcal{V}\\) (rename as \\(\\mathcal{V}_1\\)).\nFind \\(\\mathcal{X} = \\mathcal{V}_1 \\Lambda_1^{1/2},\\) and coordinates of the points are given by the row of \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "projects/MDS/mds.html#standardized-residual-sum-of-squares",
    "href": "projects/MDS/mds.html#standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Standardized Residual Sum of Squares",
    "text": "Standardized Residual Sum of Squares\n\\[STRESS(q) = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^{(q)} - \\hat{d}_{rs}^{(q)}\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}^{(q)}\\right)^2}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}^{(q)}\\) is the distance between object \\(r\\) and object \\(s\\) for \\(q\\) configuration; and\n\\(\\hat{d}_{rs}^{(q)}\\) is the regression of \\(d_{rs}^{(q)}\\) and \\(\\delta_{rs}\\)(data distance).\n\nKruskall (1978) suggests the stress be informally interpreted according to the guidelines:\n\nStress Value Category\n\n\nStress\nGoodness of fit\n\n\n\n\n20%\nPoor\n\n\n10%\nFair\n\n\n5%\nGood\n\n\n2.5%\nExcellent\n\n\n0%\nPerfect"
  },
  {
    "objectID": "projects/MDS/mds.html#squared-standardized-residual-sum-of-squares",
    "href": "projects/MDS/mds.html#squared-standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Squared Standardized Residual Sum of Squares",
    "text": "Squared Standardized Residual Sum of Squares\n\\[SSTRESS = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^2 - \\hat{d}_{rs}^2\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}\\right)^4}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}\\) is the distance between object \\(r\\) and object \\(s\\) for the configuration; and\n\\(\\hat{d}_{rs}\\) is the regression of \\(d_{rs}\\) and \\(\\delta_{rs}\\)(data distance)."
  },
  {
    "objectID": "projects/MDS/mds.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "href": "projects/MDS/mds.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "title": "Multidimensional Scaling",
    "section": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City",
    "text": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City\n\n\n\n\n\nMap of Iligan\n\n\n\n\nThe goal for this data is to visualize and understand the spatial relationships among different barangays based on their geographic distances. We want to see if MDS accurately captures the original distances between each barangay in Iligan City. After performing MDS, we expect that Barangay Buru-un will be shown to be closer to Barangay Ditucalan or Maria Cristina, as opposed to its distance from Rogongon. The table showcases the list of barangays in Iligan City\n\nList of Barangays\n\n\nAbuno\nHinaplanon\nPala-o\nSaray\n\n\nAcmac\nHindang\nPanoroganan\nSuarez\n\n\nBagong Silang\nKabacsanan\nPoblacion\nTambacan\n\n\nBonbonon\nKalilangan\nPuga-an\nTibanga\n\n\nBunawan\nKiwalan\nRogongon\nTipanoy\n\n\nBuru-un\nLanipao\nSan Miguel\nTomas L. Cabili\n\n\nDalipuga\nLuinab\nSan Roque\nUpper Tominobo\n\n\nDel Carmen\nMahayahay\nSanta Elena\nTubod\n\n\nDigkilaan\nMainit\nSanta Filomena\nUbaldo Laya\n\n\nDitucalan\nMandulog\nSantiago\nUpper Hinaplanon\n\n\nDulag\nMaria Cristina\nSanto Rosario\nVilla Verde\n\n\n\n\nData\nThe data for this application includes the coordinates of the barangay halls, which represent the locations of each barangay. The coordinates were collected using web scraping.\n\nbarangays &lt;- read.delim(\"Barangays.txt\", header = FALSE)\ncoordinates &lt;- data.frame()\nfor(i in 1:nrow(barangays)){\n  barangay &lt;- tolower(barangays[i, ])\n  website &lt;- glue(\"https://www.philatlas.com/mindanao/r10/iligan/{barangay}.html\")\n  h &lt;- read_html(website)\n  latitude &lt;- html_elements(h, xpath='.//span[@id=\"latitude\"]') %&gt;% \n    html_text()\n  longitude &lt;- html_elements(h, xpath='.//span[@id=\"longitude\"]') %&gt;% \n    html_text()\n  c &lt;- cbind.data.frame(barangay, \n                        latitude = as.numeric(latitude), \n                        longitude = as.numeric(longitude))\n  coordinates &lt;- rbind.data.frame(coordinates, c)\n}\nknitr::kable(head(coordinates))\n\n\n\n\nHead of Iligan City Barangays Data\n\n\nbarangay\nlatitude\nlongitude\n\n\n\n\nabuno\n8.1846\n124.2571\n\n\nacmac\n8.2740\n124.2649\n\n\nbagong-silang\n8.2418\n124.2520\n\n\nbonbonon\n8.2667\n124.2902\n\n\nbunawan\n8.3023\n124.3028\n\n\nburu-un\n8.1872\n124.1688\n\n\n\n\n\nThe table above shows the first six of the barangays only. Also, the table illustrates the respective latitude and longitude to each of the barangays in Iligan city. Now that we are done the web scraping part, we proceed to the checking of assumptions.\n\n\nChecking Assumptions\n\n# Missing Data\ncoordinates %&gt;% sapply(., function(x) sum(is.na(x)))\n\n barangay  latitude longitude \n        0         0         0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the three variables. Therefore, there is no need to take any further steps to handle any missing data.\n\n# Duplicates\ncoordinates %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 3\n# ℹ 3 variables: barangay &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\nThe output of the code above indicates that the data contains no duplicates. This suggests that the coordinates of the barangays are unique, as expected.\n\n\nCalculating Haversine Distance\nThe Haversine formula calculates the shortest distance between two points on the surface of a sphere, given their longitudes and latitudes. This is particularly useful in geography for finding the distance between two locations on Earth, as it accounts for the Earth’s curvature.\nWhen we provide the coordinates (latitude and longitude) of two barangays, the Haversine formula helps determine the great-circle (or orthodromic) distance between them. This distance represents the shortest path between the two points along the surface of the sphere, rather than a straight line through the Earth’s interior. The formula of Haversine is\n\\[d = 2r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\phi_2 - \\phi_1}{2}\\right) + \\cos(\\phi_1) \\cos(\\phi_2) \\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\\]\nwhere:\n\n\\(\\phi_1, \\phi_2\\) are the latitudes of the two points in radians,\n\\(\\lambda_1, \\lambda_2\\) are the longitudes of the two points in radians,\n\\(r\\) is the radius of the Earth (approximately 6371 kilometers or 3959 miles),\n\\(d\\) is the distance between the two points.\n\nThe Haversine formula can be utilized in the distm() function, which takes latitude and longitude as parameters. Set the fun parameter to distHaversine to apply the formula mentioned above. The output is in matrix form, with the distance metric given in meters.\n\ndistances &lt;- distm(coordinates[, c(\"longitude\", \"latitude\")], \n                   fun = distHaversine)\ndistances &lt;- distances/1000 #convert to km\ncolnames(distances) &lt;- coordinates$barangay\nrownames(distances) &lt;- coordinates$barangay\n\ndistances[1:5, 1:5]\n\n                  abuno    acmac bagong-silang bonbonon   bunawan\nabuno          0.000000 9.988996      6.392220 9.840034 14.036344\nacmac          9.988996 0.000000      3.855926 2.903147  5.230182\nbagong-silang  6.392220 3.855926      0.000000 5.039188  8.756445\nbonbonon       9.840034 2.903147      5.039188 0.000000  4.199009\nbunawan       14.036344 5.230182      8.756445 4.199009  0.000000\n\n\nAbove is the output of the first five rows and five columns of the distance matrix. As expected, the distance of any point to itself is zero. Additionally, the distance from abuno to acmac, which is 9.988996, is the same as from acmac to abuno. This symmetry confirms that the matrix is symmetric.\nNow, we will proceed to find the optimal dimension that best represents our data. This involves analyzing the stress values and visualizing the corresponding plots to determine which dimensionality reduction provides the clearest and most informative representation of the underlying structure of our dataset.\nThe code below defines a function based on the steps outlined in the Procedure section. This function has two parameters: distance, which is the distance matrix, and dimension, which specifies the number of dimensions the user wishes to explore. The function outputs the stress values for each dimension, along with corresponding plots to help determine the most suitable dimension.\n\noptimal_dimension &lt;- function(distance, dimension){\n  stress &lt;- data.frame()\n  for(i in 1:dimension){\n    mds_result &lt;- cmdscale(distance, k = i)\n    \n    # Calculate the distances in the reduced space\n    fitted_distance &lt;- dist(mds_result)\n    fitted_distance &lt;- as.vector(as.matrix(fitted_distance))\n    original_distance &lt;- as.vector(as.matrix(distance))\n    \n    reg_output &lt;- lm(fitted_distance ~ original_distance)\n    predicted_distance &lt;- fitted(reg_output)\n    \n    # Calculating stress using the modified formula\n    stress_value &lt;- sqrt(sum((fitted_distance - predicted_distance)^2) / \n                           sum(fitted_distance^2))\n    s &lt;- cbind.data.frame(dim = i, stress = stress_value)\n    stress &lt;- rbind.data.frame(stress, s)\n  }\n  p &lt;- ggplot(stress, aes(x=dim, y=stress)) +\n    geom_line() + geom_point() + scale_color_brewer(palette=\"Paired\") +\n    labs(x = \"Dimensions\", title = \"Stress Plot\") +\n    theme_minimal()\n  print(p)\n  \n  return(stress)\n}\n\nTo find the optimal dimension for our analysis, we use the elbow method. This method involves looking at a line plot and finding the ‘elbow,’ a point where the benefits of adding more dimensions start to diminish significantly. Identifying this point helps us select the dimensionality that provides the most meaningful reduction in complexity without significant loss of information.\n\noptimal_dimension(distances, 10)\n\n\n\n\nBarangays in Iligan City Stress Plot\n\n\n\n\n   dim       stress\n1    1 2.263671e-01\n2    2 8.481692e-08\n3    3 7.949367e-08\n4    4 7.949368e-08\n5    5 7.949369e-08\n6    6 7.949369e-08\n7    7 7.949369e-08\n8    8 7.949369e-08\n9    9 7.949369e-08\n10  10 7.949369e-08\n\n\nThe stress plot above illustrates how stress diminishes as the number of dimensions increases. While the elbow in the plot might be subtle due to the small stress values, it appears around two dimensions. This suggests that adding more dimensions beyond two does not significantly enhance the data representation. Therefore, based on this plot, using two dimensions is likely adequate for capturing the structure of the data without adding unnecessary complexity or overfitting. The stress value for two dimensions is near \\(0\\%\\), indicating a perfect goodness of fit as suggested by Kruskal. This extremely low stress value demonstrates an excellent representation of the data in the chosen dimensional space. The stress values for dimensions three to ten are also near \\(0\\%\\), but selecting a dimension within that range becomes challenging to visualize effectively. Higher dimensions can complicate the interpretation without providing significant additional clarity.\nHaving identified the optimal dimension that best represents our data, we will now proceed to perform MDS using two dimensions. To carry out metric MDS in R, we utilize the cmdscale() function, providing it with the distance matrix and specifying two as the number of dimensions.\n\nmds_result &lt;- cmdscale(distances, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City (Inverted)\n\n\n\n\nThe figure above presents an inverted version of the barangay locations in Iligan City. To match the exact location as the original map, we simply multiply the coordinates by -1. This sign reversal does not change the distances between the barangays based on the two dimensions, and the new dimension is therefore just as satisfactory as the original one.\n\nmds_df$V1 &lt;- -1 * mds_df$V1\nmds_df$V2 &lt;- -1 * mds_df$V2 \nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City\n\n\n\n\nThe points in Figure 4 now accurately represent the locations of barangays in Iligan City. As stated earlier, our objective was to verify whether Barangay Buru-un is closer to Barangay Maria Cristina or Barangay Ditucalan than to Barangay Rogongon. The MDS has effectively captured this relationship, as well as the distances between other barangays. Using two dimensions, the MDS has successfully mapped the precise distances among the barangays in Iligan City."
  },
  {
    "objectID": "projects/MDS/mds.html#principal-coordinate-analysis-example-sparrow-data",
    "href": "projects/MDS/mds.html#principal-coordinate-analysis-example-sparrow-data",
    "title": "Multidimensional Scaling",
    "section": "2. Principal Coordinate Analysis Example: Sparrow Data",
    "text": "2. Principal Coordinate Analysis Example: Sparrow Data\nAfter a severe storm on February 1, 1898, a number of moribund sparrows were taken to Hermon Bumpus’ biological laboratory at Brown University, Rhode Island. Subsequently, about half of the birds died, and Bumpus saw this as an opportunity to see whether he could find any support for Charles Darwin’s theory of natural selection. To this end, he made eight morphological measurements on each bird and also weighed the birds. The results for five of the measurements are shown below, for females only.\nThe dataset contains five morphological measurements of the female sparrows where\n\ntotal.length - measures from the tip of the beak to the end of the tail in *mm*\nalar.extent - the distance from tip to tip of extended wings\nbeak.and.head - length of beak and head (from the tip of the beak to the occiput)\nhumerus - length of humerus\nsternum - length of sternum\nsurvival - the survival status of the bird, true if alive, or false otherwise\n\nAt the conclusion of our PCoA modeling, we will compare the results with those obtained from PCA. This comparison will help us understand the differences in how each method captures and represents the underlying data structure.\n\nsparrow_data &lt;- read_excel(\"sparrow.xlsx\")\nsparrow &lt;- sparrow_data[, -c(1,7)]\nknitr::kable(head(sparrow_data), caption = \"First six row of female sparrows\")\n\n\n\n\nFirst six row of female sparrows\n\n\n\n\n\n\n\n\n\n\n\nBird\ntotal.length\nalar.extent\nbeak.and.head\nhumerus\nsternum\nsurvival\n\n\n\n\n1\n156\n245\n31.6\n18.5\n20.5\nT\n\n\n2\n154\n240\n30.4\n17.9\n19.6\nT\n\n\n3\n153\n240\n31.0\n18.4\n20.6\nT\n\n\n4\n153\n236\n30.9\n17.7\n20.2\nT\n\n\n5\n155\n243\n31.5\n18.6\n20.3\nT\n\n\n6\n163\n247\n32.0\n19.0\n20.9\nT\n\n\n\n\n\nFor PCoA purpose, we remove the first column and the second column, and this can be achieved by the following code\n\nsparrow &lt;- sparrow_data[, -c(1,7)]\n\nNow, we proceed by checking the assumptions.\n\nChecking Assumptions\n\n# Missing Data\nsparrow %&gt;% sapply(., function(x) sum(is.na(x)))\n\n total.length   alar.extent beak.and.head       humerus       sternum \n            0             0             0             0             0 \n\n\nThe zero value from the result above suggests that each of the independent variables have no missing datas.\n\n# Duplicates\nsparrow %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# A tibble: 0 × 5\n# ℹ 5 variables: total.length &lt;dbl&gt;, alar.extent &lt;dbl&gt;, beak.and.head &lt;dbl&gt;,\n#   humerus &lt;dbl&gt;, sternum &lt;dbl&gt;\n\n\nThe result from the code suggests that no morphological measurements of the female sparrows are the same. Now we are done checking the assumptions, we now proceed to calculating the distance matrix of the data.\n\n\nCalculating Euclidean distance\nSince we are about to model a PCoA example, the distance that is to be utilized is euclidean. This can be performed in R by using the dist() function that feeds the dataset and set the method to euclidean which indicates the usage of Euclidean distance.\n\ndist_matrix &lt;- dist(sparrow, method = \"euclidean\")\ndist_matrix %&gt;% as.matrix() %&gt;% .[1:5,1:5]\n\n         1        2        3        4        5\n1 0.000000 5.622277 5.863446 9.550916 2.249444\n2 5.622277 0.000000 1.615549 4.201190 3.491418\n3 5.863446 1.615549 0.000000 4.081666 3.657868\n4 9.550916 4.201190 4.081666 0.000000 7.360706\n5 2.249444 3.491418 3.657868 7.360706 0.000000\n\n\nNow we are done calculating the distance matrix, and checking the assumptions, the preparations for modeling is now complete. Now, we find the optimal dimension of the dataset.\n\n\nModeling\nWe set the dimension to 5, as a similar process to PCA.\n\noptimal_dimension(dist_matrix, 5)\n\n\n\n\nSparrows Stress Plot\n\n\n\n\n  dim       stress\n1   1 1.437590e-01\n2   2 2.456834e-02\n3   3 1.041855e-02\n4   4 2.913686e-03\n5   5 5.677040e-16\n\n\nFigure 5 illustrates how the stress decreases as the number of dimensions increases from one to five. There’s a noticeable drop in stress when moving from one to two dimensions, indicating that two dimensions capture much more of the data’s structure compared to just one. The stress continues to reduce slightly as we add a third dimension, but the rate of decline slows down significantly.\nFrom the plot, we can see an ‘elbow’ forming between the second dimensions. This suggests that adding more dimensions beyond two brings minimal improvement in stress reduction.\n\nmds_result &lt;- cmdscale(dist_matrix, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(x = V1, y = V2)) +\n  geom_point(aes(color = sparrow_data$survival)) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", \n       title = \"PCoA Plot\", color = \"survival\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Sparrows\n\n\n\n\nFigure 6 shows the PCoA results for a dataset on sparrows, visualized in a two-dimensional space. The plot categorizes sparrows based on a survival attribute, with the categories represented by two different colors: red (‘F’) for those that did not survive and blue (‘T’) for those that did survive.\nThe horizontal axis (Dimension 1) and the vertical axis (Dimension 2) represent the two principal coordinates derived from the dataset. There isn’t a clear, distinct clustering pattern where one group is entirely separate from the other. Both survived (‘T’) and not survived (‘F’) sparrows are spread throughout the plot. This suggests that based solely on these two principal coordinates, there isn’t much differentiation in the dataset that correlates strongly with the survival outcomes.\nWe compare it to PCA with two as the remained principal components,\n\nsparrow_pca &lt;- prcomp(sparrow, scale = TRUE)\nautoplot(sparrow_pca, data = sparrow_data, colour = 'survival', label.size = 3) +\n  labs(title = \"PCA Plot\") +\n  theme_minimal()\n\n\n\n\nPCA plot of Sparrows\n\n\n\n\nFigure 7 shows the PCA plot of sparrows, visualized in a space defined by the first two principal components: PC1 and PC2. This plot also categorizes the sparrows based on their survival status, with red dots (‘F’) representing those that did not survive and blue dots (‘T’) for those that did survive.\nThe first principal component (PC1) explains a significant portion of the variance in the dataset at 72.32%, indicating that it captures a major underlying pattern or trend related to the features measured. PC2 explains an additional 10.63% of the variance, providing a secondary perspective on the data.\nSimilar to the PCoA plot, the PCA plot does not show clear, distinct clusters of survival outcomes. Both survival and non-survival sparrows are scattered throughout the PCA space. Both PCA and PCoA plots show an overlap of survival statuses with no clear separation between the ‘F’ and ‘T’ categories. This suggests that the survival outcome is not strongly linearly correlated with the principal components derived from the data in both analyses.\nPCA, which focuses more on maximizing variance and often reveals more defined patterns and relationships in the data, still shows an overlap similar to PCoA. This suggests that the variables influencing survival might be complex and not easily separable by linear methods like PCA and PCoA.\nWhile PCoA is more about preserving the distance or dissimilarity accurately in a lower-dimensional space, PCA aims to capture the variance in the data. The similar patterns of overlap in both analyses indicate that further investigation might be needed, perhaps with different analytical approaches or additional data features, to better understand the factors affecting survival."
  },
  {
    "objectID": "projects/MDS/mds.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "href": "projects/MDS/mds.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "title": "Multidimensional Scaling",
    "section": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data",
    "text": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data\nIn this nMDS example, the will use the Doubs Fish data, the Fish community composition of the Doubs River in France. They come from Verneaux’s PhD thesis (1973), where he proposed to use fish species to characterize ecological zones along European rivers and streams. The values in ‘Doubs.fish’ are counts of individuals of each of 27 species observed in a set of 30 sites located along the 453 km long Doubs River, France. The data contains the following:\n\nenv - is a data frame with 30 rows (sites) and 11 environmental variables.\nfish - is a data frame with 30 rows (sites) and 27 fish species.\nxy - is a data frame with 30 rows (sites) and 2 spatial coordinates.\nspecies - is a data frame with 27 rows (species) and 4 columns (names)\n\nThe goal here is to visualize the fish abundance at different sites. It can reveal natural groupings (clusters) of sites with similar fish communities This can be crucial for managing fish populations.\nThe data can be access by performing the data() function and feed doubs in it. The site 8 is removed in the data since that site have no species.\n\ndata(doubs)\nspe &lt;- doubs$fish[-8,]\nspe %&gt;% .[1:6, 1:12]\n\n  Cogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi\n1    0    3    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0\n4    0    4    5    5    0    0    0    0    0    1    0    0\n5    0    2    3    2    0    0    0    0    5    2    0    0\n6    0    3    4    5    0    0    0    0    1    2    0    0\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\nab &lt;- table(unlist(spe))\npar(mfrow = c(1, 2))\nbarplot(ab, las = 1, col = grey(5:0/5), main = \"Abundance Distribution\",\n        xlab = \"Abundance class\", ylab = \"Frequency\")\nsite.pre &lt;- rowSums(spe &gt; 0)\nbarplot(site.pre, main = \"Species richness\",\n        xlab = \"Sites\", ylab = \"Number of species\",\n        col = \"grey \", las = 1)\n\n\n\n\nAbundance Distribution and Species Richness\n\n\n\n\nThe plot on the left above reveals that the majority of species observations across various sites register an abundance of zero. This is expected given the large number of sites surveyed—–30 in total—–with many sites not recording the presence of different fish species. There are also sites where the count of fish species present is five, with decreasing frequencies noted for higher abundance classes. This pattern highlights that while some sites exhibit richer biodiversity, many others have little to no fish presence.\nThe ‘Species Richness’ plot on the right side illustrates the number of different species found at each site, showing considerable variability. Some sites have a high species richness, nearing 25 species, while others have much lower counts. This variability might suggest differences in environmental conditions, habitat quality, or other ecological factors influencing species distribution across the sites.\n\nsum(spe == 0)\n\n[1] 408\n\n\n\nsum(spe == 0)/(nrow(spe) * ncol(spe))\n\n[1] 0.5210728\n\n\nOver half of the dataset is made up of zeros, which is quite high but not unusual for species abundance data. The presence of many zeros can cause a ‘double zero problem,’ where the absence of species at different sites can inflates their similarity based on what they both lack, rather than what they have. This means two sites might seem similar because they are both missing certain species, but this doesn’t necessarily reflect true ecological similarity. Ideally, we want the presence of species, rather than their absence, to inform how similar two sites are.\nTo counter this double zero issue, we’ll transform the species data. Pierre Legendre and Gallagher in 2001 recommended five possible pre-transformations for species data, four of which can be performed using the decostand() function from the vegan package.\nOne effective transformation is the Hellinger transformation, which represents species abundances as the square root of their relative abundance at each site, as suggested by Borcard, Gillet, and Legendre (2011). This approach addresses the problem with double zeros effectively. We will use this transformation on our fish abundance dataset to ensure our similarity assessments are more ecologically meaningful. And we will compare this without the transformation to determine if there is changes in the output.\n\nspe.hel &lt;- decostand(spe, method = \"hellinger\")\n\n\n\nChecking Assumptions\n\n# Missing Data\nspe %&gt;% sapply(., function(x) sum(is.na(x)))\n\nCogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi Gogo Eslu Pefl Rham \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \nLegi Scer Cyca Titi Abbr Icme Acce Ruru Blbj Alal Anan \n   0    0    0    0    0    0    0    0    0    0    0 \n\n# Duplicates\nspe %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 27\n# ℹ 27 variables: Cogo &lt;dbl&gt;, Satr &lt;dbl&gt;, Phph &lt;dbl&gt;, Neba &lt;dbl&gt;, Thth &lt;dbl&gt;,\n#   Teso &lt;dbl&gt;, Chna &lt;dbl&gt;, Chto &lt;dbl&gt;, Lele &lt;dbl&gt;, Lece &lt;dbl&gt;, Baba &lt;dbl&gt;,\n#   Spbi &lt;dbl&gt;, Gogo &lt;dbl&gt;, Eslu &lt;dbl&gt;, Pefl &lt;dbl&gt;, Rham &lt;dbl&gt;, Legi &lt;dbl&gt;,\n#   Scer &lt;dbl&gt;, Cyca &lt;dbl&gt;, Titi &lt;dbl&gt;, Abbr &lt;dbl&gt;, Icme &lt;dbl&gt;, Acce &lt;dbl&gt;,\n#   Ruru &lt;dbl&gt;, Blbj &lt;dbl&gt;, Alal &lt;dbl&gt;, Anan &lt;dbl&gt;\n\n\nMDS can be performed in R using the metaMDS() function from the vegan package, which is quite user-friendly. This function can handle both raw data and precomputed distance matrices. We will use the Bray-Curtis dissimilarity for this dataset. The Bray-Curtis dissimilarity is a statistic used to quantify the compositional dissimilarity between two different sites or samples based on counts or measurements of species abundance. It is particularly useful in ecology and environmental science for comparing the composition of different ecological communities.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe, distance=\"bray\")\n\nRun 0 stress 0.0747782 \nRun 1 stress 0.112424 \nRun 2 stress 0.08930115 \nRun 3 stress 0.1104319 \nRun 4 stress 0.08987062 \nRun 5 stress 0.08886178 \nRun 6 stress 0.08886178 \nRun 7 stress 0.1219299 \nRun 8 stress 0.1209556 \nRun 9 stress 0.07376216 \n... New best solution\n... Procrustes: rmse 0.0193953  max resid 0.09464088 \nRun 10 stress 0.07478342 \nRun 11 stress 0.07506678 \nRun 12 stress 0.08797377 \nRun 13 stress 0.1118912 \nRun 14 stress 0.07477838 \nRun 15 stress 0.1123498 \nRun 16 stress 0.08696388 \nRun 17 stress 0.09157401 \nRun 18 stress 0.1125696 \nRun 19 stress 0.1119532 \nRun 20 stress 0.1124389 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     4: no. of iterations &gt;= maxit\n    15: stress ratio &gt; sratmax\n     1: scale factor of the gradient &lt; sfgrmin\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe \nDistance: bray \n\nDimensions: 2 \nStress:     0.07376216 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 9 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe' \n\n\nThe data will be sufficiently explained in two dimensions, which suggests that the resulting plot or ordination will map the data onto two principal axes. The stress value reported is 0.07376216, which is a measure of the fit quality of the NMDS configuration. A stress value below 0.1, is typically considered an excellent fit, basing on the category Kruskall presented.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nNMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe NMDS plot provided has a stress level of 0.074 indicating a reliable fit, visually maps the relationships between different sites, numbered on the plot, and the fish species found there marked in red. The plot shows how some sites cluster together, suggesting they share similar species compositions—likely due to similar environmental conditions or close geographical proximity. Other sites stand out as more isolated, indicating unique ecological characteristics or distinct species that might not be present in more central or clustered locations. Mjority of the fishes are clustered in the site 29, just like what is shown in the descriptive stat eralier. The Teso species solos the site 15, indicating maybe the fish is a territorial or there is an environmental variable there that only the said fish is interested.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot\n\n\n\n\nA Shepard plot is a diagnostic tool used in nMDS to assess the quality of the MDS solution. The Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance \\((R^2 &gt; 0.995)\\) highlighting a high goodness of fit. Both the non-metric and linear fits show high \\(R^2\\) values, suggesting that the MDS model has been very effective at preserving the distances between data points when reducing dimensionality. The non-metric fit is particularly effective, which is typical as non-metric MDS aims to preserve the rank order rather than the actual values of distances, making it more flexible in handling varied data scales and distributions. The effectiveness of the non-metric MDS in this case suggests that if the data contained non-linear relationships or was from different scales of measurement, the method still managed to capture and represent these complexities accurately.\nWe now proceed in performing nMDS for the hellinger transformed of the dataset.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe.hel, distance=\"bray\")\n\nRun 0 stress 0.06746136 \nRun 1 stress 0.1024773 \nRun 2 stress 0.1016353 \nRun 3 stress 0.1024777 \nRun 4 stress 0.1016351 \nRun 5 stress 0.1067558 \nRun 6 stress 0.1067557 \nRun 7 stress 0.102477 \nRun 8 stress 0.09577038 \nRun 9 stress 0.06742454 \n... New best solution\n... Procrustes: rmse 0.008564238  max resid 0.04061037 \nRun 10 stress 0.06747835 \n... Procrustes: rmse 0.01111845  max resid 0.05270437 \nRun 11 stress 0.08323628 \nRun 12 stress 0.1058012 \nRun 13 stress 0.07528549 \nRun 14 stress 0.06743465 \n... Procrustes: rmse 0.005265951  max resid 0.02493877 \nRun 15 stress 0.06742479 \n... Procrustes: rmse 0.00010268  max resid 0.0004864955 \n... Similar to previous best\nRun 16 stress 0.1089367 \nRun 17 stress 0.1016995 \nRun 18 stress 0.06742515 \n... Procrustes: rmse 0.0002102026  max resid 0.0009945225 \n... Similar to previous best\nRun 19 stress 0.0957702 \nRun 20 stress 0.06742439 \n... New best solution\n... Procrustes: rmse 6.295875e-05  max resid 0.0002982595 \n... Similar to previous best\n*** Best solution repeated 1 times\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe.hel, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe.hel \nDistance: bray \n\nDimensions: 2 \nStress:     0.06742439 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 20 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe.hel' \n\n\nThe output above suggests that dimension two is enough to represent the data. This reduction allows for easier visualization and interpretation of the data’s underlying patterns. A stress value of 0.06742439 indicates an excellent fit of the NMDS model to the data, suggesting that the two-dimensional representation is a reliable depiction of the species dissimilarities. The stress value of Hellinger transformed data is lower than that of without transformation, indicating that the transformed model is much better.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nHellinger Transformed of NMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe numbered points represent different sites, and the red text labels specific fish species found at those sites. Similar to the earlier plot, sites that are close together likely have similar species compositions, and the species names near each cluster give insight into the distinct ecological characteristics of those clusters. This plot includes more specific information about the species present at each site compared to the earlier plot. This detail allows for a more in-depth understanding of the ecological and biological diversity across the sampled sites. Both plots have low stress values, but this one is slightly lower (0.067 compared to 0.074), indicating a slightly better fit in the representation of dissimilarities.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot of Hellinger Transformed\n\n\n\n\nThe non-metric fit, with an \\(R^2\\) of 0.995, indicates an excellent adherence to the rank order of the original dissimilarities. This value shows that the nMDS model has successfully captured the underlying structure of the data that preserves the relative dissimilarities among observations. The linear fit has an \\(R^2\\) of 0.981, which is also very high, suggesting that the distances in the MDS space linearly correlate well with the observed dissimilarities. Compared to the Shepard PLot without performing Hellinger transformation, this is better."
  }
]