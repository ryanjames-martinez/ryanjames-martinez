[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan James J. Martinez",
    "section": "",
    "text": "Hello there! I’m Ryan James J. Martinez, a dedicated Statistician currently pursuing an MS degree with a major in Statistics at MSU-IIT. With a robust foundation in statistical analysis and a deep proficiency in the R programming language, I specialize in analyzing, visualizing, and interpreting complex data. My portfolio highlights projects that showcase my statistical expertise. When I’m not on data platforms, I find joy in working out and reading.\n\n\nI am well-versed in different regression techniques, multivariate techniques, feature engineering, non-parametric statistics, time series, etc. I also have the capacity of constructing sophisticated machine-learning models. Skills that have an advanced understanding but are not limited to:\n\nSampling Techniques\nStatistical Analysis\nBayesian Analysis\nText Analysis\nData Visualization\nData Cleaning\nData Modeling\nData Scraping\n\n\n\n\nMindanao State University - Iligan Institute of Technology | Tibanga, IC\n\nBS Statistics | 2019-2023\nMS Statistics | 2023-current\n\n\n \n  \n   \n  \n    \n     facebook\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ryan James J. Martinez",
    "section": "",
    "text": "Mindanao State University - Iligan Institute of Technology | Tibanga, IC\n\nBS Statistics | 2019-2023\nMS Statistics | 2023-current"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ryan James J. Martinez",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Portfolio",
    "section": "",
    "text": "Hello there! I’m Ryan James J. Martinez, a dedicated Statistician currently pursuing an MS degree with a major in Statistics at MSU-IIT. With a robust foundation in statistical analysis and a deep proficiency in the R programming language, I specialize in analyzing, visualizing, and interpreting complex data. My portfolio highlights projects that showcase my statistical expertise. When I’m not on data platforms, I find joy in working out and reading.\nI am well-versed in different regression techniques, multivariate techniques, feature engineering, non-parametric statistics, time series, etc. I also have the capacity of constructing sophisticated machine-learning models. Skills that have an advanced understanding but are not limited to:\n\nStatistical Analysis\nSampling Techniques\nBayesian Analysis\nData Visualization\nData Cleaning\nData Modeling\nData Scraping"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Ryan James J. Martinez",
    "section": "",
    "text": "I am well-versed in different regression techniques, multivariate techniques, feature engineering, non-parametric statistics, time series, etc. I also have the capacity of constructing sophisticated machine-learning models. Skills that have an advanced understanding but are not limited to:\n\nSampling Techniques\nStatistical Analysis\nBayesian Analysis\nText Analysis\nData Visualization\nData Cleaning\nData Modeling\nData Scraping"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Turorial & Projects",
    "section": "",
    "text": "Ordinal Regression Analysis\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMultidimensional Scaling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html",
    "href": "projects/Ordered Logit/ordered_logit.html",
    "title": "Ordinal Regression Analysis",
    "section": "",
    "text": "Introduction\nOrdinal outcomes are common in research studies and real-world applications. These outcomes are not binary and instead fall into multiple ordered categories. It is essential to understand and model such outcomes for decision-making purposes.\nTo analyze and model the relationships between one or more predictor variables and an ordinal response variable, statisticians use ordinal logistic regression. This statistical model is an extension of logistic regression, which can handle the dependent variable with two or more ordered categories. On the other hand, multinomial logistic regression is an extended version of binary logistic regression. However, it does not preserve the ranking information in the dependent variable when returning the information on the contribution of each independent variable.\nThe difference in response scales might require different equations to model them correctly. For example, the ordered scale needs a different function than the unordered scale. In the case of the ordered scale, it is more useful to work with cumulative probabilities (McCullagh and Nelder, 1989). For instance, the cumulative probability of “agreeing” includes the probabilities of “agreeing” and “strongly agreeing” combined. However, this approach only makes sense if the order of categories is clear.\n\n\nOrdinal Logistic Regression\n\nBackground\nPeter McCullagh - A Northern Irish-born American statistician. Distinguished Service Professor in the Department of Statistics at the University of Chicago.\n\nThesis - Analysis of Ordered Categorical Data (1977)\n\n\n\nMotivation of the study\nMcCullagh’s motivation for the ordinal model is to develop statistical methods that even after combining levels of responses, the validity of conclusion will not be affected by the new number of responses. The amalgamation of the response categories in this way will normally reduce the available information, change the estimate, the attained significance level and so on. The important point is that the same parameter is being measured however many categories are combined (McCullagh, 1980)\n\n\nRegression Model\nLet \\(Y\\) denote the ordered response with \\(k\\) categories, with \\(k \\geq 3\\). Then \\(P(Y \\leq j)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(j = 1, ..., k-1\\).\nThe model can be defined as \\[log\\left\\{\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right\\} = logit(P(Y \\leq j)) = \\beta_{j0} - \\beta_{1}X_{1} - \\cdots - \\beta_{p}X_{p}\\] where \\(j = 1,...,k-1\\)\nRemark: If \\(k = 2\\), however ordered nature the dependent variable is, the model is the same as binary logistic regression.\n\n\nAssumptions\nThe assumptions of the Ordinal Logistic Regression are as follow:\n\nThe dependent variable is ordered.\nOne or more of the independent variables are either continuous, categorical or ordinal.\nNo zero cell count.\nNo multicollinearity.\nParallel Regression Lines\n\nThe data of the dependent variable must be ordinal in nature to best explain the outcome of the model.\nOrdinal logistic regression works if the independent variable is only continuous, categorical, or ordinal. If there are several independent variable, the model also works if all of it are only continuous, only categorical, or only ordinal.\nThe frequency of each of the category of the dependent variable must not be zero. Furthermore, it is also suggested if each category has equal count since if an event of greater cell with less count occurs,, the less reliable the chi-square test will be.\nThe regression model also assumes that the effect of the independent variable/s is the same for all categories of the dependent variable. This assumption is called the proportional odds, parallel lines, parallel slopes, or parallel regression assumption (Borooah, 2002; Hardin & Hilbe, 2007; Long, 1997), and this will be referred as the parallel lines assumption.\n\n\n\nModel Evaluation and Diagnostics\nAssessing model fit is the process of checking whether the model fits the data sufficiently well. The methods of assessment are the following:\n\nPseudo R-squared\nIn linear regression, the coefficient of determination, \\(R^2\\), measures the variance in the dependent variable explained by the predictors, with higher \\(R^2\\) values (up to a maximum of 1) indicating a better explanatory fit of the model. For regression models with categorical dependent variables, calculating a direct \\(R^2\\) similar to that in linear models is unfeasible. Instead, approximations of \\(R^2\\) are used to estimate the model’s explanatory power, adapting the concept to fit the context of categorical outcomes (IBM, 2023). For this assessment, there are three values of \\(R^2\\), each of which have different formulas. The following are the three different \\(R^2\\):\n\\[R^2_{\\text{McF}} = 1 - \\frac{\\log(L_M)}{\\log(L_0)}\\] \\[R^2_{\\text{CS}} = 1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}\\] \\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}}{1 - L_0^{2/n}}\\] where \\(L_M\\) is the fitted model, \\(L_0\\) is the null model, \\(n\\) is the sample size.\n\n\\(R^2_{\\text{McF}}\\) is McFadden’s version of pseudo \\(R^2\\), based on the log-likelihood kernels for the intercept-only model and the full estimated model (McFadden, 1974).\nCox and Snell’s \\(R^2_{\\text{CS}}\\) is based on the log likelihood for the model compared to the log-likelihood for a baseline model (Cox and Snell 1989).\nNagelkerke’s \\(R^2_{\\text{Nagelkerke}}\\) is an adjusted version of the Cox and Snell \\(R^2\\) that adjusts the scale of the statistic to cover the full range from 0 to 1 (Nagelkerke, 1991).\n\nThe category of the the values of \\(R^2\\) can be summarized as follows:\n\n0 to 0.2: Weak fit\n0.2 to 0.4: Moderate fit\n0.4 to 0.6: Good fit\n0.6 to 0.8: Very good fit\n0.8 to 1: Excellent fit\n\n\n\nLikelihood Ratio Test\nThe likelihood ratio test compares the likelihoods of these two models to determine whether the additional parameters in the full model significantly improve the fit compared to the reduced model. The null hypothesis of the test is the simpler model (reduced model) is sufficient to explain the data. There is no significant improvement in model fit when adding extra parameters. The alternative hypothesis is the more complex model (full model) provides a significantly better fit to the data than the simpler model. This test is calculated as the ratio of the likelihood of the full model to the likelihood of the reduced model. Mathematically, it is represented as: \\[LR = -2 * (\\text{log likelihood of reduced model} - \\text{log likelihood of full model})\\]\n\n\nLipsitz Test\nThe Lipsitz test checks if your model fits the real data well. In other words, it sees if the predictions your model makes is the same about the actual data. The test compares what your model predicts (expected probabilities) with what actually occurs (observed frequencies). If your model is good, the predictions and actual outcomes should be pretty close. The null hypothesis of this test is that the frequencies of the ordinal response variable are consistent with the expected frequencies predicted by the model (Lipsitz, et al., 1996).\n\n\nAccuracy\nAccuracy is one metric for evaluating classification models. It is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset. Accuracy follows the definition:\n\\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nWhere the number of correct predictions, as the name implies, are the accurate predicted values by the model, and the total number of predictions are the total observations in the validation set.\n\n\nSensitivity and Specificity\nSensitivity is the metric that evaluates a model’s ability to predict true positives of each available category. Specificity is the metric that evaluates a model’s ability to predict true negatives of each available category (Mitrani, A., 2019). The equations below are for calculating sensitivity and specificity:\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\]\nWhere true positives are the number of observations the model predicted were positive that were actually positive. While false negatives are the number of observations the model predicted were positive that were actually negative. Moreover, false negatives, are the number of observations the model predicted were negative that were actually positive. Lastly, true negatives are the number of observations the model predicted were negative that were actually negative.\n\n\nBootstrapping\nBootstrapping is a statistical technique used to estimate the sampling distribution of an estimator by resampling with replacement from the original data. This is often used when the theoretical distribution of an estimator is complex or unknown. This method involves repeatedly drawing samples, typically thousands of times, from the data set and calculating the statistic of interest for each sample. The bootstrap method allows for the estimation of standard errors, confidence intervals, and significance tests, which are critical in many statistical analyses (Efron and Tibshirani, 1994).\nBootstrapping does not rely on the assumptions of normality and can be applied to complex, skewed, or small datasets where other methods might fail or provide biased estimates (Davison and Hinkley, 1997). It offers a straightforward way to derive robust estimates of standard errors and confidence intervals for complex estimators or models without needing explicit formulas.\n\n\n\nImplementation in R\n\nExample 1\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\n\napply (Dependent Variable) - The apply variable is an ordered categorical variable with responses to a survey about whether a student feels they are “Unlikely” (1), “Somewhat likely” (2), or “Very likely” (3) to apply to graduate school.\nparental education status variable or pared - The pared variable is a binary variable indicating if at least one parent has attended graduate school. 1 - at least one parent has a graduate degree; 0 otherwise\npublic - The public variable is a binary variable indicating if the undergraduate institution is public (as opposed to private); 1- public; 0-private\ngpa - gpa variable is the student’s grade point average (1-4)\n\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\n\n\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\npared (Parental Education Status): Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\npublic (Institution Type): Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\ngpa: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\nTransition from “Unlikely” to “Somewhat likely”: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\nTransition from “Somewhat likely” to “Very likely”: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\npared1: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\npublic: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\ngpa: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\n\n\nAssessment of Model Fit\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\n\nConfusion Matrix and Statistics by Class\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\nPrediction\n1\n2\n3\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\n\nRemoving Insignificant Variable\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\n\n\nModeling\n\nComparison of Coefficients and Intercepts\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nValue\nStd. Error\nt value\np value\n\nValue\nStd. Error\nt value\np value\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n1.0457\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n-\n-\n-\n-\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n0.6042\n0.2539\n2.3794\n0.0173\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n2.1763\n0.7671\n2.8370\n0.0046\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n4.2716\n0.7922\n5.3924\n0.0000\n\n\n\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\n\nComparison of Confidence Interval of Log Odds\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nlogit\n2.5 %\n97.5 %\n\nlogit\n2.5 %\n97.5 %\n\n\npared1\n1.0477\n0.5282\n1.5722\n\n1.0457\n0.5265\n1.570\n\n\npublic1\n-0.0588\n-0.6522\n0.5191\n\n-\n-\n-\n\n\ngpa\n0.6159\n0.1076\n1.1309\n\n0.6042\n0.1090\n1.106\n\n\n\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nComparison of Confidence Interval of Odds Ratio\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nOR\n2.5 %\n97.5 %\n\nOR\n2.5 %\n97.5 %\n\n\npared1\n2.8511\n1.6958\n4.8170\n\n2.8454\n1.6931\n4.8065\n\n\npublic1\n0.9429\n0.5209\n1.6806\n\n-\n-\n-\n\n\ngpa\n1.8514\n1.1136\n3.098\n\n1.8299\n1.1152\n3.0223\n\n\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\n\nComparison of Pseudo R-Squared\n\n\n\nPseudo R-Squared\n\n\n\n\n\nOriginal\n\nRemodeled\n\n\nMcFadden\n0.0326231\n\n0.0325706\n\n\nCox and Snell (ML)\n0.0586601\n\n0.0585685\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n0.0694569\n\n\n\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\n\nComparison of Likelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\nOriginal\n-3\n-12.09\n24.18\n2.29e-05\n\n\nRemodeled\n-2\n-12.071\n24.141\n5.7e-06\n\n\n\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nComparison of Lipsitz Goodness of Fit Test\n\n\n\nLR statistic\ndf\np.value\n\n\n\n\nOriginal\n8.5407\n9\n0.4807\n\n\nRemodeled\n8.5904\n9\n0.4759\n\n\n\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\n\nComparison of Confusion Matrix\n\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n\nPrediction\n1\n2\n3\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\nOriginal\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\nRemodeled\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\n\nComparison of Residual Deviance and AIC\n\n\n\nResidual Deviance\nAIC\n\n\n\n\nOriginal\n717.0249\n727.0249\n\n\nRemodeled\n717.0638\n725.0638\n\n\n\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\n\nBootstrap Statistics\n\n\n\noriginal\nbias\nstd. error\n\n\n\n\nt1*\n1.96108366\n-1.243752\n1.737844\n\n\nt2*\n-0.03891634\n-1.243752\n1.737844\n\n\n\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\n\nBootstrap Confidence Interval\n\n\n\nLevel\nBasic\n\n\n\n\n\\(t1*\\)\n95 %\n(1.923, 8.295)\n\n\n\\(t2*\\)\n95 %\n(-0.0766, 6.2952)\n\n\n\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\n\nExample 2\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\n\nsurvived - 1 if yes, 0 if did not survived;\nsex - 1 if male, 0 for female;\nage - in years (and for some children, fractions of a year); and\npassengerClass(Dependent Variable) - class of the passengers, either 1st, 2nd, or 3rd class.\n\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\n\n\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\nsurvivedyes: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\nsexmale: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\nage: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n1st|2nd: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n2nd|3rd: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\nsurvivedyes: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\nsexmale: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\nage: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\n\n\nAssessing of Model Fit\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\n\nConfusion Matrix and Statistics by Class of Titanic Survival Data\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\nPrediction\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\n\n\nRemoving Insignificant Variable\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\n\n\nModeling\n\nComparison of Coefficients and Intercepts\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nValue\nStd. Error\nt value\np value\n\nValue\nStd. Error\nt value\np value\n\n\nsurvivedyes\n-1.751\n0.184\n-9.538\n0\n\n-1.592\n0.149\n-10.683\n0\n\n\nsexmale\n-0.275\n0.181\n-1.517\n0.129\n\n-\n-\n-\n-\n\n\nage\n-0.066\n0.005\n-12.131\n0\n\n-0.066\n0.005\n-12.154\n0\n\n\n1st|2nd\n-4.082\n0.276\n-14.774\n0\n\n-3.848\n0.226\n-17.011\n0\n\n\n2nd|3rd\n-2.688\n0.253\n-10.617\n0\n\n-2.456\n0.198\n-12.377\n0\n\n\n\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\n\nComparison of Confidence Interval of Logit\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nlogit\n2.5 %\n97.5 %\n\nlogit\n2.5 %\n97.5 %\n\n\nsurvivedyes\n-1.7506\n-2.11550\n-1.3953\n\n-1.5917\n-1.8866\n-1.3022\n\n\nsexmale\n-0.2745\n-0.6328\n0.0772\n\n-\n-\n-\n\n\nage\n-0.0657\n-0.0765\n-0.0553\n\n-0.0659\n-0.0767\n-0.0554\n\n\n\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nComparison of Confidence Interval of Odds Ratio\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nOR\n2.5 %\n97.5 %\n\nOR\n2.5 %\n97.5 %\n\n\nsurvivedyes\n0.1737\n0.1206\n0.2478\n\n0.2036\n0.1516\n0.2719\n\n\nsexmale\n0.7599\n0.5311\n1.0802\n\n-\n-\n-\n\n\nage\n0.9364\n0.9263\n0.9462\n\n0.9362\n0.9262\n0.9461\n\n\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\n\nComparison of Pseudo R-squared\n\n\n\nPseudo R-Squared\n\n\n\n\n\nOriginal\n\nRemodeled\n\n\nMcFadden\n0.149588\n\n0.148262\n\n\nCox and Snell (ML)\n0.270207\n\n0.268165\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n0.305342\n\n\n\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\n\nComparison on LRT\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\nOriginal\n-3\n-131.51\n263.02\n0\n\n\nRemodeled\n-2\n-130.34\n260.69\n0\n\n\n\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nComparison on Lipsitz Test\n\n\n\nLR Statistic\ndf\np value\n\n\n\n\nOriginal\n16.42\n9\n0.05861\n\n\nRemodeled\n13.064\n9\n0.1597\n\n\n\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\n\nComparison of Confusion Matrix\n\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n\nPrediction\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\nOriginal\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\n1st\n42\n0\n15\n\n\n\n\n\n\n\n\n\nRemodeled\n2nd\n16\n0\n37\n0.59\n0.58\nNA\n0.62\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\n\nComparison on Residual Deviance and AIC\n\n\n\nResidual Deviance\nAIC\n\n\n\n\nOriginal\n1495.276\n1505.276\n\n\nRemodeled\n1497.609\n1505.609\n\n\n\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\n\nBootstrap Statistics\n\n\n\noriginal\nbias\nstd. error\n\n\n\n\nt1*\n-0.3329053\n-1.09505\n3.537359\n\n\nt2*\n-2.3329053\n-1.09505\n3.537359\n\n\n\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\n\nBootstrap Confidence Interval\n\n\n\nLevel\nBasic\n\n\n\n\n\\(t1*\\)\n95 %\n(-2.6513, 10.0923)\n\n\n\\(t2*\\)\n95 %\n(-4.651, 8.092)\n\n\n\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\n\n\nConclusion and Recommendation\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\nThe additional of variables that relates well to the dependent variable that could affect the decision-making process for potential graduate students. Examples would be psychological factors or financial considerations. For historical datasets like the Titanic, extending the analysis to include crew data and comparing it with other maritime disasters could provide broader insights.\nIt is recommended to refine data handling and model fitting techniques, such as addressing any imbalance in class distributions within datasets or employing more sophisticated methods for handling missing data and outliers to improve model accuracy and reliability.\n\n\n\nReferences\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press."
  },
  {
    "objectID": "projects/Ordered Logit/Written Report.html",
    "href": "projects/Ordered Logit/Written Report.html",
    "title": "\n",
    "section": "",
    "text": "Ordinal outcomes are common in research studies and real-world applications. These outcomes are not binary and instead fall into multiple ordered categories. It is essential to understand and model such outcomes for decision-making purposes.\nTo analyze and model the relationships between one or more predictor variables and an ordinal response variable, statisticians use ordinal logistic regression. This statistical model is an extension of logistic regression, which can handle the dependent variable with two or more ordered categories. On the other hand, multinomial logistic regression is an extended version of binary logistic regression. However, it does not preserve the ranking information in the dependent variable when returning the information on the contribution of each independent variable.\nThe difference in response scales might require different equations to model them correctly. For example, the ordered scale needs a different function than the unordered scale. In the case of the ordered scale, it is more useful to work with cumulative probabilities (McCullagh and Nelder, 1989). For instance, the cumulative probability of “agreeing” includes the probabilities of “agreeing” and “strongly agreeing” combined. However, this approach only makes sense if the order of categories is clear.\n\n- A Northern Irish-born American statistician. Distinguished Service Professor in the Department of Statistics at the University of Chicago.\nMotivation of the study\nMcCullagh’s motivation for the ordinal model is to develop statistical methods that even after combining levels of responses, the validity of conclusion will not be affected by the new number of responses. The amalgamation of the response categories in this way will normally reduce the available information, change the estimate, the attained significance level and so on. The important point is that the same parameter is being measured however many categories are combined (McCullagh, 1980)\nLet \\(Y\\) denote the ordered response with \\(k\\) categories, with \\(k \\geq 3\\). Then \\(P(Y \\leq j)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(j = 1, ..., k-1\\).\nThe model can be defined as \\[log\\left\\{\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right\\} = logit(P(Y \\leq j)) = \\beta_{j0} - \\beta_{1}X_{1} - \\cdots - \\beta_{p}X_{p}\\] where \\(j = 1,...,k-1\\)\n: If \\(k = 2\\), however ordered nature the dependent variable is, the model is the same as binary logistic regression.\nThe assumptions of the Ordinal Logistic Regression are as follow:\n\nThe dependent variable is ordered.\nOne or more of the independent variables are either continuous, categorical or ordinal.\nNo zero cell count.\nNo multicollinearity.\nParallel Regression Lines\n\nThe data of the dependent variable must be ordinal in nature to best explain the outcome of the model.\nOrdinal logistic regression works if the independent variable is only continuous, categorical, or ordinal. If there are several independent variable, the model also works if all of it are only continuous, only categorical, or only ordinal.\nThe frequency of each of the category of the dependent variable must not be zero. Furthermore, it is also suggested if each category has equal count since if an event of greater cell with less count occurs,, the less reliable the chi-square test will be.\nThe regression model also assumes that the effect of the independent variable/s is the same for all categories of the dependent variable. This assumption is called the proportional odds, parallel lines, parallel slopes, or parallel regression assumption (Borooah, 2002; Hardin & Hilbe, 2007; Long, 1997), and this will be referred as the parallel lines assumption.\n\nAssessing model fit is the process of checking whether the model fits the data sufficiently well. The methods of assessment are the following:\nIn linear regression, the coefficient of determination, \\(R^2\\), measures the variance in the dependent variable explained by the predictors, with higher \\(R^2\\) values (up to a maximum of 1) indicating a better explanatory fit of the model. For regression models with categorical dependent variables, calculating a direct \\(R^2\\) similar to that in linear models is unfeasible. Instead, approximations of \\(R^2\\) are used to estimate the model’s explanatory power, adapting the concept to fit the context of categorical outcomes (IBM, 2023). For this assessment, there are three values of \\(R^2\\), each of which have different formulas. The following are the three different \\(R^2\\):\n\\[R^2_{\\text{McF}} = 1 - \\frac{\\log(L_M)}{\\log(L_0)}\\] \\[R^2_{\\text{CS}} = 1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}\\] \\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}}{1 - L_0^{2/n}}\\] where \\(L_M\\) is the fitted model, \\(L_0\\) is the null model, \\(n\\) is the sample size.\n\n\\(R^2_{\\text{McF}}\\) is McFadden’s version of pseudo \\(R^2\\), based on the log-likelihood kernels for the intercept-only model and the full estimated model (McFadden, 1974).\nCox and Snell’s \\(R^2_{\\text{CS}}\\) is based on the log likelihood for the model compared to the log-likelihood for a baseline model (Cox and Snell 1989).\nNagelkerke’s \\(R^2_{\\text{Nagelkerke}}\\) is an adjusted version of the Cox and Snell \\(R^2\\) that adjusts the scale of the statistic to cover the full range from 0 to 1 (Nagelkerke, 1991).\n\nThe category of the the values of \\(R^2\\) can be summarized as follows:\nThe likelihood ratio test compares the likelihoods of these two models to determine whether the additional parameters in the full model significantly improve the fit compared to the reduced model. The null hypothesis of the test is the simpler model (reduced model) is sufficient to explain the data. There is no significant improvement in model fit when adding extra parameters. The alternative hypothesis is the more complex model (full model) provides a significantly better fit to the data than the simpler model. This test is calculated as the ratio of the likelihood of the full model to the likelihood of the reduced model. Mathematically, it is represented as: \\[LR = -2 * (\\text{log likelihood of reduced model} - \\text{log likelihood of full model})\\]\nThe Lipsitz test checks if your model fits the real data well. In other words, it sees if the predictions your model makes is the same about the actual data. The test compares what your model predicts (expected probabilities) with what actually occurs (observed frequencies). If your model is good, the predictions and actual outcomes should be pretty close. The null hypothesis of this test is that the frequencies of the ordinal response variable are consistent with the expected frequencies predicted by the model (Lipsitz, et al., 1996).\nAccuracy is one metric for evaluating classification models. It is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset. Accuracy follows the definition:\n\\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nWhere the number of correct predictions, as the name implies, are the accurate predicted values by the model, and the total number of predictions are the total observations in the validation set.\nSensitivity is the metric that evaluates a model’s ability to predict true positives of each available category. Specificity is the metric that evaluates a model’s ability to predict true negatives of each available category (Mitrani, A., 2019). The equations below are for calculating sensitivity and specificity:\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\]\nWhere true positives are the number of observations the model predicted were positive that were actually positive. While false negatives are the number of observations the model predicted were positive that were actually negative. Moreover, false negatives, are the number of observations the model predicted were negative that were actually positive. Lastly, true negatives are the number of observations the model predicted were negative that were actually negative.\nBootstrapping is a statistical technique used to estimate the sampling distribution of an estimator by resampling with replacement from the original data. This is often used when the theoretical distribution of an estimator is complex or unknown. This method involves repeatedly drawing samples, typically thousands of times, from the data set and calculating the statistic of interest for each sample. The bootstrap method allows for the estimation of standard errors, confidence intervals, and significance tests, which are critical in many statistical analyses (Efron and Tibshirani, 1994).\nBootstrapping does not rely on the assumptions of normality and can be applied to complex, skewed, or small datasets where other methods might fail or provide biased estimates (Davison and Hinkley, 1997). It offers a straightforward way to derive robust estimates of standard errors and confidence intervals for complex estimators or models without needing explicit formulas.\n\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\n: Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\n: Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\n: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\n: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\n: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\n: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\nModeling\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\n\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\n: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\n: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\n: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\n: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\n: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\n: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\n\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\nModeling\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\n\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press.\n\n\nHelper Packages\n\nlibrary(foreign)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(brant)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(rcompanion)\nlibrary(ggcorrplot)\nlibrary(ordinal)\nlibrary(generalhoslem)\nlibrary(caret)\nlibrary(effects)\nlibrary(rsample)\nlibrary(boot)\n\nExample 1\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\n\nModeling\n\nfit &lt;- polr(apply ~ pared + gpa, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + gpa, data = training_data, Hess = TRUE)\n\nCoefficients:\n        Value Std. Error t value\npared1 1.0457     0.2656   3.937\ngpa    0.6042     0.2539   2.379\n\nIntercepts:\n    Value  Std. Error t value\n1|2 2.1763 0.7671     2.8370 \n2|3 4.2716 0.7922     5.3924 \n\nResidual Deviance: 717.0638 \nAIC: 725.0638 \n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nround(coefs, 4)\n\n        Value Std. Error t value p value\npared1 1.0457     0.2656  3.9365  0.0001\ngpa    0.6042     0.2539  2.3794  0.0173\n1|2    2.1763     0.7671  2.8370  0.0046\n2|3    4.2716     0.7922  5.3924  0.0000\n\n\nConfience Interval\n\nci &lt;- confint(fit, level = 0.95)\ncbind(logit = coef(fit), ci)\n\n           logit     2.5 %   97.5 %\npared1 1.0457078 0.5265351 1.569960\ngpa    0.6042468 0.1090159 1.106025\n\n\n\nexp(cbind(OR = coef(fit), ci))\n\n             OR    2.5 %   97.5 %\npared1 2.845412 1.693056 4.806454\ngpa    1.829873 1.115180 3.022320\n\n\nAssessment of Model Fit\n\nmodel &lt;- clm(apply ~ pared + gpa, data = training_data)\n\nPseudo R-squared\n\nnagelkerke(model)$Pseudo.R.squared.for.model.vs.null\n\n                             Pseudo.R.squared\nMcFadden                            0.0325706\nCox and Snell (ML)                  0.0585685\nNagelkerke (Cragg and Uhler)        0.0694569\n\n\nLikelihood Ratio Test\n\nnagelkerke(model)$Likelihood.ratio.test\n\n Df.diff LogLik.diff  Chisq    p.value\n      -2     -12.071 24.141 5.7245e-06\n\n\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + gpa\nLR statistic = 8.5904, df = 9, p-value = 0.4759\n\n\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2  3\n         1 48  4  0\n         2 28 11  0\n         3  7  2  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.59            \n                 95% CI : (0.4871, 0.6874)\n    No Information Rate : 0.83            \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.1834          \n                                          \n Mcnemar's Test P-Value : 5.887e-06       \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            0.5783   0.6471       NA\nSpecificity            0.7647   0.6627     0.91\nPos Pred Value         0.9231   0.2821       NA\nNeg Pred Value         0.2708   0.9016       NA\nPrevalence             0.8300   0.1700     0.00\nDetection Rate         0.4800   0.1100     0.00\nDetection Prevalence   0.5200   0.3900     0.09\nBalanced Accuracy      0.6715   0.6549       NA\n\n\nBootstrapping\n\nboot_stats &lt;- function(data, indices) {\n  boot_data &lt;- data[indices, ]\n  \n  original_model &lt;- polr(apply ~ pared + public + gpa, \n                         data = boot_data, Hess = TRUE)\n  remodeled_model &lt;- polr(apply ~ pared + gpa, \n                          data = boot_data, Hess = TRUE)\n  \n  # Calculate differences in AIC and Residual Deviance\n  aic_diff &lt;- AIC(original_model) - AIC(remodeled_model)\n  deviance_diff &lt;- original_model$deviance - remodeled_model$deviance\n  \n  return(c(aic_diff, deviance_diff))\n}\nn_bootstraps &lt;- 1000  \nset.seed(2024)\nresults &lt;- boot(data = training_data, statistic = boot_stats, R = 1000)\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = training_data, statistic = boot_stats, R = 1000)\n\n\nBootstrap Statistics :\n       original    bias    std. error\nt1*  1.96108366 -1.243752    1.737844\nt2* -0.03891634 -1.243752    1.737844\n\n# AIC CI differences\nboot.ci(results, type = c(\"basic\", \"perc\"), index = 1) \n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = c(\"basic\", \"perc\"), index = 1)\n\nIntervals : \nLevel      Basic              Percentile     \n95%   ( 1.923,  8.295 )   (-4.373,  1.999 )  \nCalculations and Intervals on Original Scale\n\n# Resid Dev CI diff\nboot.ci(results, type = c(\"basic\", \"perc\"), index = 2)  \n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = c(\"basic\", \"perc\"), index = 2)\n\nIntervals : \nLevel      Basic              Percentile     \n95%   (-0.0766,  6.2952 )   (-6.3731, -0.0012 )  \nCalculations and Intervals on Original Scale\n\n\nExample 2\n\ndata2 &lt;- TitanicSurvival\ndata2 &lt;- na.omit(data2)\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.59174    0.14900  -10.68\nage         -0.06588    0.00542  -12.15\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -3.8482   0.2262   -17.0112\n2nd|3rd  -2.4557   0.1984   -12.3769\n\nResidual Deviance: 1497.609 \nAIC: 1505.609 \n\n\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p)\nround(coefs, 4)\n\n              Value Std. Error  t value p value\nsurvivedyes -1.5917     0.1490 -10.6825       0\nage         -0.0659     0.0054 -12.1544       0\n1st|2nd     -3.8482     0.2262 -17.0112       0\n2nd|3rd     -2.4557     0.1984 -12.3769       0\n\n\nConfience Interval\n\nci &lt;- confint(fit, level = 0.95)\ncbind(logit = coef(fit), ci)\n\n                  logit       2.5 %      97.5 %\nsurvivedyes -1.59173671 -1.88659681 -1.30221789\nage         -0.06587928 -0.07668334 -0.05542251\n\n\n\nexp(cbind(OR = coef(fit), ci))\n\n                   OR     2.5 %    97.5 %\nsurvivedyes 0.2035718 0.1515868 0.2719280\nage         0.9362439 0.9261831 0.9460853\n\n\nAssessment of Model fit\n\nmodel &lt;- clm(passengerClass ~ survived + age, data = training_data)\n\nPseudo R-squared\n\nnagelkerke(model)$Pseudo.R.squared.for.model.vs.null\n\n                             Pseudo.R.squared\nMcFadden                             0.148262\nCox and Snell (ML)                   0.268165\nNagelkerke (Cragg and Uhler)         0.305342\n\n\nLikelihood Ratio Test\n\nnagelkerke(model)$Likelihood.ratio.test\n\n Df.diff LogLik.diff  Chisq    p.value\n      -2     -130.34 260.69 2.4683e-57\n\n\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + age\nLR statistic = 13.064, df = 9, p-value = 0.1597\n\n\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 1st 2nd 3rd\n       1st  42   0  15\n       2nd  16   0  37\n       3rd  15   0  86\n\nOverall Statistics\n                                         \n               Accuracy : 0.6066         \n                 95% CI : (0.5372, 0.673)\n    No Information Rate : 0.654          \n    P-Value [Acc &gt; NIR] : 0.9346         \n                                         \n                  Kappa : 0.3372         \n                                         \n Mcnemar's Test P-Value : 1.833e-11      \n\nStatistics by Class:\n\n                     Class: 1st Class: 2nd Class: 3rd\nSensitivity              0.5753         NA     0.6232\nSpecificity              0.8913     0.7488     0.7945\nPos Pred Value           0.7368         NA     0.8515\nNeg Pred Value           0.7987         NA     0.5273\nPrevalence               0.3460     0.0000     0.6540\nDetection Rate           0.1991     0.0000     0.4076\nDetection Prevalence     0.2701     0.2512     0.4787\nBalanced Accuracy        0.7333         NA     0.7089\n\n\nBootstrapping\n\nboot_stats &lt;- function(data, indices) {\n  boot_data &lt;- data[indices, ]\n  \n  original_model &lt;- polr(passengerClass ~ survived + sex + age, \n                         data = boot_data, Hess=TRUE)\n  remodeled_model &lt;- polr(passengerClass ~ survived + age, \n                          data = boot_data, Hess=TRUE)\n  \n  # Calculate differences in AIC and Residual Deviance\n  aic_diff &lt;- AIC(original_model) - AIC(remodeled_model)\n  deviance_diff &lt;- original_model$deviance - remodeled_model$deviance\n  \n  return(c(aic_diff, deviance_diff))\n}\nn_bootstraps &lt;- 1000  \nset.seed(2024)\nresults &lt;- boot(data = training_data, statistic = boot_stats, R = 1000)\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = training_data, statistic = boot_stats, R = 1000)\n\n\nBootstrap Statistics :\n      original   bias    std. error\nt1* -0.3329053 -1.09505    3.537359\nt2* -2.3329053 -1.09505    3.537359\n\nboot.ci(results, type = c(\"basic\", \"perc\"), index = 1)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = c(\"basic\", \"perc\"), index = 1)\n\nIntervals : \nLevel      Basic              Percentile     \n95%   ( -2.6513,  10.0923 )   (-10.7581,   1.9855 )  \nCalculations and Intervals on Original Scale\n\nboot.ci(results, type = c(\"basic\", \"perc\"), index = 2)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = c(\"basic\", \"perc\"), index = 2)\n\nIntervals : \nLevel      Basic              Percentile     \n95%   ( -4.651,   8.092 )   (-12.758,  -0.014 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#ordinal-logistic-regression",
    "href": "projects/Ordered Logit/ordered_logit.html#ordinal-logistic-regression",
    "title": "Ordinal Regression Analysis",
    "section": "",
    "text": "- A Northern Irish-born American statistician. Distinguished Service Professor in the Department of Statistics at the University of Chicago.\n\nThesis - Analysis of Ordered Categorical Data (1977)\n\nMotivation of the study\nMcCullagh’s motivation for the ordinal model is to develop statistical methods that even after combining levels of responses, the validity of conclusion will not be affected by the new number of responses. The amalgamation of the response categories in this way will normally reduce the available information, change the estimate, the attained significance level and so on. The important point is that the same parameter is being measured however many categories are combined (McCullagh, 1980)\nLet \\(Y\\) denote the ordered response with \\(k\\) categories, with \\(k \\geq 3\\). Then \\(P(Y \\leq j)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(j = 1, ..., k-1\\).\nThe model can be defined as \\[log\\left\\{\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right\\} = logit(P(Y \\leq j)) = \\beta_{j0} - \\beta_{1}X_{1} - \\cdots - \\beta_{p}X_{p}\\] where \\(j = 1,...,k-1\\)\n: If \\(k = 2\\), however ordered nature the dependent variable is, the model is the same as binary logistic regression.\nThe assumptions of the Ordinal Logistic Regression are as follow:\n\nThe dependent variable is ordered.\nOne or more of the independent variables are either continuous, categorical or ordinal.\nNo zero cell count.\nNo multicollinearity.\nParallel Regression Lines\n\nThe data of the dependent variable must be ordinal in nature to best explain the outcome of the model.\nOrdinal logistic regression works if the independent variable is only continuous, categorical, or ordinal. If there are several independent variable, the model also works if all of it are only continuous, only categorical, or only ordinal.\nThe frequency of each of the category of the dependent variable must not be zero. Furthermore, it is also suggested if each category has equal count since if an event of greater cell with less count occurs,, the less reliable the chi-square test will be.\nThe regression model also assumes that the effect of the independent variable/s is the same for all categories of the dependent variable. This assumption is called the proportional odds, parallel lines, parallel slopes, or parallel regression assumption (Borooah, 2002; Hardin & Hilbe, 2007; Long, 1997), and this will be referred as the parallel lines assumption.\n\nAssessing model fit is the process of checking whether the model fits the data sufficiently well. The methods of assessment are the following:\nIn linear regression, the coefficient of determination, \\(R^2\\), measures the variance in the dependent variable explained by the predictors, with higher \\(R^2\\) values (up to a maximum of 1) indicating a better explanatory fit of the model. For regression models with categorical dependent variables, calculating a direct \\(R^2\\) similar to that in linear models is unfeasible. Instead, approximations of \\(R^2\\) are used to estimate the model’s explanatory power, adapting the concept to fit the context of categorical outcomes (IBM, 2023). For this assessment, there are three values of \\(R^2\\), each of which have different formulas. The following are the three different \\(R^2\\):\n\\[R^2_{\\text{McF}} = 1 - \\frac{\\log(L_M)}{\\log(L_0)}\\] \\[R^2_{\\text{CS}} = 1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}\\] \\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}}{1 - L_0^{2/n}}\\] where \\(L_M\\) is the fitted model, \\(L_0\\) is the null model, \\(n\\) is the sample size.\n\n\\(R^2_{\\text{McF}}\\) is McFadden’s version of pseudo \\(R^2\\), based on the log-likelihood kernels for the intercept-only model and the full estimated model (McFadden, 1974).\nCox and Snell’s \\(R^2_{\\text{CS}}\\) is based on the log likelihood for the model compared to the log-likelihood for a baseline model (Cox and Snell 1989).\nNagelkerke’s \\(R^2_{\\text{Nagelkerke}}\\) is an adjusted version of the Cox and Snell \\(R^2\\) that adjusts the scale of the statistic to cover the full range from 0 to 1 (Nagelkerke, 1991).\n\nThe category of the the values of \\(R^2\\) can be summarized as follows:\nThe likelihood ratio test compares the likelihoods of these two models to determine whether the additional parameters in the full model significantly improve the fit compared to the reduced model. The null hypothesis of the test is the simpler model (reduced model) is sufficient to explain the data. There is no significant improvement in model fit when adding extra parameters. The alternative hypothesis is the more complex model (full model) provides a significantly better fit to the data than the simpler model. This test is calculated as the ratio of the likelihood of the full model to the likelihood of the reduced model. Mathematically, it is represented as: \\[LR = -2 * (\\text{log likelihood of reduced model} - \\text{log likelihood of full model})\\]\nThe Lipsitz test checks if your model fits the real data well. In other words, it sees if the predictions your model makes is the same about the actual data. The test compares what your model predicts (expected probabilities) with what actually occurs (observed frequencies). If your model is good, the predictions and actual outcomes should be pretty close. The null hypothesis of this test is that the frequencies of the ordinal response variable are consistent with the expected frequencies predicted by the model (Lipsitz, et al., 1996).\nAccuracy is one metric for evaluating classification models. It is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset. Accuracy follows the definition:\n\\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nWhere the number of correct predictions, as the name implies, are the accurate predicted values by the model, and the total number of predictions are the total observations in the validation set.\nSensitivity is the metric that evaluates a model’s ability to predict true positives of each available category. Specificity is the metric that evaluates a model’s ability to predict true negatives of each available category (Mitrani, A., 2019). The equations below are for calculating sensitivity and specificity:\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\]\nWhere true positives are the number of observations the model predicted were positive that were actually positive. While false negatives are the number of observations the model predicted were positive that were actually negative. Moreover, false negatives, are the number of observations the model predicted were negative that were actually positive. Lastly, true negatives are the number of observations the model predicted were negative that were actually negative.\nBootstrapping is a statistical technique used to estimate the sampling distribution of an estimator by resampling with replacement from the original data. This is often used when the theoretical distribution of an estimator is complex or unknown. This method involves repeatedly drawing samples, typically thousands of times, from the data set and calculating the statistic of interest for each sample. The bootstrap method allows for the estimation of standard errors, confidence intervals, and significance tests, which are critical in many statistical analyses (Efron and Tibshirani, 1994).\nBootstrapping does not rely on the assumptions of normality and can be applied to complex, skewed, or small datasets where other methods might fail or provide biased estimates (Davison and Hinkley, 1997). It offers a straightforward way to derive robust estimates of standard errors and confidence intervals for complex estimators or models without needing explicit formulas.\n\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\n: Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\n: Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\n: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\n: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\n: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\n: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\nModeling\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\n\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\n: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\n: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\n: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\n: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\n: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\n: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\nModeling\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\nThe additional of variables that relates well to the dependent variable that could affect the decision-making process for potential graduate students. Examples would be psychological factors or financial considerations. For historical datasets like the Titanic, extending the analysis to include crew data and comparing it with other maritime disasters could provide broader insights.\nIt is recommended to refine data handling and model fitting techniques, such as addressing any imbalance in class distributions within datasets or employing more sophisticated methods for handling missing data and outliers to improve model accuracy and reliability.\n\n\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press."
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#model-evaluation-and-diagnostics",
    "href": "projects/Ordered Logit/ordered_logit.html#model-evaluation-and-diagnostics",
    "title": "Ordinal Regression Analysis",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nAssessing model fit is the process of checking whether the model fits the data sufficiently well. The methods of assessment are the following:\n\nPseudo R-squared\nIn linear regression, the coefficient of determination, \\(R^2\\), measures the variance in the dependent variable explained by the predictors, with higher \\(R^2\\) values (up to a maximum of 1) indicating a better explanatory fit of the model. For regression models with categorical dependent variables, calculating a direct \\(R^2\\) similar to that in linear models is unfeasible. Instead, approximations of \\(R^2\\) are used to estimate the model’s explanatory power, adapting the concept to fit the context of categorical outcomes (IBM, 2023). For this assessment, there are three values of \\(R^2\\), each of which have different formulas. The following are the three different \\(R^2\\):\n\\[R^2_{\\text{McF}} = 1 - \\frac{\\log(L_M)}{\\log(L_0)}\\] \\[R^2_{\\text{CS}} = 1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}\\] \\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}}{1 - L_0^{2/n}}\\] where \\(L_M\\) is the fitted model, \\(L_0\\) is the null model, \\(n\\) is the sample size.\n\n\\(R^2_{\\text{McF}}\\) is McFadden’s version of pseudo \\(R^2\\), based on the log-likelihood kernels for the intercept-only model and the full estimated model (McFadden, 1974).\nCox and Snell’s \\(R^2_{\\text{CS}}\\) is based on the log likelihood for the model compared to the log-likelihood for a baseline model (Cox and Snell 1989).\nNagelkerke’s \\(R^2_{\\text{Nagelkerke}}\\) is an adjusted version of the Cox and Snell \\(R^2\\) that adjusts the scale of the statistic to cover the full range from 0 to 1 (Nagelkerke, 1991).\n\nThe category of the the values of \\(R^2\\) can be summarized as follows:"
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#likelihood-ratio-test",
    "href": "projects/Ordered Logit/ordered_logit.html#likelihood-ratio-test",
    "title": "Ordinal Regression Analysis",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe likelihood ratio test compares the likelihoods of these two models to determine whether the additional parameters in the full model significantly improve the fit compared to the reduced model. The null hypothesis of the test is the simpler model (reduced model) is sufficient to explain the data. There is no significant improvement in model fit when adding extra parameters. The alternative hypothesis is the more complex model (full model) provides a significantly better fit to the data than the simpler model. This test is calculated as the ratio of the likelihood of the full model to the likelihood of the reduced model. Mathematically, it is represented as: \\[LR = -2 * (\\text{log likelihood of reduced model} - \\text{log likelihood of full model})\\]\nThe Lipsitz test checks if your model fits the real data well. In other words, it sees if the predictions your model makes is the same about the actual data. The test compares what your model predicts (expected probabilities) with what actually occurs (observed frequencies). If your model is good, the predictions and actual outcomes should be pretty close. The null hypothesis of this test is that the frequencies of the ordinal response variable are consistent with the expected frequencies predicted by the model (Lipsitz, et al., 1996).\nAccuracy is one metric for evaluating classification models. It is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset. Accuracy follows the definition:\n\\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nWhere the number of correct predictions, as the name implies, are the accurate predicted values by the model, and the total number of predictions are the total observations in the validation set.\nSensitivity is the metric that evaluates a model’s ability to predict true positives of each available category. Specificity is the metric that evaluates a model’s ability to predict true negatives of each available category (Mitrani, A., 2019). The equations below are for calculating sensitivity and specificity:\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\]\nWhere true positives are the number of observations the model predicted were positive that were actually positive. While false negatives are the number of observations the model predicted were positive that were actually negative. Moreover, false negatives, are the number of observations the model predicted were negative that were actually positive. Lastly, true negatives are the number of observations the model predicted were negative that were actually negative.\nBootstrapping is a statistical technique used to estimate the sampling distribution of an estimator by resampling with replacement from the original data. This is often used when the theoretical distribution of an estimator is complex or unknown. This method involves repeatedly drawing samples, typically thousands of times, from the data set and calculating the statistic of interest for each sample. The bootstrap method allows for the estimation of standard errors, confidence intervals, and significance tests, which are critical in many statistical analyses (Efron and Tibshirani, 1994).\nBootstrapping does not rely on the assumptions of normality and can be applied to complex, skewed, or small datasets where other methods might fail or provide biased estimates (Davison and Hinkley, 1997). It offers a straightforward way to derive robust estimates of standard errors and confidence intervals for complex estimators or models without needing explicit formulas.\n\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\n: Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\n: Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\n: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\n: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\n: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\n: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\nModeling\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\n\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\n: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\n: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\n: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\n: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\n: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\n: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\nModeling\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\nThe additional of variables that relates well to the dependent variable that could affect the decision-making process for potential graduate students. Examples would be psychological factors or financial considerations. For historical datasets like the Titanic, extending the analysis to include crew data and comparing it with other maritime disasters could provide broader insights.\nIt is recommended to refine data handling and model fitting techniques, such as addressing any imbalance in class distributions within datasets or employing more sophisticated methods for handling missing data and outliers to improve model accuracy and reliability.\n\n\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press."
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#implementation-in-r",
    "href": "projects/Ordered Logit/ordered_logit.html#implementation-in-r",
    "title": "Ordinal Regression Analysis",
    "section": "Implementation in R",
    "text": "Implementation in R\n\nExample 1\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\n\napply (Dependent Variable) - The apply variable is an ordered categorical variable with responses to a survey about whether a student feels they are “Unlikely” (1), “Somewhat likely” (2), or “Very likely” (3) to apply to graduate school.\nparental education status variable or pared - The pared variable is a binary variable indicating if at least one parent has attended graduate school. 1 - at least one parent has a graduate degree; 0 otherwise\npublic - The public variable is a binary variable indicating if the undergraduate institution is public (as opposed to private); 1- public; 0-private\ngpa - gpa variable is the student’s grade point average (1-4)\n\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\n: Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\n: Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\n: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\n: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\n: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\n: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\n: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\nModeling\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\n\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\n: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\n: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\n: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\n: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\n: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\n: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\nModeling\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\n\nLipsitz Test\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\nThe additional of variables that relates well to the dependent variable that could affect the decision-making process for potential graduate students. Examples would be psychological factors or financial considerations. For historical datasets like the Titanic, extending the analysis to include crew data and comparing it with other maritime disasters could provide broader insights.\nIt is recommended to refine data handling and model fitting techniques, such as addressing any imbalance in class distributions within datasets or employing more sophisticated methods for handling missing data and outliers to improve model accuracy and reliability.\n\n\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press."
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#example-2",
    "href": "projects/Ordered Logit/ordered_logit.html#example-2",
    "title": "Ordinal Regression Analysis",
    "section": "Example 2",
    "text": "Example 2\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\n\nsurvived - 1 if yes, 0 if did not survived;\nsex - 1 if male, 0 for female;\nage - in years (and for some children, fractions of a year); and\npassengerClass(Dependent Variable) - class of the passengers, either 1st, 2nd, or 3rd class.\n\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\nsurvivedyes: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\nsexmale: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\nage: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n1st|2nd: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n2nd|3rd: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\nsurvivedyes: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\nsexmale: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\nage: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes."
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#assessing-of-model-fit",
    "href": "projects/Ordered Logit/ordered_logit.html#assessing-of-model-fit",
    "title": "Ordinal Regression Analysis",
    "section": "Assessing of Model Fit",
    "text": "Assessing of Model Fit\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\n\nConfusion Matrix and Statistics by Class of Titanic Survival Data\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n2-4 (l)6-11\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics."
  },
  {
    "objectID": "projects/Ordered Logit/ordered_logit.html#removing-insignificant-variable",
    "href": "projects/Ordered Logit/ordered_logit.html#removing-insignificant-variable",
    "title": "Ordinal Regression Analysis",
    "section": "Removing Insignificant Variable",
    "text": "Removing Insignificant Variable\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\nModeling\n\nComparison of Coefficients and Intercepts\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n2-5 (l)7-10\nValue\nStd. Error\nt value\np value\n\nValue\nStd. Error\nt value\np value\n\n\nsurvivedyes\n-1.751\n0.184\n-9.538\n0\n\n-1.592\n0.149\n-10.683\n0\n\n\nsexmale\n-0.275\n0.181\n-1.517\n0.129\n\n-\n-\n-\n-\n\n\nage\n-0.066\n0.005\n-12.131\n0\n\n-0.066\n0.005\n-12.154\n0\n\n\n1st|2nd\n-4.082\n0.276\n-14.774\n0\n\n-3.848\n0.226\n-17.011\n0\n\n\n2nd|3rd\n-2.688\n0.253\n-10.617\n0\n\n-2.456\n0.198\n-12.377\n0\n\n\n\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\n\nComparison of Confidence Interval of Logit\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n2-4 (l)6-8\nlogit\n2.5 %\n97.5 %\n\nlogit\n2.5 %\n97.5 %\n\n\nsurvivedyes\n-1.7506\n-2.11550\n-1.3953\n\n-1.5917\n-1.8866\n-1.3022\n\n\nsexmale\n-0.2745\n-0.6328\n0.0772\n\n-\n-\n-\n\n\nage\n-0.0657\n-0.0765\n-0.0553\n\n-0.0659\n-0.0767\n-0.0554\n\n\n\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nComparison of Confidence Interval of Odds Ratio\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n2-4 (l)6-8\nOR\n2.5 %\n97.5 %\n\nOR\n2.5 %\n97.5 %\n\n\nsurvivedyes\n0.1737\n0.1206\n0.2478\n\n0.2036\n0.1516\n0.2719\n\n\nsexmale\n0.7599\n0.5311\n1.0802\n\n-\n-\n-\n\n\nage\n0.9364\n0.9263\n0.9462\n\n0.9362\n0.9262\n0.9461\n\n\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\n\nComparison of Pseudo R-squared\n\n\n\nPseudo R-Squared\n\n\n\n\n2-4\nOriginal\n\nRemodeled\n\n\nMcFadden\n0.149588\n\n0.148262\n\n\nCox and Snell (ML)\n0.270207\n\n0.268165\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n0.305342\n\n\n\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\n\nComparison on LRT\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\nOriginal\n-3\n-131.51\n263.02\n0\n\n\nRemodeled\n-2\n-130.34\n260.69\n0\n\n\n\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nComparison on Lipsitz Test\n\n\n\nLR Statistic\ndf\np value\n\n\n\n\nOriginal\n16.42\n9\n0.05861\n\n\nRemodeled\n13.064\n9\n0.1597\n\n\n\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\n\nComparison of Confusion Matrix\n\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n3-5 (l)7-12\nPrediction\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\nOriginal\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\n1st\n42\n0\n15\n\n\n\n\n\n\n\n\n\nRemodeled\n2nd\n16\n0\n37\n0.59\n0.58\nNA\n0.62\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\n\nComparison on Residual Deviance and AIC\n\n\n\nResidual Deviance\nAIC\n\n\n\n\nOriginal\n1495.276\n1505.276\n\n\nRemodeled\n1497.609\n1505.609\n\n\n\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\n\nBootstrap Statistics\n\n\n\noriginal\nbias\nstd. error\n\n\n\n\nt1*\n-0.3329053\n-1.09505\n3.537359\n\n\nt2*\n-2.3329053\n-1.09505\n3.537359\n\n\n\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\n\nBootstrap Confidence Interval\n\n\n\nLevel\nBasic\n\n\n\n\n\\(t1*\\)\n95 %\n(-2.6513, 10.0923)\n\n\n\\(t2*\\)\n95 %\n(-4.651, 8.092)\n\n\n\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit."
  },
  {
    "objectID": "projects/MDS/mds.html",
    "href": "projects/MDS/mds.html",
    "title": "Multidimensional Scaling",
    "section": "",
    "text": "Dimension-reduction techniques transform complex, high-dimensional datasets into simpler two or three-dimensional visual representations. These methods ensure that the relative distances between data points are maintained, making it easier to analyze and interpret the spatial relationships and patterns inherent in the data.\nMultidimensional Scaling (MDS) is a dimension-reduction technique designed to project high-dimensional data to two or three dimensions while preserving relative distances between observations. The fundamental aim is to place items so that the distances in the low-dimensional space as closely as possible match the given dissimilarities, usually measured on a qualitative or quantitative scale.\nMDS is a powerful statistical technique for analyzing similarity or dissimilarity data, allowing researchers to visualize the level of similarity of individual cases of a dataset by representing data points in a low-dimensional space. This technique is particularly useful in fields such as psychology, ecology, and market research, where it can reveal underlying structures in complex data sets (Borg & Groenen, 2005; Kruskal & Wish, 1978) and where measuring the perceived differences between data can be pivotal.\nThis statistical technique not only helps uncover hidden relationships in data but also facilitates the creation of insightful visual representations."
  },
  {
    "objectID": "projects/MDS/mds.html#metric-multidimensional-scaling",
    "href": "projects/MDS/mds.html#metric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Metric Multidimensional Scaling",
    "text": "Metric Multidimensional Scaling\nGiven a number of objects with their dissimilarities recorded, metric Multidimensional Scaling (mMDS) aims to arrange these objects as points in a space such that the spatial distances between the points match the given dissimilarities. Each dissimilarity value, denoted by \\(\\delta_{rs}\\) is transformed by a continuous, monotonic function \\(f\\) to determine the distance \\(d_{rs}\\) between points in this space, where \\[d_{rs} = f(\\delta_{rs})\\] This method has its roots in classical scaling, a technique developed in the 1930s by Young and Householder (1938). They demonstrated that starting from a matrix of distances among all points in a Euclidean space, it’s possible to compute coordinates for these points that preserve these distances. The approach gained widespread recognition when Torgerson (1952) popularized it by adapting the method to use dissimilarities instead of actual distances, thus broadening its application."
  },
  {
    "objectID": "projects/MDS/mds.html#principal-coordinate-analysis",
    "href": "projects/MDS/mds.html#principal-coordinate-analysis",
    "title": "Multidimensional Scaling",
    "section": "Principal Coordinate Analysis",
    "text": "Principal Coordinate Analysis\nPrincipal Coordinate Analysis (PCoA), often interchangeable with metric MDS in many contexts, also focuses on preserving the actual distances between pairs of objects in the dimension-reduced space. The approach starts with a distance matrix and applies eigenvalue decomposition to derive the principal coordinates. PCoA can handle any symmetric distance matrix and thus provides a flexible method for dimensionality reduction. Primary distinction to Principal Component Analysis (PCA) is that PCoA uses distance matrix of the items while PCA utilizes covariance matrix of the exploratory variables.\nTo grasp the link between PCoA and PCA, it’s essential to revisit some foundational concepts and introduce new insights from these methods:\n\nPrincipal Components as Linear Combinations:  \nEach principal component is a linear combination of the original variables, represented by the formula: \\[Z_i = a_{i1}X_1 + a_{i2}X_2 + \\dots + a_{ip}X_p\\] This showcases how principal components simplify the data by transforming the original variables into new axes that capture the majority of the data’s variance.\nCovariance and Similarity Matrices:  \nWhen the variables have zero mean, the covariance matrix \\(C\\) is calculated as: \\[\\mathbf{C} = \\frac{\\mathbf{X}^T\\mathbf{X}}{n-1}\\] PCA uses this covariance matrix to derive eigenvalues and eigenvectors that define the principal components. In contrast, PCoA uses a matrix \\(\\mathbf{S} = \\mathbf{XX}^T\\), which represents similarities or dissimilarities, making it suitable for data that doesn’t naturally fit into a covariance analysis framework.\nCalculating and Interpreting Components:  \nThe principal components are obtained through: \\[\\mathbf{Z} = \\mathbf{XA}'\\] where \\(\\mathbf{Z}\\) includes the principal component scores, and \\(\\mathbf{A}'\\) is the matrix of eigenvectors’ transpose. This transformation allows for the original data matrix \\(\\mathbf{X}\\) to be reconstructed from \\(\\mathbf{Z}\\) and \\(\\mathbf{A}\\): \\[\\mathbf{X} = \\mathbf{ZA}\\] Unlike PCA, PCoA often scales components differently, affecting how they’re interpreted compared to PCA."
  },
  {
    "objectID": "projects/MDS/mds.html#nonmetric-multidimensional-scaling",
    "href": "projects/MDS/mds.html#nonmetric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Nonmetric Multidimensional Scaling",
    "text": "Nonmetric Multidimensional Scaling\nThe non-metric Multidimensional Scaling (nMDS) method was initially conceptualized by Shepard in the early 1960s and was further refined by Kruskal shortly thereafter. This approach involves arranging a set of \\(n\\) objects based on their dissimilarities, \\(\\delta_{rs}\\) into a configuration of \\(n\\) points within typically Euclidean space. Each object corresponds to a point such that the spatial layout reflects the dissimilarities as accurately as possible.\nNMDS relaxes the requirement of preserving the exact distances and focuses instead on preserving the rank order of distances. This method is particularly useful when the distances do not adhere strictly to a metric scale or when the exact magnitudes of distances are less important than the order of these distances. The primary aim here is to maintain the ordinal relationship of the distances, such as ensuring that if object A is closer to object B than to object C in high-dimensional space, the same should be true in the reduced space. This involves an iterative process where the configuration is adjusted to minimize a stress function shown in the diagnositc section."
  },
  {
    "objectID": "projects/MDS/mds.html#algorithm",
    "href": "projects/MDS/mds.html#algorithm",
    "title": "Multidimensional Scaling",
    "section": "Algorithm",
    "text": "Algorithm\nThe algorithm for recovering coordinates from distances between pairs of points is as follows:\n\nForm matrix \\(\\mathcal{A} = -\\frac{1}{2} \\delta_{rs}^2\\)\nForm matrix \\(\\mathcal{B} = \\mathcal{HAH}\\), where \\(\\mathcal{H}\\) is the centering matrix \\(\\mathcal{H} = \\mathcal{I}- n^{-1} \\mathbf{1}_n\\mathbf{1}_n^T\\), where \\(\\mathcal{I}\\) is the identity matrix, and \\(\\mathbf{1}_n\\) is a vector of ones.\nFind the spectral decomposition of \\(\\mathcal{B}, \\mathcal{B} = \\mathcal{V} \\Lambda \\mathcal{V}^T\\), where \\(\\Lambda\\) is the diagonal matrix formed from eigenvalues of \\(\\mathcal{B}\\), and \\(\\mathcal{V}\\) is the matrix of corresponding eigenvectors.\nIf the points were originally in a \\(p\\)-dimensional space, the first \\(p\\) eigenvalues of \\(\\mathcal{B}\\) are nonzero and the remaining \\(n-p\\) are zero. Discard these from \\(\\Lambda\\) (rename as \\(\\Lambda_1\\), and discard the corresponding eigenvalues from \\(\\mathcal{V}\\) (rename as \\(\\mathcal{V}_1\\)).\nFind \\(\\mathcal{X} = \\mathcal{V}_1 \\Lambda_1^{1/2},\\) and coordinates of the points are given by the row of \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "projects/MDS/mds.html#standardized-residual-sum-of-squares",
    "href": "projects/MDS/mds.html#standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Standardized Residual Sum of Squares",
    "text": "Standardized Residual Sum of Squares\n\\[STRESS(q) = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^{(q)} - \\hat{d}_{rs}^{(q)}\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}^{(q)}\\right)^2}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}^{(q)}\\) is the distance between object \\(r\\) and object \\(s\\) for \\(q\\) configuration; and\n\\(\\hat{d}_{rs}^{(q)}\\) is the regression of \\(d_{rs}^{(q)}\\) and \\(\\delta_{rs}\\)(data distance).\n\nKruskall (1978) suggests the stress be informally interpreted according to the guidelines:\n\nStress Value Category\n\n\nStress\nGoodness of fit\n\n\n\n\n20%\nPoor\n\n\n10%\nFair\n\n\n5%\nGood\n\n\n2.5%\nExcellent\n\n\n0%\nPerfect"
  },
  {
    "objectID": "projects/MDS/mds.html#squared-standardized-residual-sum-of-squares",
    "href": "projects/MDS/mds.html#squared-standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Squared Standardized Residual Sum of Squares",
    "text": "Squared Standardized Residual Sum of Squares\n\\[SSTRESS = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^2 - \\hat{d}_{rs}^2\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}\\right)^4}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}\\) is the distance between object \\(r\\) and object \\(s\\) for the configuration; and\n\\(\\hat{d}_{rs}\\) is the regression of \\(d_{rs}\\) and \\(\\delta_{rs}\\)(data distance)."
  },
  {
    "objectID": "projects/MDS/mds.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "href": "projects/MDS/mds.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "title": "Multidimensional Scaling",
    "section": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City",
    "text": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City\n\n\n\n\n\nMap of Iligan\n\n\n\n\nThe goal for this data is to visualize and understand the spatial relationships among different barangays based on their geographic distances. We want to see if MDS accurately captures the original distances between each barangay in Iligan City. After performing MDS, we expect that Barangay Buru-un will be shown to be closer to Barangay Ditucalan or Maria Cristina, as opposed to its distance from Rogongon. The table showcases the list of barangays in Iligan City\n\nList of Barangays\n\n\nAbuno\nHinaplanon\nPala-o\nSaray\n\n\nAcmac\nHindang\nPanoroganan\nSuarez\n\n\nBagong Silang\nKabacsanan\nPoblacion\nTambacan\n\n\nBonbonon\nKalilangan\nPuga-an\nTibanga\n\n\nBunawan\nKiwalan\nRogongon\nTipanoy\n\n\nBuru-un\nLanipao\nSan Miguel\nTomas L. Cabili\n\n\nDalipuga\nLuinab\nSan Roque\nUpper Tominobo\n\n\nDel Carmen\nMahayahay\nSanta Elena\nTubod\n\n\nDigkilaan\nMainit\nSanta Filomena\nUbaldo Laya\n\n\nDitucalan\nMandulog\nSantiago\nUpper Hinaplanon\n\n\nDulag\nMaria Cristina\nSanto Rosario\nVilla Verde\n\n\n\n\nData\nThe data for this application includes the coordinates of the barangay halls, which represent the locations of each barangay. The coordinates were collected using web scraping.\n\nbarangays &lt;- read.delim(\"Barangays.txt\", header = FALSE)\ncoordinates &lt;- data.frame()\nfor(i in 1:nrow(barangays)){\n  barangay &lt;- tolower(barangays[i, ])\n  website &lt;- glue(\"https://www.philatlas.com/mindanao/r10/iligan/{barangay}.html\")\n  h &lt;- read_html(website)\n  latitude &lt;- html_elements(h, xpath='.//span[@id=\"latitude\"]') %&gt;% \n    html_text()\n  longitude &lt;- html_elements(h, xpath='.//span[@id=\"longitude\"]') %&gt;% \n    html_text()\n  c &lt;- cbind.data.frame(barangay, \n                        latitude = as.numeric(latitude), \n                        longitude = as.numeric(longitude))\n  coordinates &lt;- rbind.data.frame(coordinates, c)\n}\nknitr::kable(head(coordinates))\n\n\n\n\nHead of Iligan City Barangays Data\n\n\nbarangay\nlatitude\nlongitude\n\n\n\n\nabuno\n8.1846\n124.2571\n\n\nacmac\n8.2740\n124.2649\n\n\nbagong-silang\n8.2418\n124.2520\n\n\nbonbonon\n8.2667\n124.2902\n\n\nbunawan\n8.3023\n124.3028\n\n\nburu-un\n8.1872\n124.1688\n\n\n\n\n\nThe table above shows the first six of the barangays only. Also, the table illustrates the respective latitude and longitude to each of the barangays in Iligan city. Now that we are done the web scraping part, we proceed to the checking of assumptions.\n\n\nChecking Assumptions\n\n# Missing Data\ncoordinates %&gt;% sapply(., function(x) sum(is.na(x)))\n\n barangay  latitude longitude \n        0         0         0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the three variables. Therefore, there is no need to take any further steps to handle any missing data.\n\n# Duplicates\ncoordinates %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 3\n# ℹ 3 variables: barangay &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\nThe output of the code above indicates that the data contains no duplicates. This suggests that the coordinates of the barangays are unique, as expected.\n\n\nCalculating Haversine Distance\nThe Haversine formula calculates the shortest distance between two points on the surface of a sphere, given their longitudes and latitudes. This is particularly useful in geography for finding the distance between two locations on Earth, as it accounts for the Earth’s curvature.\nWhen we provide the coordinates (latitude and longitude) of two barangays, the Haversine formula helps determine the great-circle (or orthodromic) distance between them. This distance represents the shortest path between the two points along the surface of the sphere, rather than a straight line through the Earth’s interior. The formula of Haversine is\n\\[d = 2r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\phi_2 - \\phi_1}{2}\\right) + \\cos(\\phi_1) \\cos(\\phi_2) \\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\\]\nwhere:\n\n\\(\\phi_1, \\phi_2\\) are the latitudes of the two points in radians,\n\\(\\lambda_1, \\lambda_2\\) are the longitudes of the two points in radians,\n\\(r\\) is the radius of the Earth (approximately 6371 kilometers or 3959 miles),\n\\(d\\) is the distance between the two points.\n\nThe Haversine formula can be utilized in the distm() function, which takes latitude and longitude as parameters. Set the fun parameter to distHaversine to apply the formula mentioned above. The output is in matrix form, with the distance metric given in meters.\n\ndistances &lt;- distm(coordinates[, c(\"longitude\", \"latitude\")], \n                   fun = distHaversine)\ndistances &lt;- distances/1000 #convert to km\ncolnames(distances) &lt;- coordinates$barangay\nrownames(distances) &lt;- coordinates$barangay\n\ndistances[1:5, 1:5]\n\n                  abuno    acmac bagong-silang bonbonon   bunawan\nabuno          0.000000 9.988996      6.392220 9.840034 14.036344\nacmac          9.988996 0.000000      3.855926 2.903147  5.230182\nbagong-silang  6.392220 3.855926      0.000000 5.039188  8.756445\nbonbonon       9.840034 2.903147      5.039188 0.000000  4.199009\nbunawan       14.036344 5.230182      8.756445 4.199009  0.000000\n\n\nAbove is the output of the first five rows and five columns of the distance matrix. As expected, the distance of any point to itself is zero. Additionally, the distance from abuno to acmac, which is 9.988996, is the same as from acmac to abuno. This symmetry confirms that the matrix is symmetric.\nNow, we will proceed to find the optimal dimension that best represents our data. This involves analyzing the stress values and visualizing the corresponding plots to determine which dimensionality reduction provides the clearest and most informative representation of the underlying structure of our dataset.\nThe code below defines a function based on the steps outlined in the Procedure section. This function has two parameters: distance, which is the distance matrix, and dimension, which specifies the number of dimensions the user wishes to explore. The function outputs the stress values for each dimension, along with corresponding plots to help determine the most suitable dimension.\n\noptimal_dimension &lt;- function(distance, dimension){\n  stress &lt;- data.frame()\n  for(i in 1:dimension){\n    mds_result &lt;- cmdscale(distance, k = i)\n    \n    # Calculate the distances in the reduced space\n    fitted_distance &lt;- dist(mds_result)\n    fitted_distance &lt;- as.vector(as.matrix(fitted_distance))\n    original_distance &lt;- as.vector(as.matrix(distance))\n    \n    reg_output &lt;- lm(fitted_distance ~ original_distance)\n    predicted_distance &lt;- fitted(reg_output)\n    \n    # Calculating stress using the modified formula\n    stress_value &lt;- sqrt(sum((fitted_distance - predicted_distance)^2) / \n                           sum(fitted_distance^2))\n    s &lt;- cbind.data.frame(dim = i, stress = stress_value)\n    stress &lt;- rbind.data.frame(stress, s)\n  }\n  p &lt;- ggplot(stress, aes(x=dim, y=stress)) +\n    geom_line() + geom_point() + scale_color_brewer(palette=\"Paired\") +\n    labs(x = \"Dimensions\", title = \"Stress Plot\") +\n    theme_minimal()\n  print(p)\n  \n  return(stress)\n}\n\nTo find the optimal dimension for our analysis, we use the elbow method. This method involves looking at a line plot and finding the ‘elbow,’ a point where the benefits of adding more dimensions start to diminish significantly. Identifying this point helps us select the dimensionality that provides the most meaningful reduction in complexity without significant loss of information.\n\noptimal_dimension(distances, 10)\n\n\n\n\nBarangays in Iligan City Stress Plot\n\n\n\n\n   dim       stress\n1    1 2.263671e-01\n2    2 8.481692e-08\n3    3 7.949367e-08\n4    4 7.949368e-08\n5    5 7.949369e-08\n6    6 7.949369e-08\n7    7 7.949369e-08\n8    8 7.949369e-08\n9    9 7.949369e-08\n10  10 7.949369e-08\n\n\nThe stress plot above illustrates how stress diminishes as the number of dimensions increases. While the elbow in the plot might be subtle due to the small stress values, it appears around two dimensions. This suggests that adding more dimensions beyond two does not significantly enhance the data representation. Therefore, based on this plot, using two dimensions is likely adequate for capturing the structure of the data without adding unnecessary complexity or overfitting. The stress value for two dimensions is near \\(0\\%\\), indicating a perfect goodness of fit as suggested by Kruskal. This extremely low stress value demonstrates an excellent representation of the data in the chosen dimensional space. The stress values for dimensions three to ten are also near \\(0\\%\\), but selecting a dimension within that range becomes challenging to visualize effectively. Higher dimensions can complicate the interpretation without providing significant additional clarity.\nHaving identified the optimal dimension that best represents our data, we will now proceed to perform MDS using two dimensions. To carry out metric MDS in R, we utilize the cmdscale() function, providing it with the distance matrix and specifying two as the number of dimensions.\n\nmds_result &lt;- cmdscale(distances, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City (Inverted)\n\n\n\n\nThe figure above presents an inverted version of the barangay locations in Iligan City. To match the exact location as the original map, we simply multiply the coordinates by -1. This sign reversal does not change the distances between the barangays based on the two dimensions, and the new dimension is therefore just as satisfactory as the original one.\n\nmds_df$V1 &lt;- -1 * mds_df$V1\nmds_df$V2 &lt;- -1 * mds_df$V2 \nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City\n\n\n\n\nThe points in Figure 4 now accurately represent the locations of barangays in Iligan City. As stated earlier, our objective was to verify whether Barangay Buru-un is closer to Barangay Maria Cristina or Barangay Ditucalan than to Barangay Rogongon. The MDS has effectively captured this relationship, as well as the distances between other barangays. Using two dimensions, the MDS has successfully mapped the precise distances among the barangays in Iligan City."
  },
  {
    "objectID": "projects/MDS/mds.html#principal-coordinate-analysis-example-sparrow-data",
    "href": "projects/MDS/mds.html#principal-coordinate-analysis-example-sparrow-data",
    "title": "Multidimensional Scaling",
    "section": "2. Principal Coordinate Analysis Example: Sparrow Data",
    "text": "2. Principal Coordinate Analysis Example: Sparrow Data\nAfter a severe storm on February 1, 1898, a number of moribund sparrows were taken to Hermon Bumpus’ biological laboratory at Brown University, Rhode Island. Subsequently, about half of the birds died, and Bumpus saw this as an opportunity to see whether he could find any support for Charles Darwin’s theory of natural selection. To this end, he made eight morphological measurements on each bird and also weighed the birds. The results for five of the measurements are shown below, for females only.\nThe dataset contains five morphological measurements of the female sparrows where\n\ntotal.length - measures from the tip of the beak to the end of the tail in *mm*\nalar.extent - the distance from tip to tip of extended wings\nbeak.and.head - length of beak and head (from the tip of the beak to the occiput)\nhumerus - length of humerus\nsternum - length of sternum\nsurvival - the survival status of the bird, true if alive, or false otherwise\n\nAt the conclusion of our PCoA modeling, we will compare the results with those obtained from PCA. This comparison will help us understand the differences in how each method captures and represents the underlying data structure.\n\nsparrow_data &lt;- read_excel(\"sparrow.xlsx\")\nsparrow &lt;- sparrow_data[, -c(1,7)]\nknitr::kable(head(sparrow_data), caption = \"First six row of female sparrows\")\n\n\n\n\nFirst six row of female sparrows\n\n\n\n\n\n\n\n\n\n\n\nBird\ntotal.length\nalar.extent\nbeak.and.head\nhumerus\nsternum\nsurvival\n\n\n\n\n1\n156\n245\n31.6\n18.5\n20.5\nT\n\n\n2\n154\n240\n30.4\n17.9\n19.6\nT\n\n\n3\n153\n240\n31.0\n18.4\n20.6\nT\n\n\n4\n153\n236\n30.9\n17.7\n20.2\nT\n\n\n5\n155\n243\n31.5\n18.6\n20.3\nT\n\n\n6\n163\n247\n32.0\n19.0\n20.9\nT\n\n\n\n\n\nFor PCoA purpose, we remove the first column and the second column, and this can be achieved by the following code\n\nsparrow &lt;- sparrow_data[, -c(1,7)]\n\nNow, we proceed by checking the assumptions.\n\nChecking Assumptions\n\n# Missing Data\nsparrow %&gt;% sapply(., function(x) sum(is.na(x)))\n\n total.length   alar.extent beak.and.head       humerus       sternum \n            0             0             0             0             0 \n\n\nThe zero value from the result above suggests that each of the independent variables have no missing datas.\n\n# Duplicates\nsparrow %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# A tibble: 0 × 5\n# ℹ 5 variables: total.length &lt;dbl&gt;, alar.extent &lt;dbl&gt;, beak.and.head &lt;dbl&gt;,\n#   humerus &lt;dbl&gt;, sternum &lt;dbl&gt;\n\n\nThe result from the code suggests that no morphological measurements of the female sparrows are the same. Now we are done checking the assumptions, we now proceed to calculating the distance matrix of the data.\n\n\nCalculating Euclidean distance\nSince we are about to model a PCoA example, the distance that is to be utilized is euclidean. This can be performed in R by using the dist() function that feeds the dataset and set the method to euclidean which indicates the usage of Euclidean distance.\n\ndist_matrix &lt;- dist(sparrow, method = \"euclidean\")\ndist_matrix %&gt;% as.matrix() %&gt;% .[1:5,1:5]\n\n         1        2        3        4        5\n1 0.000000 5.622277 5.863446 9.550916 2.249444\n2 5.622277 0.000000 1.615549 4.201190 3.491418\n3 5.863446 1.615549 0.000000 4.081666 3.657868\n4 9.550916 4.201190 4.081666 0.000000 7.360706\n5 2.249444 3.491418 3.657868 7.360706 0.000000\n\n\nNow we are done calculating the distance matrix, and checking the assumptions, the preparations for modeling is now complete. Now, we find the optimal dimension of the dataset.\n\n\nModeling\nWe set the dimension to 5, as a similar process to PCA.\n\noptimal_dimension(dist_matrix, 5)\n\n\n\n\nSparrows Stress Plot\n\n\n\n\n  dim       stress\n1   1 1.437590e-01\n2   2 2.456834e-02\n3   3 1.041855e-02\n4   4 2.913686e-03\n5   5 5.677040e-16\n\n\nFigure 5 illustrates how the stress decreases as the number of dimensions increases from one to five. There’s a noticeable drop in stress when moving from one to two dimensions, indicating that two dimensions capture much more of the data’s structure compared to just one. The stress continues to reduce slightly as we add a third dimension, but the rate of decline slows down significantly.\nFrom the plot, we can see an ‘elbow’ forming between the second dimensions. This suggests that adding more dimensions beyond two brings minimal improvement in stress reduction.\n\nmds_result &lt;- cmdscale(dist_matrix, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(x = V1, y = V2)) +\n  geom_point(aes(color = sparrow_data$survival)) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", \n       title = \"PCoA Plot\", color = \"survival\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Sparrows\n\n\n\n\nFigure 6 shows the PCoA results for a dataset on sparrows, visualized in a two-dimensional space. The plot categorizes sparrows based on a survival attribute, with the categories represented by two different colors: red (‘F’) for those that did not survive and blue (‘T’) for those that did survive.\nThe horizontal axis (Dimension 1) and the vertical axis (Dimension 2) represent the two principal coordinates derived from the dataset. There isn’t a clear, distinct clustering pattern where one group is entirely separate from the other. Both survived (‘T’) and not survived (‘F’) sparrows are spread throughout the plot. This suggests that based solely on these two principal coordinates, there isn’t much differentiation in the dataset that correlates strongly with the survival outcomes.\nWe compare it to PCA with two as the remained principal components,\n\nsparrow_pca &lt;- prcomp(sparrow, scale = TRUE)\nautoplot(sparrow_pca, data = sparrow_data, colour = 'survival', label.size = 3) +\n  labs(title = \"PCA Plot\") +\n  theme_minimal()\n\n\n\n\nPCA plot of Sparrows\n\n\n\n\nFigure 7 shows the PCA plot of sparrows, visualized in a space defined by the first two principal components: PC1 and PC2. This plot also categorizes the sparrows based on their survival status, with red dots (‘F’) representing those that did not survive and blue dots (‘T’) for those that did survive.\nThe first principal component (PC1) explains a significant portion of the variance in the dataset at 72.32%, indicating that it captures a major underlying pattern or trend related to the features measured. PC2 explains an additional 10.63% of the variance, providing a secondary perspective on the data.\nSimilar to the PCoA plot, the PCA plot does not show clear, distinct clusters of survival outcomes. Both survival and non-survival sparrows are scattered throughout the PCA space. Both PCA and PCoA plots show an overlap of survival statuses with no clear separation between the ‘F’ and ‘T’ categories. This suggests that the survival outcome is not strongly linearly correlated with the principal components derived from the data in both analyses.\nPCA, which focuses more on maximizing variance and often reveals more defined patterns and relationships in the data, still shows an overlap similar to PCoA. This suggests that the variables influencing survival might be complex and not easily separable by linear methods like PCA and PCoA.\nWhile PCoA is more about preserving the distance or dissimilarity accurately in a lower-dimensional space, PCA aims to capture the variance in the data. The similar patterns of overlap in both analyses indicate that further investigation might be needed, perhaps with different analytical approaches or additional data features, to better understand the factors affecting survival."
  },
  {
    "objectID": "projects/MDS/mds.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "href": "projects/MDS/mds.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "title": "Multidimensional Scaling",
    "section": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data",
    "text": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data\nIn this nMDS example, the will use the Doubs Fish data, the Fish community composition of the Doubs River in France. They come from Verneaux’s PhD thesis (1973), where he proposed to use fish species to characterize ecological zones along European rivers and streams. The values in ‘Doubs.fish’ are counts of individuals of each of 27 species observed in a set of 30 sites located along the 453 km long Doubs River, France. The data contains the following:\n\nenv - is a data frame with 30 rows (sites) and 11 environmental variables.\nfish - is a data frame with 30 rows (sites) and 27 fish species.\nxy - is a data frame with 30 rows (sites) and 2 spatial coordinates.\nspecies - is a data frame with 27 rows (species) and 4 columns (names)\n\nThe goal here is to visualize the fish abundance at different sites. It can reveal natural groupings (clusters) of sites with similar fish communities This can be crucial for managing fish populations.\nThe data can be access by performing the data() function and feed doubs in it. The site 8 is removed in the data since that site have no species.\n\ndata(doubs)\nspe &lt;- doubs$fish[-8,]\nspe %&gt;% .[1:6, 1:12]\n\n  Cogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi\n1    0    3    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0\n4    0    4    5    5    0    0    0    0    0    1    0    0\n5    0    2    3    2    0    0    0    0    5    2    0    0\n6    0    3    4    5    0    0    0    0    1    2    0    0\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\nab &lt;- table(unlist(spe))\npar(mfrow = c(1, 2))\nbarplot(ab, las = 1, col = grey(5:0/5), main = \"Abundance Distribution\",\n        xlab = \"Abundance class\", ylab = \"Frequency\")\nsite.pre &lt;- rowSums(spe &gt; 0)\nbarplot(site.pre, main = \"Species richness\",\n        xlab = \"Sites\", ylab = \"Number of species\",\n        col = \"grey \", las = 1)\n\n\n\n\nAbundance Distribution and Species Richness\n\n\n\n\nThe plot on the left above reveals that the majority of species observations across various sites register an abundance of zero. This is expected given the large number of sites surveyed—–30 in total—–with many sites not recording the presence of different fish species. There are also sites where the count of fish species present is five, with decreasing frequencies noted for higher abundance classes. This pattern highlights that while some sites exhibit richer biodiversity, many others have little to no fish presence.\nThe ‘Species Richness’ plot on the right side illustrates the number of different species found at each site, showing considerable variability. Some sites have a high species richness, nearing 25 species, while others have much lower counts. This variability might suggest differences in environmental conditions, habitat quality, or other ecological factors influencing species distribution across the sites.\n\nsum(spe == 0)\n\n[1] 408\n\n\n\nsum(spe == 0)/(nrow(spe) * ncol(spe))\n\n[1] 0.5210728\n\n\nOver half of the dataset is made up of zeros, which is quite high but not unusual for species abundance data. The presence of many zeros can cause a ‘double zero problem,’ where the absence of species at different sites can inflates their similarity based on what they both lack, rather than what they have. This means two sites might seem similar because they are both missing certain species, but this doesn’t necessarily reflect true ecological similarity. Ideally, we want the presence of species, rather than their absence, to inform how similar two sites are.\nTo counter this double zero issue, we’ll transform the species data. Pierre Legendre and Gallagher in 2001 recommended five possible pre-transformations for species data, four of which can be performed using the decostand() function from the vegan package.\nOne effective transformation is the Hellinger transformation, which represents species abundances as the square root of their relative abundance at each site, as suggested by Borcard, Gillet, and Legendre (2011). This approach addresses the problem with double zeros effectively. We will use this transformation on our fish abundance dataset to ensure our similarity assessments are more ecologically meaningful. And we will compare this without the transformation to determine if there is changes in the output.\n\nspe.hel &lt;- decostand(spe, method = \"hellinger\")\n\n\n\nChecking Assumptions\n\n# Missing Data\nspe %&gt;% sapply(., function(x) sum(is.na(x)))\n\nCogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi Gogo Eslu Pefl Rham \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \nLegi Scer Cyca Titi Abbr Icme Acce Ruru Blbj Alal Anan \n   0    0    0    0    0    0    0    0    0    0    0 \n\n# Duplicates\nspe %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 27\n# ℹ 27 variables: Cogo &lt;dbl&gt;, Satr &lt;dbl&gt;, Phph &lt;dbl&gt;, Neba &lt;dbl&gt;, Thth &lt;dbl&gt;,\n#   Teso &lt;dbl&gt;, Chna &lt;dbl&gt;, Chto &lt;dbl&gt;, Lele &lt;dbl&gt;, Lece &lt;dbl&gt;, Baba &lt;dbl&gt;,\n#   Spbi &lt;dbl&gt;, Gogo &lt;dbl&gt;, Eslu &lt;dbl&gt;, Pefl &lt;dbl&gt;, Rham &lt;dbl&gt;, Legi &lt;dbl&gt;,\n#   Scer &lt;dbl&gt;, Cyca &lt;dbl&gt;, Titi &lt;dbl&gt;, Abbr &lt;dbl&gt;, Icme &lt;dbl&gt;, Acce &lt;dbl&gt;,\n#   Ruru &lt;dbl&gt;, Blbj &lt;dbl&gt;, Alal &lt;dbl&gt;, Anan &lt;dbl&gt;\n\n\nMDS can be performed in R using the metaMDS() function from the vegan package, which is quite user-friendly. This function can handle both raw data and precomputed distance matrices. We will use the Bray-Curtis dissimilarity for this dataset. The Bray-Curtis dissimilarity is a statistic used to quantify the compositional dissimilarity between two different sites or samples based on counts or measurements of species abundance. It is particularly useful in ecology and environmental science for comparing the composition of different ecological communities.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe, distance=\"bray\")\n\nRun 0 stress 0.0747782 \nRun 1 stress 0.112424 \nRun 2 stress 0.08930115 \nRun 3 stress 0.1104319 \nRun 4 stress 0.08987062 \nRun 5 stress 0.08886178 \nRun 6 stress 0.08886178 \nRun 7 stress 0.1219299 \nRun 8 stress 0.1209556 \nRun 9 stress 0.07376216 \n... New best solution\n... Procrustes: rmse 0.0193953  max resid 0.09464088 \nRun 10 stress 0.07478342 \nRun 11 stress 0.07506678 \nRun 12 stress 0.08797377 \nRun 13 stress 0.1118912 \nRun 14 stress 0.07477838 \nRun 15 stress 0.1123498 \nRun 16 stress 0.08696388 \nRun 17 stress 0.09157401 \nRun 18 stress 0.1125696 \nRun 19 stress 0.1119532 \nRun 20 stress 0.1124389 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     4: no. of iterations &gt;= maxit\n    15: stress ratio &gt; sratmax\n     1: scale factor of the gradient &lt; sfgrmin\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe \nDistance: bray \n\nDimensions: 2 \nStress:     0.07376216 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 9 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe' \n\n\nThe data will be sufficiently explained in two dimensions, which suggests that the resulting plot or ordination will map the data onto two principal axes. The stress value reported is 0.07376216, which is a measure of the fit quality of the NMDS configuration. A stress value below 0.1, is typically considered an excellent fit, basing on the category Kruskall presented.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nNMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe NMDS plot provided has a stress level of 0.074 indicating a reliable fit, visually maps the relationships between different sites, numbered on the plot, and the fish species found there marked in red. The plot shows how some sites cluster together, suggesting they share similar species compositions—likely due to similar environmental conditions or close geographical proximity. Other sites stand out as more isolated, indicating unique ecological characteristics or distinct species that might not be present in more central or clustered locations. Mjority of the fishes are clustered in the site 29, just like what is shown in the descriptive stat eralier. The Teso species solos the site 15, indicating maybe the fish is a territorial or there is an environmental variable there that only the said fish is interested.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot\n\n\n\n\nA Shepard plot is a diagnostic tool used in nMDS to assess the quality of the MDS solution. The Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance \\((R^2 &gt; 0.995)\\) highlighting a high goodness of fit. Both the non-metric and linear fits show high \\(R^2\\) values, suggesting that the MDS model has been very effective at preserving the distances between data points when reducing dimensionality. The non-metric fit is particularly effective, which is typical as non-metric MDS aims to preserve the rank order rather than the actual values of distances, making it more flexible in handling varied data scales and distributions. The effectiveness of the non-metric MDS in this case suggests that if the data contained non-linear relationships or was from different scales of measurement, the method still managed to capture and represent these complexities accurately.\nWe now proceed in performing nMDS for the hellinger transformed of the dataset.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe.hel, distance=\"bray\")\n\nRun 0 stress 0.06746136 \nRun 1 stress 0.1024773 \nRun 2 stress 0.1016353 \nRun 3 stress 0.1024777 \nRun 4 stress 0.1016351 \nRun 5 stress 0.1067558 \nRun 6 stress 0.1067557 \nRun 7 stress 0.102477 \nRun 8 stress 0.09577038 \nRun 9 stress 0.06742454 \n... New best solution\n... Procrustes: rmse 0.008564238  max resid 0.04061037 \nRun 10 stress 0.06747835 \n... Procrustes: rmse 0.01111845  max resid 0.05270437 \nRun 11 stress 0.08323628 \nRun 12 stress 0.1058012 \nRun 13 stress 0.07528549 \nRun 14 stress 0.06743465 \n... Procrustes: rmse 0.005265951  max resid 0.02493877 \nRun 15 stress 0.06742479 \n... Procrustes: rmse 0.00010268  max resid 0.0004864955 \n... Similar to previous best\nRun 16 stress 0.1089367 \nRun 17 stress 0.1016995 \nRun 18 stress 0.06742515 \n... Procrustes: rmse 0.0002102026  max resid 0.0009945225 \n... Similar to previous best\nRun 19 stress 0.0957702 \nRun 20 stress 0.06742439 \n... New best solution\n... Procrustes: rmse 6.295875e-05  max resid 0.0002982595 \n... Similar to previous best\n*** Best solution repeated 1 times\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe.hel, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe.hel \nDistance: bray \n\nDimensions: 2 \nStress:     0.06742439 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 20 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe.hel' \n\n\nThe output above suggests that dimension two is enough to represent the data. This reduction allows for easier visualization and interpretation of the data’s underlying patterns. A stress value of 0.06742439 indicates an excellent fit of the NMDS model to the data, suggesting that the two-dimensional representation is a reliable depiction of the species dissimilarities. The stress value of Hellinger transformed data is lower than that of without transformation, indicating that the transformed model is much better.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nHellinger Transformed of NMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe numbered points represent different sites, and the red text labels specific fish species found at those sites. Similar to the earlier plot, sites that are close together likely have similar species compositions, and the species names near each cluster give insight into the distinct ecological characteristics of those clusters. This plot includes more specific information about the species present at each site compared to the earlier plot. This detail allows for a more in-depth understanding of the ecological and biological diversity across the sampled sites. Both plots have low stress values, but this one is slightly lower (0.067 compared to 0.074), indicating a slightly better fit in the representation of dissimilarities.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot of Hellinger Transformed\n\n\n\n\nThe non-metric fit, with an \\(R^2\\) of 0.995, indicates an excellent adherence to the rank order of the original dissimilarities. This value shows that the nMDS model has successfully captured the underlying structure of the data that preserves the relative dissimilarities among observations. The linear fit has an \\(R^2\\) of 0.981, which is also very high, suggesting that the distances in the MDS space linearly correlate well with the observed dissimilarities. Compared to the Shepard PLot without performing Hellinger transformation, this is better."
  },
  {
    "objectID": "projects/MDS/index.html",
    "href": "projects/MDS/index.html",
    "title": "Multidimensional Scaling",
    "section": "",
    "text": "Dimension-reduction techniques transform complex, high-dimensional datasets into simpler two or three-dimensional visual representations. These methods ensure that the relative distances between data points are maintained, making it easier to analyze and interpret the spatial relationships and patterns inherent in the data.\nMultidimensional Scaling (MDS) is a dimension-reduction technique designed to project high-dimensional data to two or three dimensions while preserving relative distances between observations. The fundamental aim is to place items so that the distances in the low-dimensional space as closely as possible match the given dissimilarities, usually measured on a qualitative or quantitative scale.\nMDS is a powerful statistical technique for analyzing similarity or dissimilarity data, allowing researchers to visualize the level of similarity of individual cases of a dataset by representing data points in a low-dimensional space. This technique is particularly useful in fields such as psychology, ecology, and market research, where it can reveal underlying structures in complex data sets (Borg & Groenen, 2005; Kruskal & Wish, 1978) and where measuring the perceived differences between data can be pivotal.\nThis statistical technique not only helps uncover hidden relationships in data but also facilitates the creation of insightful visual representations."
  },
  {
    "objectID": "projects/MDS/index.html#metric-multidimensional-scaling",
    "href": "projects/MDS/index.html#metric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Metric Multidimensional Scaling",
    "text": "Metric Multidimensional Scaling\nGiven a number of objects with their dissimilarities recorded, metric Multidimensional Scaling (mMDS) aims to arrange these objects as points in a space such that the spatial distances between the points match the given dissimilarities. Each dissimilarity value, denoted by \\(\\delta_{rs}\\) is transformed by a continuous, monotonic function \\(f\\) to determine the distance \\(d_{rs}\\) between points in this space, where \\[d_{rs} = f(\\delta_{rs})\\] This method has its roots in classical scaling, a technique developed in the 1930s by Young and Householder (1938). They demonstrated that starting from a matrix of distances among all points in a Euclidean space, it’s possible to compute coordinates for these points that preserve these distances. The approach gained widespread recognition when Torgerson (1952) popularized it by adapting the method to use dissimilarities instead of actual distances, thus broadening its application."
  },
  {
    "objectID": "projects/MDS/index.html#principal-coordinate-analysis",
    "href": "projects/MDS/index.html#principal-coordinate-analysis",
    "title": "Multidimensional Scaling",
    "section": "Principal Coordinate Analysis",
    "text": "Principal Coordinate Analysis\nPrincipal Coordinate Analysis (PCoA), often interchangeable with metric MDS in many contexts, also focuses on preserving the actual distances between pairs of objects in the dimension-reduced space. The approach starts with a distance matrix and applies eigenvalue decomposition to derive the principal coordinates. PCoA can handle any symmetric distance matrix and thus provides a flexible method for dimensionality reduction. Primary distinction to Principal Component Analysis (PCA) is that PCoA uses distance matrix of the items while PCA utilizes covariance matrix of the exploratory variables.\nTo grasp the link between PCoA and PCA, it’s essential to revisit some foundational concepts and introduce new insights from these methods:\n\nPrincipal Components as Linear Combinations:  \nEach principal component is a linear combination of the original variables, represented by the formula: \\[Z_i = a_{i1}X_1 + a_{i2}X_2 + \\dots + a_{ip}X_p\\] This showcases how principal components simplify the data by transforming the original variables into new axes that capture the majority of the data’s variance.\nCovariance and Similarity Matrices:  \nWhen the variables have zero mean, the covariance matrix \\(C\\) is calculated as: \\[\\mathbf{C} = \\frac{\\mathbf{X}^T\\mathbf{X}}{n-1}\\] PCA uses this covariance matrix to derive eigenvalues and eigenvectors that define the principal components. In contrast, PCoA uses a matrix \\(\\mathbf{S} = \\mathbf{XX}^T\\), which represents similarities or dissimilarities, making it suitable for data that doesn’t naturally fit into a covariance analysis framework.\nCalculating and Interpreting Components:  \nThe principal components are obtained through: \\[\\mathbf{Z} = \\mathbf{XA}'\\] where \\(\\mathbf{Z}\\) includes the principal component scores, and \\(\\mathbf{A}'\\) is the matrix of eigenvectors’ transpose. This transformation allows for the original data matrix \\(\\mathbf{X}\\) to be reconstructed from \\(\\mathbf{Z}\\) and \\(\\mathbf{A}\\): \\[\\mathbf{X} = \\mathbf{ZA}\\] Unlike PCA, PCoA often scales components differently, affecting how they’re interpreted compared to PCA."
  },
  {
    "objectID": "projects/MDS/index.html#nonmetric-multidimensional-scaling",
    "href": "projects/MDS/index.html#nonmetric-multidimensional-scaling",
    "title": "Multidimensional Scaling",
    "section": "Nonmetric Multidimensional Scaling",
    "text": "Nonmetric Multidimensional Scaling\nThe non-metric Multidimensional Scaling (nMDS) method was initially conceptualized by Shepard in the early 1960s and was further refined by Kruskal shortly thereafter. This approach involves arranging a set of \\(n\\) objects based on their dissimilarities, \\(\\delta_{rs}\\) into a configuration of \\(n\\) points within typically Euclidean space. Each object corresponds to a point such that the spatial layout reflects the dissimilarities as accurately as possible.\nNMDS relaxes the requirement of preserving the exact distances and focuses instead on preserving the rank order of distances. This method is particularly useful when the distances do not adhere strictly to a metric scale or when the exact magnitudes of distances are less important than the order of these distances. The primary aim here is to maintain the ordinal relationship of the distances, such as ensuring that if object A is closer to object B than to object C in high-dimensional space, the same should be true in the reduced space. This involves an iterative process where the configuration is adjusted to minimize a stress function shown in the diagnositc section."
  },
  {
    "objectID": "projects/MDS/index.html#algorithm",
    "href": "projects/MDS/index.html#algorithm",
    "title": "Multidimensional Scaling",
    "section": "Algorithm",
    "text": "Algorithm\nThe algorithm for recovering coordinates from distances between pairs of points is as follows:\n\nForm matrix \\(\\mathcal{A} = -\\frac{1}{2} \\delta_{rs}^2\\)\nForm matrix \\(\\mathcal{B} = \\mathcal{HAH}\\), where \\(\\mathcal{H}\\) is the centering matrix \\(\\mathcal{H} = \\mathcal{I}- n^{-1} \\mathbf{1}_n\\mathbf{1}_n^T\\), where \\(\\mathcal{I}\\) is the identity matrix, and \\(\\mathbf{1}_n\\) is a vector of ones.\nFind the spectral decomposition of \\(\\mathcal{B}, \\mathcal{B} = \\mathcal{V} \\Lambda \\mathcal{V}^T\\), where \\(\\Lambda\\) is the diagonal matrix formed from eigenvalues of \\(\\mathcal{B}\\), and \\(\\mathcal{V}\\) is the matrix of corresponding eigenvectors.\nIf the points were originally in a \\(p\\)-dimensional space, the first \\(p\\) eigenvalues of \\(\\mathcal{B}\\) are nonzero and the remaining \\(n-p\\) are zero. Discard these from \\(\\Lambda\\) (rename as \\(\\Lambda_1\\), and discard the corresponding eigenvalues from \\(\\mathcal{V}\\) (rename as \\(\\mathcal{V}_1\\)).\nFind \\(\\mathcal{X} = \\mathcal{V}_1 \\Lambda_1^{1/2},\\) and coordinates of the points are given by the row of \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "projects/MDS/index.html#standardized-residual-sum-of-squares",
    "href": "projects/MDS/index.html#standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Standardized Residual Sum of Squares",
    "text": "Standardized Residual Sum of Squares\n\\[STRESS(q) = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^{(q)} - \\hat{d}_{rs}^{(q)}\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}^{(q)}\\right)^2}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}^{(q)}\\) is the distance between object \\(r\\) and object \\(s\\) for \\(q\\) configuration; and\n\\(\\hat{d}_{rs}^{(q)}\\) is the regression of \\(d_{rs}^{(q)}\\) and \\(\\delta_{rs}\\)(data distance).\n\nKruskall (1978) suggests the stress be informally interpreted according to the guidelines:\n\nStress Value Category\n\n\nStress\nGoodness of fit\n\n\n\n\n20%\nPoor\n\n\n10%\nFair\n\n\n5%\nGood\n\n\n2.5%\nExcellent\n\n\n0%\nPerfect"
  },
  {
    "objectID": "projects/MDS/index.html#squared-standardized-residual-sum-of-squares",
    "href": "projects/MDS/index.html#squared-standardized-residual-sum-of-squares",
    "title": "Multidimensional Scaling",
    "section": "Squared Standardized Residual Sum of Squares",
    "text": "Squared Standardized Residual Sum of Squares\n\\[SSTRESS = \\left\\{\\dfrac{\\sum\\sum_{r &lt; s}\\left(d_{rs}^2 - \\hat{d}_{rs}^2\\right)^2}{\\sum\\sum_{r &lt; s} \\left(d_{rs}\\right)^4}\\right\\}^{1/2}\\] where\n\n\\(d_{rs}\\) is the distance between object \\(r\\) and object \\(s\\) for the configuration; and\n\\(\\hat{d}_{rs}\\) is the regression of \\(d_{rs}\\) and \\(\\delta_{rs}\\)(data distance)."
  },
  {
    "objectID": "projects/MDS/index.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "href": "projects/MDS/index.html#metric-multidimensional-scaling-example-barangays-of-iligan-city",
    "title": "Multidimensional Scaling",
    "section": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City",
    "text": "1. Metric Multidimensional Scaling Example: Barangays of Iligan City\n\n\n\n\n\nMap of Iligan\n\n\n\n\nThe goal for this data is to visualize and understand the spatial relationships among different barangays based on their geographic distances. We want to see if MDS accurately captures the original distances between each barangay in Iligan City. After performing MDS, we expect that Barangay Buru-un will be shown to be closer to Barangay Ditucalan or Maria Cristina, as opposed to its distance from Rogongon. The table showcases the list of barangays in Iligan City\n\nList of Barangays\n\n\nAbuno\nHinaplanon\nPala-o\nSaray\n\n\nAcmac\nHindang\nPanoroganan\nSuarez\n\n\nBagong Silang\nKabacsanan\nPoblacion\nTambacan\n\n\nBonbonon\nKalilangan\nPuga-an\nTibanga\n\n\nBunawan\nKiwalan\nRogongon\nTipanoy\n\n\nBuru-un\nLanipao\nSan Miguel\nTomas L. Cabili\n\n\nDalipuga\nLuinab\nSan Roque\nUpper Tominobo\n\n\nDel Carmen\nMahayahay\nSanta Elena\nTubod\n\n\nDigkilaan\nMainit\nSanta Filomena\nUbaldo Laya\n\n\nDitucalan\nMandulog\nSantiago\nUpper Hinaplanon\n\n\nDulag\nMaria Cristina\nSanto Rosario\nVilla Verde\n\n\n\n\nData\nThe data for this application includes the coordinates of the barangay halls, which represent the locations of each barangay. The coordinates were collected using web scraping.\n\nbarangays &lt;- read.delim(\"Barangays.txt\", header = FALSE)\ncoordinates &lt;- data.frame()\nfor(i in 1:nrow(barangays)){\n  barangay &lt;- tolower(barangays[i, ])\n  website &lt;- glue(\"https://www.philatlas.com/mindanao/r10/iligan/{barangay}.html\")\n  h &lt;- read_html(website)\n  latitude &lt;- html_elements(h, xpath='.//span[@id=\"latitude\"]') %&gt;% \n    html_text()\n  longitude &lt;- html_elements(h, xpath='.//span[@id=\"longitude\"]') %&gt;% \n    html_text()\n  c &lt;- cbind.data.frame(barangay, \n                        latitude = as.numeric(latitude), \n                        longitude = as.numeric(longitude))\n  coordinates &lt;- rbind.data.frame(coordinates, c)\n}\nknitr::kable(head(coordinates))\n\n\n\n\nHead of Iligan City Barangays Data\n\n\nbarangay\nlatitude\nlongitude\n\n\n\n\nabuno\n8.1846\n124.2571\n\n\nacmac\n8.2740\n124.2649\n\n\nbagong-silang\n8.2418\n124.2520\n\n\nbonbonon\n8.2667\n124.2902\n\n\nbunawan\n8.3023\n124.3028\n\n\nburu-un\n8.1872\n124.1688\n\n\n\n\n\nThe table above shows the first six of the barangays only. Also, the table illustrates the respective latitude and longitude to each of the barangays in Iligan city. Now that we are done the web scraping part, we proceed to the checking of assumptions.\n\n\nChecking Assumptions\n\n# Missing Data\ncoordinates %&gt;% sapply(., function(x) sum(is.na(x)))\n\n barangay  latitude longitude \n        0         0         0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the three variables. Therefore, there is no need to take any further steps to handle any missing data.\n\n# Duplicates\ncoordinates %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 3\n# ℹ 3 variables: barangay &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\nThe output of the code above indicates that the data contains no duplicates. This suggests that the coordinates of the barangays are unique, as expected.\n\n\nCalculating Haversine Distance\nThe Haversine formula calculates the shortest distance between two points on the surface of a sphere, given their longitudes and latitudes. This is particularly useful in geography for finding the distance between two locations on Earth, as it accounts for the Earth’s curvature.\nWhen we provide the coordinates (latitude and longitude) of two barangays, the Haversine formula helps determine the great-circle (or orthodromic) distance between them. This distance represents the shortest path between the two points along the surface of the sphere, rather than a straight line through the Earth’s interior. The formula of Haversine is\n\\[d = 2r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\phi_2 - \\phi_1}{2}\\right) + \\cos(\\phi_1) \\cos(\\phi_2) \\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right)}\\right)\\]\nwhere:\n\n\\(\\phi_1, \\phi_2\\) are the latitudes of the two points in radians,\n\\(\\lambda_1, \\lambda_2\\) are the longitudes of the two points in radians,\n\\(r\\) is the radius of the Earth (approximately 6371 kilometers or 3959 miles),\n\\(d\\) is the distance between the two points.\n\nThe Haversine formula can be utilized in the distm() function, which takes latitude and longitude as parameters. Set the fun parameter to distHaversine to apply the formula mentioned above. The output is in matrix form, with the distance metric given in meters.\n\ndistances &lt;- distm(coordinates[, c(\"longitude\", \"latitude\")], \n                   fun = distHaversine)\ndistances &lt;- distances/1000 #convert to km\ncolnames(distances) &lt;- coordinates$barangay\nrownames(distances) &lt;- coordinates$barangay\n\ndistances[1:5, 1:5]\n\n                  abuno    acmac bagong-silang bonbonon   bunawan\nabuno          0.000000 9.988996      6.392220 9.840034 14.036344\nacmac          9.988996 0.000000      3.855926 2.903147  5.230182\nbagong-silang  6.392220 3.855926      0.000000 5.039188  8.756445\nbonbonon       9.840034 2.903147      5.039188 0.000000  4.199009\nbunawan       14.036344 5.230182      8.756445 4.199009  0.000000\n\n\nAbove is the output of the first five rows and five columns of the distance matrix. As expected, the distance of any point to itself is zero. Additionally, the distance from abuno to acmac, which is 9.988996, is the same as from acmac to abuno. This symmetry confirms that the matrix is symmetric.\nNow, we will proceed to find the optimal dimension that best represents our data. This involves analyzing the stress values and visualizing the corresponding plots to determine which dimensionality reduction provides the clearest and most informative representation of the underlying structure of our dataset.\nThe code below defines a function based on the steps outlined in the Procedure section. This function has two parameters: distance, which is the distance matrix, and dimension, which specifies the number of dimensions the user wishes to explore. The function outputs the stress values for each dimension, along with corresponding plots to help determine the most suitable dimension.\n\noptimal_dimension &lt;- function(distance, dimension){\n  stress &lt;- data.frame()\n  for(i in 1:dimension){\n    mds_result &lt;- cmdscale(distance, k = i)\n    \n    # Calculate the distances in the reduced space\n    fitted_distance &lt;- dist(mds_result)\n    fitted_distance &lt;- as.vector(as.matrix(fitted_distance))\n    original_distance &lt;- as.vector(as.matrix(distance))\n    \n    reg_output &lt;- lm(fitted_distance ~ original_distance)\n    predicted_distance &lt;- fitted(reg_output)\n    \n    # Calculating stress using the modified formula\n    stress_value &lt;- sqrt(sum((fitted_distance - predicted_distance)^2) / \n                           sum(fitted_distance^2))\n    s &lt;- cbind.data.frame(dim = i, stress = stress_value)\n    stress &lt;- rbind.data.frame(stress, s)\n  }\n  p &lt;- ggplot(stress, aes(x=dim, y=stress)) +\n    geom_line() + geom_point() + scale_color_brewer(palette=\"Paired\") +\n    labs(x = \"Dimensions\", title = \"Stress Plot\") +\n    theme_minimal()\n  print(p)\n  \n  return(stress)\n}\n\nTo find the optimal dimension for our analysis, we use the elbow method. This method involves looking at a line plot and finding the ‘elbow,’ a point where the benefits of adding more dimensions start to diminish significantly. Identifying this point helps us select the dimensionality that provides the most meaningful reduction in complexity without significant loss of information.\n\noptimal_dimension(distances, 10)\n\n\n\n\nBarangays in Iligan City Stress Plot\n\n\n\n\n   dim       stress\n1    1 2.263671e-01\n2    2 8.481692e-08\n3    3 7.949367e-08\n4    4 7.949368e-08\n5    5 7.949369e-08\n6    6 7.949369e-08\n7    7 7.949369e-08\n8    8 7.949369e-08\n9    9 7.949369e-08\n10  10 7.949369e-08\n\n\nThe stress plot above illustrates how stress diminishes as the number of dimensions increases. While the elbow in the plot might be subtle due to the small stress values, it appears around two dimensions. This suggests that adding more dimensions beyond two does not significantly enhance the data representation. Therefore, based on this plot, using two dimensions is likely adequate for capturing the structure of the data without adding unnecessary complexity or overfitting. The stress value for two dimensions is near \\(0\\%\\), indicating a perfect goodness of fit as suggested by Kruskal. This extremely low stress value demonstrates an excellent representation of the data in the chosen dimensional space. The stress values for dimensions three to ten are also near \\(0\\%\\), but selecting a dimension within that range becomes challenging to visualize effectively. Higher dimensions can complicate the interpretation without providing significant additional clarity.\nHaving identified the optimal dimension that best represents our data, we will now proceed to perform MDS using two dimensions. To carry out metric MDS in R, we utilize the cmdscale() function, providing it with the distance matrix and specifying two as the number of dimensions.\n\nmds_result &lt;- cmdscale(distances, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City (Inverted)\n\n\n\n\nThe figure above presents an inverted version of the barangay locations in Iligan City. To match the exact location as the original map, we simply multiply the coordinates by -1. This sign reversal does not change the distances between the barangays based on the two dimensions, and the new dimension is therefore just as satisfactory as the original one.\n\nmds_df$V1 &lt;- -1 * mds_df$V1\nmds_df$V2 &lt;- -1 * mds_df$V2 \nggplot(mds_df, aes(V1, V2)) +\n  geom_point() +\n  geom_text(aes(label = rownames(distances)), size = 3.5, vjust=1) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", title = \"MDS Plot\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Barangays in Iligan City\n\n\n\n\nThe points in Figure 4 now accurately represent the locations of barangays in Iligan City. As stated earlier, our objective was to verify whether Barangay Buru-un is closer to Barangay Maria Cristina or Barangay Ditucalan than to Barangay Rogongon. The MDS has effectively captured this relationship, as well as the distances between other barangays. Using two dimensions, the MDS has successfully mapped the precise distances among the barangays in Iligan City."
  },
  {
    "objectID": "projects/MDS/index.html#principal-coordinate-analysis-example-sparrow-data",
    "href": "projects/MDS/index.html#principal-coordinate-analysis-example-sparrow-data",
    "title": "Multidimensional Scaling",
    "section": "2. Principal Coordinate Analysis Example: Sparrow Data",
    "text": "2. Principal Coordinate Analysis Example: Sparrow Data\nAfter a severe storm on February 1, 1898, a number of moribund sparrows were taken to Hermon Bumpus’ biological laboratory at Brown University, Rhode Island. Subsequently, about half of the birds died, and Bumpus saw this as an opportunity to see whether he could find any support for Charles Darwin’s theory of natural selection. To this end, he made eight morphological measurements on each bird and also weighed the birds. The results for five of the measurements are shown below, for females only.\nThe dataset contains five morphological measurements of the female sparrows where\n\ntotal.length - measures from the tip of the beak to the end of the tail in *mm*\nalar.extent - the distance from tip to tip of extended wings\nbeak.and.head - length of beak and head (from the tip of the beak to the occiput)\nhumerus - length of humerus\nsternum - length of sternum\nsurvival - the survival status of the bird, true if alive, or false otherwise\n\nAt the conclusion of our PCoA modeling, we will compare the results with those obtained from PCA. This comparison will help us understand the differences in how each method captures and represents the underlying data structure.\n\nsparrow_data &lt;- read_excel(\"sparrow.xlsx\")\nsparrow &lt;- sparrow_data[, -c(1,7)]\nknitr::kable(head(sparrow_data), caption = \"First six row of female sparrows\")\n\n\n\n\nFirst six row of female sparrows\n\n\n\n\n\n\n\n\n\n\n\nBird\ntotal.length\nalar.extent\nbeak.and.head\nhumerus\nsternum\nsurvival\n\n\n\n\n1\n156\n245\n31.6\n18.5\n20.5\nT\n\n\n2\n154\n240\n30.4\n17.9\n19.6\nT\n\n\n3\n153\n240\n31.0\n18.4\n20.6\nT\n\n\n4\n153\n236\n30.9\n17.7\n20.2\nT\n\n\n5\n155\n243\n31.5\n18.6\n20.3\nT\n\n\n6\n163\n247\n32.0\n19.0\n20.9\nT\n\n\n\n\n\nFor PCoA purpose, we remove the first column and the second column, and this can be achieved by the following code\n\nsparrow &lt;- sparrow_data[, -c(1,7)]\n\nNow, we proceed by checking the assumptions.\n\nChecking Assumptions\n\n# Missing Data\nsparrow %&gt;% sapply(., function(x) sum(is.na(x)))\n\n total.length   alar.extent beak.and.head       humerus       sternum \n            0             0             0             0             0 \n\n\nThe zero value from the result above suggests that each of the independent variables have no missing datas.\n\n# Duplicates\nsparrow %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# A tibble: 0 × 5\n# ℹ 5 variables: total.length &lt;dbl&gt;, alar.extent &lt;dbl&gt;, beak.and.head &lt;dbl&gt;,\n#   humerus &lt;dbl&gt;, sternum &lt;dbl&gt;\n\n\nThe result from the code suggests that no morphological measurements of the female sparrows are the same. Now we are done checking the assumptions, we now proceed to calculating the distance matrix of the data.\n\n\nCalculating Euclidean distance\nSince we are about to model a PCoA example, the distance that is to be utilized is euclidean. This can be performed in R by using the dist() function that feeds the dataset and set the method to euclidean which indicates the usage of Euclidean distance.\n\ndist_matrix &lt;- dist(sparrow, method = \"euclidean\")\ndist_matrix %&gt;% as.matrix() %&gt;% .[1:5,1:5]\n\n         1        2        3        4        5\n1 0.000000 5.622277 5.863446 9.550916 2.249444\n2 5.622277 0.000000 1.615549 4.201190 3.491418\n3 5.863446 1.615549 0.000000 4.081666 3.657868\n4 9.550916 4.201190 4.081666 0.000000 7.360706\n5 2.249444 3.491418 3.657868 7.360706 0.000000\n\n\nNow we are done calculating the distance matrix, and checking the assumptions, the preparations for modeling is now complete. Now, we find the optimal dimension of the dataset.\n\n\nModeling\nWe set the dimension to 5, as a similar process to PCA.\n\noptimal_dimension(dist_matrix, 5)\n\n\n\n\nSparrows Stress Plot\n\n\n\n\n  dim       stress\n1   1 1.437590e-01\n2   2 2.456834e-02\n3   3 1.041855e-02\n4   4 2.913686e-03\n5   5 5.677040e-16\n\n\nFigure 5 illustrates how the stress decreases as the number of dimensions increases from one to five. There’s a noticeable drop in stress when moving from one to two dimensions, indicating that two dimensions capture much more of the data’s structure compared to just one. The stress continues to reduce slightly as we add a third dimension, but the rate of decline slows down significantly.\nFrom the plot, we can see an ‘elbow’ forming between the second dimensions. This suggests that adding more dimensions beyond two brings minimal improvement in stress reduction.\n\nmds_result &lt;- cmdscale(dist_matrix, k = 2)\nmds_df &lt;- as.data.frame(mds_result)\nggplot(mds_df, aes(x = V1, y = V2)) +\n  geom_point(aes(color = sparrow_data$survival)) +\n  labs(x = \"Dimension 1\", y = \"Dimension 2\", \n       title = \"PCoA Plot\", color = \"survival\") +\n  theme_minimal()\n\n\n\n\nMDS plot of Sparrows\n\n\n\n\nFigure 6 shows the PCoA results for a dataset on sparrows, visualized in a two-dimensional space. The plot categorizes sparrows based on a survival attribute, with the categories represented by two different colors: red (‘F’) for those that did not survive and blue (‘T’) for those that did survive.\nThe horizontal axis (Dimension 1) and the vertical axis (Dimension 2) represent the two principal coordinates derived from the dataset. There isn’t a clear, distinct clustering pattern where one group is entirely separate from the other. Both survived (‘T’) and not survived (‘F’) sparrows are spread throughout the plot. This suggests that based solely on these two principal coordinates, there isn’t much differentiation in the dataset that correlates strongly with the survival outcomes.\nWe compare it to PCA with two as the remained principal components,\n\nsparrow_pca &lt;- prcomp(sparrow, scale = TRUE)\nautoplot(sparrow_pca, data = sparrow_data, colour = 'survival', label.size = 3) +\n  labs(title = \"PCA Plot\") +\n  theme_minimal()\n\n\n\n\nPCA plot of Sparrows\n\n\n\n\nFigure 7 shows the PCA plot of sparrows, visualized in a space defined by the first two principal components: PC1 and PC2. This plot also categorizes the sparrows based on their survival status, with red dots (‘F’) representing those that did not survive and blue dots (‘T’) for those that did survive.\nThe first principal component (PC1) explains a significant portion of the variance in the dataset at 72.32%, indicating that it captures a major underlying pattern or trend related to the features measured. PC2 explains an additional 10.63% of the variance, providing a secondary perspective on the data.\nSimilar to the PCoA plot, the PCA plot does not show clear, distinct clusters of survival outcomes. Both survival and non-survival sparrows are scattered throughout the PCA space. Both PCA and PCoA plots show an overlap of survival statuses with no clear separation between the ‘F’ and ‘T’ categories. This suggests that the survival outcome is not strongly linearly correlated with the principal components derived from the data in both analyses.\nPCA, which focuses more on maximizing variance and often reveals more defined patterns and relationships in the data, still shows an overlap similar to PCoA. This suggests that the variables influencing survival might be complex and not easily separable by linear methods like PCA and PCoA.\nWhile PCoA is more about preserving the distance or dissimilarity accurately in a lower-dimensional space, PCA aims to capture the variance in the data. The similar patterns of overlap in both analyses indicate that further investigation might be needed, perhaps with different analytical approaches or additional data features, to better understand the factors affecting survival."
  },
  {
    "objectID": "projects/MDS/index.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "href": "projects/MDS/index.html#nonmetric-multidimensional-scaling-example-doubs-fish-data",
    "title": "Multidimensional Scaling",
    "section": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data",
    "text": "3. Nonmetric Multidimensional Scaling Example: Doubs Fish Data\nIn this nMDS example, the will use the Doubs Fish data, the Fish community composition of the Doubs River in France. They come from Verneaux’s PhD thesis (1973), where he proposed to use fish species to characterize ecological zones along European rivers and streams. The values in ‘Doubs.fish’ are counts of individuals of each of 27 species observed in a set of 30 sites located along the 453 km long Doubs River, France. The data contains the following:\n\nenv - is a data frame with 30 rows (sites) and 11 environmental variables.\nfish - is a data frame with 30 rows (sites) and 27 fish species.\nxy - is a data frame with 30 rows (sites) and 2 spatial coordinates.\nspecies - is a data frame with 27 rows (species) and 4 columns (names)\n\nThe goal here is to visualize the fish abundance at different sites. It can reveal natural groupings (clusters) of sites with similar fish communities This can be crucial for managing fish populations.\nThe data can be access by performing the data() function and feed doubs in it. The site 8 is removed in the data since that site have no species.\n\ndata(doubs)\nspe &lt;- doubs$fish[-8,]\nspe %&gt;% .[1:6, 1:12]\n\n  Cogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi\n1    0    3    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0\n4    0    4    5    5    0    0    0    0    0    1    0    0\n5    0    2    3    2    0    0    0    0    5    2    0    0\n6    0    3    4    5    0    0    0    0    1    2    0    0\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\nab &lt;- table(unlist(spe))\npar(mfrow = c(1, 2))\nbarplot(ab, las = 1, col = grey(5:0/5), main = \"Abundance Distribution\",\n        xlab = \"Abundance class\", ylab = \"Frequency\")\nsite.pre &lt;- rowSums(spe &gt; 0)\nbarplot(site.pre, main = \"Species richness\",\n        xlab = \"Sites\", ylab = \"Number of species\",\n        col = \"grey \", las = 1)\n\n\n\n\nAbundance Distribution and Species Richness\n\n\n\n\nThe plot on the left above reveals that the majority of species observations across various sites register an abundance of zero. This is expected given the large number of sites surveyed—–30 in total—–with many sites not recording the presence of different fish species. There are also sites where the count of fish species present is five, with decreasing frequencies noted for higher abundance classes. This pattern highlights that while some sites exhibit richer biodiversity, many others have little to no fish presence.\nThe ‘Species Richness’ plot on the right side illustrates the number of different species found at each site, showing considerable variability. Some sites have a high species richness, nearing 25 species, while others have much lower counts. This variability might suggest differences in environmental conditions, habitat quality, or other ecological factors influencing species distribution across the sites.\n\nsum(spe == 0)\n\n[1] 408\n\n\n\nsum(spe == 0)/(nrow(spe) * ncol(spe))\n\n[1] 0.5210728\n\n\nOver half of the dataset is made up of zeros, which is quite high but not unusual for species abundance data. The presence of many zeros can cause a ‘double zero problem,’ where the absence of species at different sites can inflates their similarity based on what they both lack, rather than what they have. This means two sites might seem similar because they are both missing certain species, but this doesn’t necessarily reflect true ecological similarity. Ideally, we want the presence of species, rather than their absence, to inform how similar two sites are.\nTo counter this double zero issue, we’ll transform the species data. Pierre Legendre and Gallagher in 2001 recommended five possible pre-transformations for species data, four of which can be performed using the decostand() function from the vegan package.\nOne effective transformation is the Hellinger transformation, which represents species abundances as the square root of their relative abundance at each site, as suggested by Borcard, Gillet, and Legendre (2011). This approach addresses the problem with double zeros effectively. We will use this transformation on our fish abundance dataset to ensure our similarity assessments are more ecologically meaningful. And we will compare this without the transformation to determine if there is changes in the output.\n\nspe.hel &lt;- decostand(spe, method = \"hellinger\")\n\n\n\nChecking Assumptions\n\n# Missing Data\nspe %&gt;% sapply(., function(x) sum(is.na(x)))\n\nCogo Satr Phph Neba Thth Teso Chna Chto Lele Lece Baba Spbi Gogo Eslu Pefl Rham \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \nLegi Scer Cyca Titi Abbr Icme Acce Ruru Blbj Alal Anan \n   0    0    0    0    0    0    0    0    0    0    0 \n\n# Duplicates\nspe %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup() \n\n# A tibble: 0 × 27\n# ℹ 27 variables: Cogo &lt;dbl&gt;, Satr &lt;dbl&gt;, Phph &lt;dbl&gt;, Neba &lt;dbl&gt;, Thth &lt;dbl&gt;,\n#   Teso &lt;dbl&gt;, Chna &lt;dbl&gt;, Chto &lt;dbl&gt;, Lele &lt;dbl&gt;, Lece &lt;dbl&gt;, Baba &lt;dbl&gt;,\n#   Spbi &lt;dbl&gt;, Gogo &lt;dbl&gt;, Eslu &lt;dbl&gt;, Pefl &lt;dbl&gt;, Rham &lt;dbl&gt;, Legi &lt;dbl&gt;,\n#   Scer &lt;dbl&gt;, Cyca &lt;dbl&gt;, Titi &lt;dbl&gt;, Abbr &lt;dbl&gt;, Icme &lt;dbl&gt;, Acce &lt;dbl&gt;,\n#   Ruru &lt;dbl&gt;, Blbj &lt;dbl&gt;, Alal &lt;dbl&gt;, Anan &lt;dbl&gt;\n\n\nMDS can be performed in R using the metaMDS() function from the vegan package, which is quite user-friendly. This function can handle both raw data and precomputed distance matrices. We will use the Bray-Curtis dissimilarity for this dataset. The Bray-Curtis dissimilarity is a statistic used to quantify the compositional dissimilarity between two different sites or samples based on counts or measurements of species abundance. It is particularly useful in ecology and environmental science for comparing the composition of different ecological communities.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe, distance=\"bray\")\n\nRun 0 stress 0.0747782 \nRun 1 stress 0.112424 \nRun 2 stress 0.08930115 \nRun 3 stress 0.1104319 \nRun 4 stress 0.08987062 \nRun 5 stress 0.08886178 \nRun 6 stress 0.08886178 \nRun 7 stress 0.1219299 \nRun 8 stress 0.1209556 \nRun 9 stress 0.07376216 \n... New best solution\n... Procrustes: rmse 0.0193953  max resid 0.09464088 \nRun 10 stress 0.07478342 \nRun 11 stress 0.07506678 \nRun 12 stress 0.08797377 \nRun 13 stress 0.1118912 \nRun 14 stress 0.07477838 \nRun 15 stress 0.1123498 \nRun 16 stress 0.08696388 \nRun 17 stress 0.09157401 \nRun 18 stress 0.1125696 \nRun 19 stress 0.1119532 \nRun 20 stress 0.1124389 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     4: no. of iterations &gt;= maxit\n    15: stress ratio &gt; sratmax\n     1: scale factor of the gradient &lt; sfgrmin\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe \nDistance: bray \n\nDimensions: 2 \nStress:     0.07376216 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 9 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe' \n\n\nThe data will be sufficiently explained in two dimensions, which suggests that the resulting plot or ordination will map the data onto two principal axes. The stress value reported is 0.07376216, which is a measure of the fit quality of the NMDS configuration. A stress value below 0.1, is typically considered an excellent fit, basing on the category Kruskall presented.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nNMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe NMDS plot provided has a stress level of 0.074 indicating a reliable fit, visually maps the relationships between different sites, numbered on the plot, and the fish species found there marked in red. The plot shows how some sites cluster together, suggesting they share similar species compositions—likely due to similar environmental conditions or close geographical proximity. Other sites stand out as more isolated, indicating unique ecological characteristics or distinct species that might not be present in more central or clustered locations. Mjority of the fishes are clustered in the site 29, just like what is shown in the descriptive stat eralier. The Teso species solos the site 15, indicating maybe the fish is a territorial or there is an environmental variable there that only the said fish is interested.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot\n\n\n\n\nA Shepard plot is a diagnostic tool used in nMDS to assess the quality of the MDS solution. The Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance \\((R^2 &gt; 0.995)\\) highlighting a high goodness of fit. Both the non-metric and linear fits show high \\(R^2\\) values, suggesting that the MDS model has been very effective at preserving the distances between data points when reducing dimensionality. The non-metric fit is particularly effective, which is typical as non-metric MDS aims to preserve the rank order rather than the actual values of distances, making it more flexible in handling varied data scales and distributions. The effectiveness of the non-metric MDS in this case suggests that if the data contained non-linear relationships or was from different scales of measurement, the method still managed to capture and represent these complexities accurately.\nWe now proceed in performing nMDS for the hellinger transformed of the dataset.\n\nset.seed(2024)\nspe.nmds &lt;- metaMDS(spe.hel, distance=\"bray\")\n\nRun 0 stress 0.06746136 \nRun 1 stress 0.1024773 \nRun 2 stress 0.1016353 \nRun 3 stress 0.1024777 \nRun 4 stress 0.1016351 \nRun 5 stress 0.1067558 \nRun 6 stress 0.1067557 \nRun 7 stress 0.102477 \nRun 8 stress 0.09577038 \nRun 9 stress 0.06742454 \n... New best solution\n... Procrustes: rmse 0.008564238  max resid 0.04061037 \nRun 10 stress 0.06747835 \n... Procrustes: rmse 0.01111845  max resid 0.05270437 \nRun 11 stress 0.08323628 \nRun 12 stress 0.1058012 \nRun 13 stress 0.07528549 \nRun 14 stress 0.06743465 \n... Procrustes: rmse 0.005265951  max resid 0.02493877 \nRun 15 stress 0.06742479 \n... Procrustes: rmse 0.00010268  max resid 0.0004864955 \n... Similar to previous best\nRun 16 stress 0.1089367 \nRun 17 stress 0.1016995 \nRun 18 stress 0.06742515 \n... Procrustes: rmse 0.0002102026  max resid 0.0009945225 \n... Similar to previous best\nRun 19 stress 0.0957702 \nRun 20 stress 0.06742439 \n... New best solution\n... Procrustes: rmse 6.295875e-05  max resid 0.0002982595 \n... Similar to previous best\n*** Best solution repeated 1 times\n\nspe.nmds\n\n\nCall:\nmetaMDS(comm = spe.hel, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe.hel \nDistance: bray \n\nDimensions: 2 \nStress:     0.06742439 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 20 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe.hel' \n\n\nThe output above suggests that dimension two is enough to represent the data. This reduction allows for easier visualization and interpretation of the data’s underlying patterns. A stress value of 0.06742439 indicates an excellent fit of the NMDS model to the data, suggesting that the two-dimensional representation is a reliable depiction of the species dissimilarities. The stress value of Hellinger transformed data is lower than that of without transformation, indicating that the transformed model is much better.\n\nplot(spe.nmds, type=\"t\", main=paste(\"NMDS/Bray - Stress =\", \n     round(spe.nmds$stress,3)))\n\n\n\n\nHellinger Transformed of NMDS biplot of a Bray–Curtis dissimilarity matrix of the fish abundance data.\n\n\n\n\nThe numbered points represent different sites, and the red text labels specific fish species found at those sites. Similar to the earlier plot, sites that are close together likely have similar species compositions, and the species names near each cluster give insight into the distinct ecological characteristics of those clusters. This plot includes more specific information about the species present at each site compared to the earlier plot. This detail allows for a more in-depth understanding of the ecological and biological diversity across the sampled sites. Both plots have low stress values, but this one is slightly lower (0.067 compared to 0.074), indicating a slightly better fit in the representation of dissimilarities.\n\nstressplot(spe.nmds, main=\"Shepard plot\")\n\n\n\n\nShepard plot of Hellinger Transformed\n\n\n\n\nThe non-metric fit, with an \\(R^2\\) of 0.995, indicates an excellent adherence to the rank order of the original dissimilarities. This value shows that the nMDS model has successfully captured the underlying structure of the data that preserves the relative dissimilarities among observations. The linear fit has an \\(R^2\\) of 0.981, which is also very high, suggesting that the distances in the MDS space linearly correlate well with the observed dissimilarities. Compared to the Shepard PLot without performing Hellinger transformation, this is better."
  },
  {
    "objectID": "projects/Ordered Logit/index.html",
    "href": "projects/Ordered Logit/index.html",
    "title": "Ordinal Regression Analysis",
    "section": "",
    "text": "Introduction\nOrdinal outcomes are common in research studies and real-world applications. These outcomes are not binary and instead fall into multiple ordered categories. It is essential to understand and model such outcomes for decision-making purposes.\nTo analyze and model the relationships between one or more predictor variables and an ordinal response variable, statisticians use ordinal logistic regression. This statistical model is an extension of logistic regression, which can handle the dependent variable with two or more ordered categories. On the other hand, multinomial logistic regression is an extended version of binary logistic regression. However, it does not preserve the ranking information in the dependent variable when returning the information on the contribution of each independent variable.\nThe difference in response scales might require different equations to model them correctly. For example, the ordered scale needs a different function than the unordered scale. In the case of the ordered scale, it is more useful to work with cumulative probabilities (McCullagh and Nelder, 1989). For instance, the cumulative probability of “agreeing” includes the probabilities of “agreeing” and “strongly agreeing” combined. However, this approach only makes sense if the order of categories is clear.\n\n\nOrdinal Logistic Regression\n\nBackground\nPeter McCullagh - A Northern Irish-born American statistician. Distinguished Service Professor in the Department of Statistics at the University of Chicago.\n\nThesis - Analysis of Ordered Categorical Data (1977)\n\n\n\nMotivation of the study\nMcCullagh’s motivation for the ordinal model is to develop statistical methods that even after combining levels of responses, the validity of conclusion will not be affected by the new number of responses. The amalgamation of the response categories in this way will normally reduce the available information, change the estimate, the attained significance level and so on. The important point is that the same parameter is being measured however many categories are combined (McCullagh, 1980)\n\n\nRegression Model\nLet \\(Y\\) denote the ordered response with \\(k\\) categories, with \\(k \\geq 3\\). Then \\(P(Y \\leq j)\\) is the cumulative probability of \\(Y\\) less than or equal to a specific category \\(j = 1, ..., k-1\\).\nThe model can be defined as \\[log\\left\\{\\frac{P(Y \\leq j)}{P(Y &gt; j)}\\right\\} = logit(P(Y \\leq j)) = \\beta_{j0} - \\beta_{1}X_{1} - \\cdots - \\beta_{p}X_{p}\\] where \\(j = 1,...,k-1\\)\nRemark: If \\(k = 2\\), however ordered nature the dependent variable is, the model is the same as binary logistic regression.\n\n\nAssumptions\nThe assumptions of the Ordinal Logistic Regression are as follow:\n\nThe dependent variable is ordered.\nOne or more of the independent variables are either continuous, categorical or ordinal.\nNo zero cell count.\nNo multicollinearity.\nParallel Regression Lines\n\nThe data of the dependent variable must be ordinal in nature to best explain the outcome of the model.\nOrdinal logistic regression works if the independent variable is only continuous, categorical, or ordinal. If there are several independent variable, the model also works if all of it are only continuous, only categorical, or only ordinal.\nThe frequency of each of the category of the dependent variable must not be zero. Furthermore, it is also suggested if each category has equal count since if an event of greater cell with less count occurs,, the less reliable the chi-square test will be.\nThe regression model also assumes that the effect of the independent variable/s is the same for all categories of the dependent variable. This assumption is called the proportional odds, parallel lines, parallel slopes, or parallel regression assumption (Borooah, 2002; Hardin & Hilbe, 2007; Long, 1997), and this will be referred as the parallel lines assumption.\n\n\n\nModel Evaluation and Diagnostics\nAssessing model fit is the process of checking whether the model fits the data sufficiently well. The methods of assessment are the following:\n\nPseudo R-squared\nIn linear regression, the coefficient of determination, \\(R^2\\), measures the variance in the dependent variable explained by the predictors, with higher \\(R^2\\) values (up to a maximum of 1) indicating a better explanatory fit of the model. For regression models with categorical dependent variables, calculating a direct \\(R^2\\) similar to that in linear models is unfeasible. Instead, approximations of \\(R^2\\) are used to estimate the model’s explanatory power, adapting the concept to fit the context of categorical outcomes (IBM, 2023). For this assessment, there are three values of \\(R^2\\), each of which have different formulas. The following are the three different \\(R^2\\):\n\\[R^2_{\\text{McF}} = 1 - \\frac{\\log(L_M)}{\\log(L_0)}\\] \\[R^2_{\\text{CS}} = 1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}\\] \\[R^2_{\\text{Nagelkerke}} = \\frac{1 - \\left(\\frac{L_0}{L_M}\\right)^{2/n}}{1 - L_0^{2/n}}\\] where \\(L_M\\) is the fitted model, \\(L_0\\) is the null model, \\(n\\) is the sample size.\n\n\\(R^2_{\\text{McF}}\\) is McFadden’s version of pseudo \\(R^2\\), based on the log-likelihood kernels for the intercept-only model and the full estimated model (McFadden, 1974).\nCox and Snell’s \\(R^2_{\\text{CS}}\\) is based on the log likelihood for the model compared to the log-likelihood for a baseline model (Cox and Snell 1989).\nNagelkerke’s \\(R^2_{\\text{Nagelkerke}}\\) is an adjusted version of the Cox and Snell \\(R^2\\) that adjusts the scale of the statistic to cover the full range from 0 to 1 (Nagelkerke, 1991).\n\nThe category of the the values of \\(R^2\\) can be summarized as follows:\n\n0 to 0.2: Weak fit\n0.2 to 0.4: Moderate fit\n0.4 to 0.6: Good fit\n0.6 to 0.8: Very good fit\n0.8 to 1: Excellent fit\n\n\n\nLikelihood Ratio Test\nThe likelihood ratio test compares the likelihoods of these two models to determine whether the additional parameters in the full model significantly improve the fit compared to the reduced model. The null hypothesis of the test is the simpler model (reduced model) is sufficient to explain the data. There is no significant improvement in model fit when adding extra parameters. The alternative hypothesis is the more complex model (full model) provides a significantly better fit to the data than the simpler model. This test is calculated as the ratio of the likelihood of the full model to the likelihood of the reduced model. Mathematically, it is represented as: \\[LR = -2 * (\\text{log likelihood of reduced model} - \\text{log likelihood of full model})\\]\n\n\nLipsitz Test\nThe Lipsitz test checks if your model fits the real data well. In other words, it sees if the predictions your model makes is the same about the actual data. The test compares what your model predicts (expected probabilities) with what actually occurs (observed frequencies). If your model is good, the predictions and actual outcomes should be pretty close. The null hypothesis of this test is that the frequencies of the ordinal response variable are consistent with the expected frequencies predicted by the model (Lipsitz, et al., 1996).\n\n\nAccuracy\nAccuracy is one metric for evaluating classification models. It is the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset. Accuracy follows the definition:\n\\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\]\nWhere the number of correct predictions, as the name implies, are the accurate predicted values by the model, and the total number of predictions are the total observations in the validation set.\n\n\nSensitivity and Specificity\nSensitivity is the metric that evaluates a model’s ability to predict true positives of each available category. Specificity is the metric that evaluates a model’s ability to predict true negatives of each available category (Mitrani, A., 2019). The equations below are for calculating sensitivity and specificity:\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\]\nWhere true positives are the number of observations the model predicted were positive that were actually positive. While false negatives are the number of observations the model predicted were positive that were actually negative. Moreover, false negatives, are the number of observations the model predicted were negative that were actually positive. Lastly, true negatives are the number of observations the model predicted were negative that were actually negative.\n\n\nBootstrapping\nBootstrapping is a statistical technique used to estimate the sampling distribution of an estimator by resampling with replacement from the original data. This is often used when the theoretical distribution of an estimator is complex or unknown. This method involves repeatedly drawing samples, typically thousands of times, from the data set and calculating the statistic of interest for each sample. The bootstrap method allows for the estimation of standard errors, confidence intervals, and significance tests, which are critical in many statistical analyses (Efron and Tibshirani, 1994).\nBootstrapping does not rely on the assumptions of normality and can be applied to complex, skewed, or small datasets where other methods might fail or provide biased estimates (Davison and Hinkley, 1997). It offers a straightforward way to derive robust estimates of standard errors and confidence intervals for complex estimators or models without needing explicit formulas.\n\n\n\nImplementation in R\n\nExample 1\nA study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Data on parental educational, a binary variable indicating if at least one parent has attended graduate school and whether the undergraduate institution is public or private, and the current GPA of the student is also collected. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”.\n\nFeature Description\n\napply (Dependent Variable) - The apply variable is an ordered categorical variable with responses to a survey about whether a student feels they are “Unlikely” (1), “Somewhat likely” (2), or “Very likely” (3) to apply to graduate school.\nparental education status variable or pared - The pared variable is a binary variable indicating if at least one parent has attended graduate school. 1 - at least one parent has a graduate degree; 0 otherwise\npublic - The public variable is a binary variable indicating if the undergraduate institution is public (as opposed to private); 1- public; 0-private\ngpa - gpa variable is the student’s grade point average (1-4)\n\nGoal\nTo quantitatively assess the influence of various factors on the likelihood that a student will decide to apply to graduate school(moving from being “unlikely” to “somewhat likely,” or from “somewhat likely” to “very likely”).\nPartitioning\nThe data is split into partition with \\(80\\%\\) falls in the training data, while the remaining \\(20\\%\\) is for the testing data. This partitioning approach ensures that the model is trained on a substantial portion of the data. This allows it to learn the underlying patterns of the data effectively. Meanwhile, the testing data provides an unbiased assessment of the model’s performance on unseen data.\n\ntraining_data &lt;- read.csv(\"training_data.csv\")\ntesting_data &lt;- read.csv(\"testing_data.csv\")\n\nprint(paste0(\"Training Data: \", nrow(training_data), \n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 400; Testing Data: 100\"\n\n\n\nknitr::kable(head(training_data), \n             caption = \"First Six Rows of the Example 1 Data.\")\n\n\nFirst Six Rows of the Example 1 Data.\n\n\napply\npared\npublic\ngpa\n\n\n\n\nvery likely\n0\n0\n3.26\n\n\nsomewhat likely\n1\n0\n3.21\n\n\nunlikely\n1\n1\n3.94\n\n\nsomewhat likely\n0\n0\n2.81\n\n\nsomewhat likely\n0\n0\n2.53\n\n\nunlikely\n0\n1\n2.59\n\n\n\n\n\n\n\nDescriptive Statistics\nDescriptive statistics provides basic information about the features of the data. In this way, it offers a clear overview of the data distribution and central tendencies. This analysis is crucial for identifying trends, spotting anomalies, and laying the groundwork for statistical examinations.\n\n# Checking for Missing values\nsapply(training_data, function(x) sum(is.na(x)))\n\n apply  pared public    gpa \n     0      0      0      0 \n\n\nThe code snippet above sums the number of missing values in each of the variables. The result obtained shows that there are no missing values in any of the four variables. Therefore, there is no need to take any further steps to handle any missing data. Next, the distribution of categorical variables will be examined to determine if there is any imbalance present.\n\nsapply(training_data[, c(\"apply\", \"pared\", \"public\")], table)\n\n$apply\n\nsomewhat likely        unlikely     very likely \n            140             220              40 \n\n$pared\n\n  0   1 \n337  63 \n\n$public\n\n  0   1 \n343  57 \n\n\nThe concept of the code above counts the number of categories in each of the variables. In the apply variable, the unlikely holds the majority of the counts. Furthermore, the distribution of the frequency suggests that they are not equal or approximately equal. A possible consequence of this is that when modeling, the fitted model may not perform well in prediction. For the pared and public variables, there is an imbalance in the values, the majority of the counts are zero. Again, a possible consequence of this phenomenon is that the model may not perform well in forecasting.\n\nftable(xtabs(~ public + apply + pared, data = training_data))\n\n                       pared   0   1\npublic apply                        \n0      somewhat likely        98  26\n       unlikely              175  14\n       very likely            20  10\n1      somewhat likely        12   4\n       unlikely               25   6\n       very likely             7   3\n\n\nIt shows the frequency counts of respondents classified by their likelihood of applying—categorized as “somewhat likely,” “unlikely,” and “very likely”—across combinations of two binary conditions: public (0 or 1) and pared (0 or 1). For instance, under the public = 0 category, 98 respondents are “somewhat likely” to apply when pared is 0, and 26 are “somewhat likely” when pared is 1. The table indicates that the majority of respondents, especially when public is 0, are “unlikely” to apply. It is also noticeable that there is unequal in the frequency in the different categories of apply variable. This phenomenon can significantly impact the accuracy and performance of the model. If certain categories are dominant over the others, it could skew the model’s ability to accurately estimate relationships between less frequent categories. Furthermore, it may not provide enough data to accurately estimate the model.\nChecking outliers in regression analysis is crucial as the presence of it may affect the performance of the model. One of the ways for checking the presence of the event is by performing boxplots. \n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\n\n\n\nBoxplot of public and pared variable\n\n\n\n\nThe boxplots above compares the distributions og gpa based on public and pared variables. From the two boxplots, there is existence of outliers in the data points, this is represented as the points that deviate outside the whiskers. However, there are only small portion of it, hence, the modeling can proceed at ease. While at it, we might as well explain the distribution of the boxplots. The distribution of gpa does not show dramatic differences between the categories within each condition (0 and 1). However, for condition 1—public—students who are “very likely” to apply seem to have a slightly higher median GPA compared to the other categories. For the pared, the distribution of gpa are somewhat consistent across categories, but there is a noticeable shift in medians. For condition 0, students who are “unlikely” to apply tend to have lower GPA medians. For condition 1, students who are “very likely” to apply have noticeably higher GPA.\nThe nature of the data now is transformed into factor to be fitted by ordinal logistic regressions. The code snippet below simply do the virtue of transformation.\n\ntraining_data$apply &lt;- ifelse(training_data$apply == \"unlikely\", 1, \n                       ifelse(training_data$apply == \"somewhat likely\", 2, 3))\ntraining_data$apply &lt;- as.factor(training_data$apply)\ntraining_data$pared &lt;- as.factor(training_data$pared)\ntraining_data$public &lt;- as.factor(training_data$public)\nstr(training_data)\n\n'data.frame':   400 obs. of  4 variables:\n $ apply : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 2 1 2 2 1 2 2 1 2 ...\n $ pared : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 2 ...\n $ public: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n $ gpa   : num  3.26 3.21 3.94 2.81 2.53 ...\n\n\n\n\nChecking Assumptions\nIn the descriptive statistics performed earlier, it is evident that the nature of the dependent variable is ordinal and has three categories in fact(Unlikely, Somewhat Likely, Very likely). The independent variables also are continuous (gpa), categorical(pared and public). There is no zero count phenomenon in each of the category of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -1])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nNo multicollinearity appeared since the data only have one continuous independent variable. Notice that, there is a clear image of disproportions in the frequency of pared and public.\nChecking Parallel Regression Lines\nThe assumption parallel regression lines can be checked using Brant’s test. It is a parallel lines assumption at which the effect of the independent variable is the same for all categories of the dependent variable (Arfan and Sherwani, 2017, p.212). In other words, parallel lines assumption means that the correlation between dependent and independent variable does not change for the categories of dependent variable, and thus, to test the unchangeability of the parameter estimates at cut-off points (Arı and Yıldız, 2014, p.10).\nIf violated, we can still perform the model but be cautious in interpreting the results because the estimated coefficients may not fully capture the relationship between the predictor variables and the ordinal response variable if the assumption of parallel regression lines is violated.\nThe null hypothesis of the test is that the parallel regression assumption holds, while the alternative is it does not hold. The said test can be done in R using the brant() function in package. To perform the test, it requires to fit an ordinal logistic regression model first using the polr() function.\n\n# Modeling\nprop.odds &lt;- polr(apply ~ pared + public + gpa, data = training_data)\n\n# Brant's Test\nbrant(prop.odds)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     4.34    3   0.23\npared1      0.13    1   0.72\npublic1     3.44    1   0.06\ngpa     0.18    1   0.67\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe output of the brant() function contains four columns, particularly, test for the variable, the \\(\\chi^2\\), the df, and the probability or the p-value. Notice that, there are only three independent variables, but an additional variable appeared in the output, the Omnibus. The Omnibus variable is the global assessment of the assumption. The p-values for all variables are greater than 0.05, hence, the proportional odds assumption holds\nNow, the checking of assumptions is done, and none are violated, albeit there is the presence of an imbalance in the frequencies of the categorical variables. Nevertheless, the next thing to perform is to proceed with the modeling part of the ordinal logistic regression in the following subsection.\n\n\nModeling\nTo model ordinal logistic regression in R, the function polr() does the honors as mentioned earlier. However, this time, the Hess parameter is added and set to TRUE to perform the Hessian matrix in the model. The Hessian matrix, also known as the Hessian or the Hessian matrix of second partial derivatives, is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of logistic regression, the Hessian matrix is used to calculate standard errors, test statistics, and confidence intervals for the estimated coefficients (parameters) of the model.\n\nfit &lt;- polr(apply ~ pared + public + gpa, data = training_data, Hess = TRUE)\nsummary(fit)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n    Value   Std. Error t value\n1|2  2.2039  0.7795     2.8272\n2|3  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\nAfter checking the summary of the model using the summary() function, it provides the coefficients, intercepts, residual deviance, and AIC. Notice that in the coefficient and intercepts part, there is no p-value of the output. This is hard to interpret as we cannot determine which of which is statistically significant. Hence, before interpreting, the calculation of the p-value ought to be performed first. The codes below do the virtue of performing what is needed.\n\ncoefs &lt;- coef(summary(fit))\n\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), caption = \"Coefficients and Intercepts\")\n\n\nCoefficients and Intercepts\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n\n\n\n\nNow, from the table above, the estimated model can be written as:\n\\[logit(\\hat{P}(Y \\leq 1) = 2.20 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\] \\[logit(\\hat{P}(Y \\leq 2) = 4.30 - 1.05*\\textbf{pared} - (-0.6)*\\textbf{public} - 0.62*\\textbf{gpa}\\]\nInterpretation\nThe last two rows in the coefficients and intercepts table are the intercepts, or cutpoints, of the Ordinal Logistic Regression. These cutpoints indicate where the latent variable is cut to make the three groups that are observed in the data. The public1 is the only variable that is not statistically significant since its p-value is greater than \\(0.05\\). The rest of the independent variable, and the intercepts are significant at the arbitrary \\(0.05\\) alpha value.\nCoefficients:\npared (Parental Education Status): Holding all other variables constant, if a student’s parent has attended graduate school (pared = 1) rather than not (pared = 0), the log-odds of the student being in a higher category (e.g., from “Unlikely” to “Somewhat likely”, or from “Somewhat likely” to “Very likely”) of applying to graduate school increase by approximately 1.05 units. Students whose parents have higher educational attainments are more likely to pursue and succeed in higher education themselves. This phenomenon is often attributed to the social and cultural capital that educated parents pass on to their children, which influences their educational aspirations and achievements (Perna and Titus, 2005).\npublic (Institution Type): Holding all other variables constant, there is no statistically significant effect of whether the undergraduate institution is public (public = 1) or private (public = 0) on the log-odds of a student being in a higher category of likelihood to apply to graduate school. Research by Bowen and Bok (1998) in their book “The Shape of the River” highlights that the type of undergraduate institution (public vs. private) does not significantly impact the subsequent success in graduate education, suggesting that factors like individual achievement and socioeconomic status might play more significant roles.\ngpa: Holding all other variables constant, for every one-unit increase in GPA, the log-odds of a student being in a higher category of likelihood to apply to graduate school increase by approximately 0.62 units. A study by Ethington and Smart (1986) indicates that GPA is a strong predictor of graduate school enrollment, reflecting academic preparedness and motivation, which are critical in higher education pursuits.\nIntercepts:\nTransition from “Unlikely” to “Somewhat likely”: Holding all other variables constant, the log-odds of a student transitioning from feeling “Unlikely” to “Somewhat likely” to apply to graduate school increase by approximately 2.20 units. This is supported by Tinto’s Theory of Student Departure (1993) which can provide a basis for understanding how certain thresholds or transitions in educational decision-making are influenced by previous educational experiences and integration within the academic system.\nTransition from “Somewhat likely” to “Very likely”: Holding all other variables constant, the log-odds of a student transitioning from feeling “Somewhat likely” to “Very likely” to apply to graduate school increase by approximately 4.30 units. According to Astin’s Theory of Involvement (1984), it argues that the degree of student involvement in academic and extracurricular activities significantly influences their commitment to educational goals, such as applying to graduate school.\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit\")\n\n\nCondidence Interval of Logit\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\npared1\n1.0476901\n0.5281768\n1.5721750\n\n\npublic1\n-0.0587857\n-0.6522060\n0.5191384\n\n\ngpa\n0.6159406\n0.1076202\n1.1309148\n\n\n\n\n\nWhen interpreting the confidence interval of the logit values, if 0 is included in the interval, it implies that the effect of the predictor variables on the outcome is not statistically significant. The true log odds could be negative, positive, or effectively zero, suggesting no effect.\nTable 3 shows the logit estimates and their corresponding \\(95\\%\\) confidence intervals for three predictors in a logistic regression model: pared1, public1, and gpa. The logit estimate for pared1 is \\(1.0477\\) indicating a positive effect on the oucome, with a confidence interval not including zero implying it is statistically significant. Conversely, public1 has a logit estimate of \\(-0.0588\\) with a confidence interval that includes zero, suggesting that this predictor does not have a statistically significant impact on the outcome. Finally, gpa shows a positive logit of \\(0.6159\\) with a confidence interval from \\(0.1076\\) to \\(1.1309\\), also indicating a significant positive effect on the outcome, as the interval does not include zero.\nFor easier comprehension, it is recommended to convert the log of odds into odds ratio. This can be done by taking the exponential to the log odds value. While at it, the \\(95\\%\\) confidence interval is calculated for each coefficient.\n\n\nIf the confidence interval for the odds ratio includes the number 1 then the calculated odds ratio would not be considered statistically significant. This can be seen from the interpretation of the odds ratio. An odds ratio of less than 1 indicates that the odds of the outcome occurring are lower with the presence or increase of the predictor variable. Conversely, an odds ratio greater than 1 suggests that the odds of the outcome occurring are higher with the presence or increase of the predictor variable. An odds ratio of exactly 1 implies that the predictor variable has no effect on the odds of the outcome; in other words, the odds are the same regardless of the presence or level of the predictor variable. Therefore, when the confidence interval for an odds ratio includes the 1, it indicates uncertainty about whether the predictor variable positively or negatively affects the odds of the outcome occurring. This means the true population odds ratio might be greater than, less than, or exactly 1 (Tenny and Hoffman, 2023).\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)),\n             caption = \"Condidence Interval of Odds Ratio\")\n\n\nCondidence Interval of Odds Ratio\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\npared1\n2.8510579\n1.6958376\n4.817114\n\n\npublic1\n0.9429088\n0.5208954\n1.680579\n\n\ngpa\n1.8513972\n1.1136247\n3.098490\n\n\n\n\n\npared1: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times—185% increase— that of students whose parents did not go to college, holding constant all other variables.\npublic: There is no statistically significant difference in the odds of a student being in a higher category of likelihood to apply to graduate school between public and private undergraduate institutions. The odds ratio of 0.94 suggests that the odds are slightly lower for students from public institutions, but the 95% CI includes 1, indicating that the difference is not statistically significant.\ngpa: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nHaving established the parameters of our regression models, we now proceed to assess their fit and robustness. This next section evaluates how well the models conform to the observed data, using a variety of diagnostic statistics and tests to ensure the reliability and validity of our findings.\n\n\nAssessment of Model Fit\nThis section contains a discussion on assessing the model using the different metrics discussed in the model evaluation and diagnostic section. The first metric to perform is the pseudo R-squared. To do it, we must remodel again the ordinal logistic regression using the clm() function as the respective functions of the other metrics do not work in polr().\n\nmodel &lt;- clm(apply ~ pared + public + gpa, data = training_data)\n\nPseudo R-Squared\nThere are three different pseudo R-squared utilized in this study, McFadden, Cox and Snell, and Nagelkerke’s pseudo R-squared. It is time-consuming to calculate each using different functions luckily, the nagelkerke() function performs the three.\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.0326231\n\n\nCox and Snell (ML)\n0.0586601\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n\n\n\n\nTable 5 displays three pseudo \\(R^2\\) values for a logistic regression model: McFadden’s at 0.0326, Cox and Snell’s at 0.05867, and Nagelkerke’s at 0.0695. These metrics assess the goodness of fit of the model, with each indicating a relatively low explanatory power: McFadden’s value suggests that the independent variables explain approximately 3.26% of the variance in the dependent variable. Cox and Snell’s and Nagelkerke’s values are slightly higher, indicating slightly better but still modest explanatory power. Nagelkerke’s value, the highest, suggests that the model explains about 6.96% of the variance.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test, \n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-12.09\n24.18\n2.29e-05\n\n\n\n\n\nTable 6 shows the results of a LRT. The test compares two nested models, with the difference in degrees of freedom (Df.diff) being \\(-3\\), indicating that the full model has three additional parameters compared to the reduced model. The LogLik.diff of \\(-12.09\\) is the difference in the log-likelihoods between the two models, where the full model has a lower log-likelihood. Despite this, the Chi-square value of 24.18 and the very small p-value suggesting that the addition of these three parameters significantly improves the model fit. Therefore, the null hypothesis is rejected which means that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  apply ~ pared + public + gpa\nLR statistic = 8.5407, df = 9, p-value = 0.4807\n\n\nSince the p-value is greater than \\(0.05\\), we do not reject the null hypothesis. The model is adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data\nAccuracy\n\ntesting_data &lt;- read.csv(\"testing_data.csv\")\ntesting_data$apply &lt;- ifelse(testing_data$apply == \"unlikely\", 1, \n                      ifelse(testing_data$apply == \"somewhat likely\", 2, 3))\ntesting_data$apply &lt;- as.factor(testing_data$apply)\ntesting_data$pared &lt;- as.factor(testing_data$pared)\ntesting_data$public &lt;- as.factor(testing_data$public)\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$apply, predicted_data)\n\nThe table presented below is only a portion of the output in the confusionMatrix() function. The full output will be presented in the appendix.\n\nConfusion Matrix and Statistics by Class\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\nPrediction\n1\n2\n3\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nThe model predicted 48 out of 52 actual Class 1 instances correctly, misclassifying 4 as Class 2 and 0 as Class 3. Out of 39 actual Class 2 instances, 11 were correctly predicted, but 28 were incorrectly classified as Class 1, showing a high misclassification rate for Somewhat likely. There were 9 actual Class 3 instances; 2 were correctly predicted, while 7 were misclassified as Class 1, indicating difficulty in correctly classifying this class.\nAccuracy\nThe overall accuracy of the model is 0.59, indicating that 59% of all predictions made by the model are correct. This suggests moderate predictive power.\nSensitivity\nAbout 58% of actual Class 1 instances were identified correctly, suggesting moderate sensitivity for this class. The model correctly identified about 65% of actual Class 2 instances, showing slightly better sensitivity for this class. Not available (NA), likely due to the small number of Class 3 instances present, making it difficult to compute a reliable sensitivity measure.\nSpecificity\nApproximately 76% of instances not belonging to Class 1 were correctly identified, indicating good specificity. About 66% of non-Class 2 instances were correctly identified, showing moderate specificity. The model was very effective in identifying non-Class 3 instances, with a specificity of 91%, suggesting that while it struggles to identify Class 3 correctly, it rarely misclassifies other classes as Class 3.\nNow that the diagnostics for the model are complete, the next step is to remodel the data by removing any non-statistically significant variables. This will help determine if there is an improvement in the model fit. The subsequent section will provide a discussion comparing the differences in coefficients, intercepts, inferences, and model diagnostics.\n\n\nRemoving Insignificant Variable\nThis section contains the remodeled version of the first example with the independent variable removed, the public variable since it is not statistically significant. Furthermore, the same flow is performed—modeling, inference, and assessment of model fit. The full output from R is shown in the Appendix. There will be a selection of which model is best by checking and comparing the value of the remodeled version and the original model in terms of the coefficients, intercepts, inference, and metrics utilized in assessing the model. The Residual Deviance and AIC will also be added to the criteria for choosing which of the two models is better.\n\n\nModeling\n\nComparison of Coefficients and Intercepts\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nValue\nStd. Error\nt value\np value\n\nValue\nStd. Error\nt value\np value\n\n\npared1\n1.0477\n0.2658\n3.9418\n0.0001\n\n1.0457\n0.2658\n3.9418\n0.0001\n\n\npublic1\n-0.0588\n0.2979\n-0.1974\n0.8435\n\n-\n-\n-\n-\n\n\ngpa\n0.6159\n0.2606\n2.3632\n0.0181\n\n0.6042\n0.2539\n2.3794\n0.0173\n\n\n1|2\n2.2039\n0.7795\n2.8272\n0.0047\n\n2.1763\n0.7671\n2.8370\n0.0046\n\n\n2|3\n4.2994\n0.8043\n5.3453\n0.0000\n\n4.2716\n0.7922\n5.3924\n0.0000\n\n\n\nTable 8 displays a comparison of coefficients and intercepts between the original and the remodeled(where the non-significant variable public1 was removed). In the remodeled version, there is a slight decrease in the coefficients for pared1 and gpa, but both maintaining statistical significance with minor adjustments in their standard errors and p-values. The intercepts for the ordinal thresholds 1|2 and 2|3 also slightly decrease but continue to show significances. The removal of public1 seems to be improving the model fit.\nConfidence Interval\n\nComparison of Confidence Interval of Log Odds\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nlogit\n2.5 %\n97.5 %\n\nlogit\n2.5 %\n97.5 %\n\n\npared1\n1.0477\n0.5282\n1.5722\n\n1.0457\n0.5265\n1.570\n\n\npublic1\n-0.0588\n-0.6522\n0.5191\n\n-\n-\n-\n\n\ngpa\n0.6159\n0.1076\n1.1309\n\n0.6042\n0.1090\n1.106\n\n\n\nThe log odds of pared1 remain almost unchanged. The same can be said for its confidence interval. The coefficient for gpa also shows a slight adjustment. The comparison of the two models, albeit with slight changes in the values, is maintaining its significant impact.\n\nComparison of Confidence Interval of Odds Ratio\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nOR\n2.5 %\n97.5 %\n\nOR\n2.5 %\n97.5 %\n\n\npared1\n2.8511\n1.6958\n4.8170\n\n2.8454\n1.6931\n4.8065\n\n\npublic1\n0.9429\n0.5209\n1.6806\n\n-\n-\n-\n\n\ngpa\n1.8514\n1.1136\n3.098\n\n1.8299\n1.1152\n3.0223\n\n\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-Squared\n\nComparison of Pseudo R-Squared\n\n\n\nPseudo R-Squared\n\n\n\n\n\nOriginal\n\nRemodeled\n\n\nMcFadden\n0.0326231\n\n0.0325706\n\n\nCox and Snell (ML)\n0.0586601\n\n0.0585685\n\n\nNagelkerke (Cragg and Uhler)\n0.0695655\n\n0.0694569\n\n\n\nComparing the Pseudo R-squared between the two models, there are changes in the three versions of \\(R^2\\). These changes can be observed in the 4th decimal place, with the remodeled logistic regression slightly decreasing its values. This is the case because the public variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of \\(R^2\\) decreased.\nLikelihood Ratio Test\n\nComparison of Likelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\nOriginal\n-3\n-12.09\n24.18\n2.29e-05\n\n\nRemodeled\n-2\n-12.071\n24.141\n5.7e-06\n\n\n\nOf the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nComparison of Lipsitz Goodness of Fit Test\n\n\n\nLR statistic\ndf\np.value\n\n\n\n\nOriginal\n8.5407\n9\n0.4807\n\n\nRemodeled\n8.5904\n9\n0.4759\n\n\n\nThe two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\n\nComparison of Confusion Matrix\n\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n\nPrediction\n1\n2\n3\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\nOriginal\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\n1\n48\n4\n0\n\n\n\n\n\n\n\n\n\nRemodeled\n2\n28\n11\n0\n0.59\n0.58\n0.65\nNA\n0.76\n0.66\n0.91\n\n\n\n3\n7\n2\n0\n\n\n\n\n\n\n\n\n\n\nThere are no changes in the comparison of the two models in the table above. The two models show the same values in accuracy, sensitivity, and specificity. The next table will show the AIC and Residual Deviance values, where the lower values correspond to the better model.\n\nComparison of Residual Deviance and AIC\n\n\n\nResidual Deviance\nAIC\n\n\n\n\nOriginal\n717.0249\n727.0249\n\n\nRemodeled\n717.0638\n725.0638\n\n\n\nThe Table 15 presents a comparison of the Residual Deviance and Akaike Information Criterion (AIC) between the original and remodeled ordinal logistic regression. The Residual Deviance, which measures the unexplained variance by the model, shows a slight increase with 0.0389, which suggests a nearly identical fit with respect of explaining the variability in the data. However, the AIC, is slightly lower in the remodeled model compared to the original. This reduction indicates that the remodeled model, despite a trivial increase in Residual Deviance, is considered more efficient due to either the parsimony of the model or the trade-off between the model complexity and fit. However, this is not true for all cases, it may because of a chance. To assess if there is significant difference in the AIC of original and remodeled and the Residual deviance, performing bootstrap analysis provides a robust method to estimate the distribution of these differences under the assumption that the sampled data adequately represent the population.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\n\nBootstrap Statistics\n\n\n\noriginal\nbias\nstd. error\n\n\n\n\nt1*\n1.96108366\n-1.243752\n1.737844\n\n\nt2*\n-0.03891634\n-1.243752\n1.737844\n\n\n\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance.\n\\(t1*\\)\nThe original difference in AIC between the original and remodeled suggests that the remodeled might have a slightly lower AIC. The negative bias suggests that the bootstrap samples yielded a smaller difference than this original estimate, implies that the AIC of the remodeled model was not as consistently lower. The standard error is relatively high and that indicates that there is variability in the AIC differences across the bootstrap samples.\n\\(t2*\\)\nThe original difference in residual deviance is nearly zero, hence, this suggests that there is no significant difference in the goodness of fit between the two models based on the original sample. The similar negative bias here as well indicates that the bootstrap samples often show no consistent advantage for either model in terms of fitting the data better. There is no clear difference in model fit between the original and remodeled.\n\nBootstrap Confidence Interval\n\n\n\nLevel\nBasic\n\n\n\n\n\\(t1*\\)\n95 %\n(1.923, 8.295)\n\n\n\\(t2*\\)\n95 %\n(-0.0766, 6.2952)\n\n\n\nThe Basic CI method is non-parametric and does not assume any specific distribution of the bootstrap estimates. For \\(t1*\\), the interval suggests a significant difference where the original model likely has a higher AIC than the remodeled model. For the \\(t2*\\), there is no clear evidence of significant difference in Residual Deviance between the two models since there is zero in the interval.\n\n\nExample 2\nThe Titanic sank on April 15, 1912, during her maiden voyage after colliding with an iceberg. The data can be found on the carData package, TitanicSurviaval, which contains information on the survival status, sex, age, and passenger class of 1309 passengers.\n\nFeature description\n\nsurvived - 1 if yes, 0 if did not survived;\nsex - 1 if male, 0 for female;\nage - in years (and for some children, fractions of a year); and\npassengerClass(Dependent Variable) - class of the passengers, either 1st, 2nd, or 3rd class.\n\nGoal\nTo perform ordinal logistic regression with passenger class as the dependent variable and survival status, sex, and age as independent variables and understand how these factors influenced the socioeconomic status of passengers aboard the Titanic. Specifically, the study aims to statistically quantify the extent to which survival outcomes, gender differences, and age disparities may have been associated with the class of the passengers.\n\nLoading the Dataset\n\ndata2 &lt;- TitanicSurvival\nknitr::kable(head(data2),\n             caption = \"First Six Rows of the Titanic Survival Data\")\n\n\nFirst Six Rows of the Titanic Survival Data\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nAllen, Miss. Elisabeth Walton\nyes\nfemale\n29.0000\n1st\n\n\nAllison, Master. Hudson Trevor\nyes\nmale\n0.9167\n1st\n\n\nAllison, Miss. Helen Loraine\nno\nfemale\n2.0000\n1st\n\n\nAllison, Mr. Hudson Joshua Crei\nno\nmale\n30.0000\n1st\n\n\nAllison, Mrs. Hudson J C (Bessi\nno\nfemale\n25.0000\n1st\n\n\nAnderson, Mr. Harry\nyes\nmale\n48.0000\n1st\n\n\n\n\n\n\n\nExploratory Data Analysis\n\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0            263              0 \n\n\nThe output of the code above contains 263 in the age which suggests that there are 263 passengers with their age not written in the data. Imputation can fill these missing values in the data, but this paper will only be limited to removing the missing values.\n\ndata2 &lt;- na.omit(data2)\nsapply(data2, function(x) sum(is.na(x)))\n\n      survived            sex            age passengerClass \n             0              0              0              0 \n\n\nDescriptive Statistics\n\nsapply(data2[, c(\"survived\", \"sex\", \"passengerClass\")], table)\n\n$survived\n\n no yes \n619 427 \n\n$sex\n\nfemale   male \n   388    658 \n\n$passengerClass\n\n1st 2nd 3rd \n284 261 501 \n\n\nThe output shown provides the frequency of each of the categorical variable. Of the total passengers, 619 did not survive while 427 survived, highlighting the tragedy’s high fatality rate. Regarding gender distribution, there were significantly more males (658) than females (388) on board. In terms of passenger class, a majority were in third class (501), followed by first (284) and second class (261), reflecting the socio-economic diversity of the passengers.\n\nftable(xtabs(~ survived + passengerClass + sex, data = data2))\n\n                        sex female male\nsurvived passengerClass                \nno       1st                     5   98\n         2nd                    11  135\n         3rd                    80  290\nyes      1st                   128   53\n         2nd                    92   23\n         3rd                    72   59\n\n\nThe contingency table illustrates the distribution of Titanic passengers across survival status, passenger class, and sex. For the passengers who did not survived, majority of it were of the males in 3rd class around 290 out of 370 male non-survivors. In contrast, females in 1st class had the highest survival rates, with 128 out of 133 female 1st class passengers surviving. It is worth noticing that are cell with low frequency in different class.\n\n\nThe class of the categorical variables of the dataset is changed into factor as this will be necessary for the modeling part.\n\ndata2$passengerClass &lt;- as.factor(data2$passengerClass)\ndata2$survived &lt;- as.factor(data2$survived)\ndata2$sex &lt;- as.factor(data2$sex)\n\nThe boxplot is performed to investigate the presence of outliers in the dataset.\n\n\n\n\n\nBoxplot of survived variable\n\n\n\n\nFigure 3 presents boxplots depicting the age distribution of Titanic passengers across different classes (1st, 2nd, 3rd), split by their survival status (yes, no). In both survival categories, first-class passengers tend to be older compared to those in second and third classes. The age ranges in first class also appear wider, particularly among survivors. Second and third class passengers show younger median ages, with tighter interquartile ranges, especially noticeable in third class. Across all classes, survivors tend to have slightly higher median ages than those who did not survive, suggesting that age may have played a role in survival, particularly in lower classes. The presence of outliers across all groups indicates variability in age among passengers within each class and survival category.\nPartitioning\nA partition of \\(80\\%\\) of the data will be in the training data, and the remaining \\(20\\%\\) is the testing data. The code below performs a stratified partitioning in the titanic dataset.\n\nset.seed(2024)\nsplit &lt;- initial_split(data2, prop = 0.80, strata = \"passengerClass\")\n\n# Create training and testing sets using the index\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nprint(paste0(\"Training Data: \", nrow(training_data),\n             \"; Testing Data: \", nrow(testing_data)))\n\n[1] \"Training Data: 835; Testing Data: 211\"\n\n\n\n\nChecking Assumptions\nThe descriptive statistics performed earlier shows evidence that the dependent variable is ordinal in nature. No zero count was also found in each of the categories of the dependent variable.\nChecking the Presence of Multicollinearity\n\nggpairs(training_data[, -4])\n\n\n\n\nCorrelation Assumption\n\n\n\n\nChecking Parallel Regression Lines\n\npar.reg &lt;- polr(passengerClass ~ survived + sex + age, data = training_data)\n\n# Brant's Test\nbrant(par.reg)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     2.89    3   0.41\nsurvivedyes 1.79    1   0.18\nsexmale     0.57    1   0.45\nage     1.79    1   0.18\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThe overall test and the results for individual predictors indicate that all variables satisfy the assumption since the p-value is greater than \\(0.05\\).\nNow the checking of assumptions is done and none are violated, next thing to perform is to proceed in the modeling part of the ordinal logistic regression.\n\n\nModeling\n\nfit &lt;- polr(passengerClass ~ survived + sex + age, data = training_data, Hess=TRUE)\nsummary(fit)\n\nCall:\npolr(formula = passengerClass ~ survived + sex + age, data = training_data, \n    Hess = TRUE)\n\nCoefficients:\n               Value Std. Error t value\nsurvivedyes -1.75059   0.183540  -9.538\nsexmale     -0.27452   0.180928  -1.517\nage         -0.06574   0.005419 -12.131\n\nIntercepts:\n        Value    Std. Error t value \n1st|2nd  -4.0817   0.2763   -14.7740\n2nd|3rd  -2.6878   0.2532   -10.6166\n\nResidual Deviance: 1495.276 \nAIC: 1505.276 \n\n\n\ncoefs &lt;- coef(summary(fit))\n# Calculate P-value\np &lt;- pnorm(abs(coefs[, \"t value\"]), lower.tail = FALSE) * 2\n# Add Column\ncoefs &lt;- cbind(coefs, \"p value\" = p) \nknitr::kable(round(coefs, 4), \n             caption = \"Coefficient and Intercepts of Titanic Survival Data\")\n\n\nCoefficient and Intercepts of Titanic Survival Data\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nsurvivedyes\n-1.7506\n0.1835\n-9.5379\n0.0000\n\n\nsexmale\n-0.2745\n0.1809\n-1.5173\n0.1292\n\n\nage\n-0.0657\n0.0054\n-12.1308\n0.0000\n\n\n1st|2nd\n-4.0817\n0.2763\n-14.7740\n0.0000\n\n\n2nd|3rd\n-2.6878\n0.2532\n-10.6166\n0.0000\n\n\n\n\n\nThe estimated model can be written as: \\[logit(\\hat{P}(Y \\leq 1) = -4.0817 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\] \\[logit(\\hat{P}(Y \\leq 2) = -2.6878 - (-1.7506)*\\textbf{survivedyes} - (-0.2745)*\\textbf{sexmale} - ( -0.0657)*\\textbf{age}\\]\nInterpretation\nCoefficients:\nsurvivedyes: For passenger who survived, the log odds of being in a lower passenger class decrease by approximately 1.7506 units while holding the other variables constant. This implies that among the survivors, there’s a significant decrease in the log odds of being in a lower passenger class than that off being in a higher passenger class. Historical analyses indicate that survival rates on the Titanic were markedly higher for first-class passengers compared to those in lower classes, often attributed to closer proximity to lifeboats and prioritization in lifeboat boarding protocols (Frey, et al. 2010).\nsexmale: For male passengers, the log odds of being in a lower passenger class decrease by approximately 0.2745 units compared to the log odds of being in a higher passenger class when other variables are held constant. This suggests that among male passengers, there’s a decrease in the log odds of being in a lower passenger class relative to being in a higher passenger class. This finding contradicts with the historical accounts that Hall (2014) documented. He noted that a significant survival advantage for women during the Titanic disaster attributed to the ‘women and children first’ policy. The lack of significance in this model could be attributed to the specific data or the influence of other variables within the model.\nage: For every one unit increase in age, the log odds of being in a lower passenger class decrease by approximately 0.0657 units when rendering the other variables as constant. The older the passenger are the less likely they are in lower passenger class. According to Spigner (2012) that age played a significant role in survival probabilities on the Titanic, with children and younger women more likely to survive, reflecting societal norms and rescue priorities.\nIntercepts:\n1st|2nd: The intercept value for the transition from 1st to 2nd class is -4.0817 when the other variables are zero. In other words, passengers are much less likely to be in the 1st class compared to the 2nd class. By Archibald and Sloan (2011), the substantial social and economic differences between the first and second classes on the Titanic are well-documented, with first-class passengers enjoying considerably more luxury and privileges, which could translate into a higher likelihood of being in a higher class.\n2nd|3rd: The intercept value for the transition from 2nd to 3rd class is -2.6878 when the other variables are held constant. In other words, passengers are much less likely to be in the 2nd class compared to the 3rd class. The differences between second and third classes were significant, with third-class passengers often experiencing much poorer living conditions and having less access to safety measures during the disaster (Beesley, 2011).\nConfidence Interval\n\nci &lt;- confint(fit, level = 0.95)\nknitr::kable(cbind(logit = coef(fit), ci), \n             caption = \"Condidence Interval of Logit of Titanic Survival Data\")\n\n\nCondidence Interval of Logit of Titanic Survival Data\n\n\n\nlogit\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n-1.750590\n-2.1154980\n-1.3953066\n\n\nsexmale\n-0.274523\n-0.6328328\n0.0771671\n\n\nage\n-0.065737\n-0.0765378\n-0.0552818\n\n\n\n\n\nTable 20 displays the confidence intervals of the logit coefficients for three independent variables. For ‘survivedyes,’ the logit coefficient is -1.750590, and the confidence interval spans from -2.1154980 to -1.3953066. This interval does not include zero, indicating a statistically significant negative relationship between survival and passenger class. For ‘sexmale,’ the coefficient is -0.274523 with a confidence interval ranging from -0.6328328 to 0.0771671. This interval crosses zero, suggesting that the effect of being male on passenger class may not be statistically significant. Lastly, for ‘age’ the coefficient is -0.065737 with a confidence interval from -0.0765378 to -0.0552818, which also does not include zero, indicating a significant negative effect where older passengers are less likely to be in higher classes.\n\nknitr::kable(exp(cbind(OR = coef(fit), ci)), \n             caption = \"Condidence Interval of Odds Ratio of Titanic Survival Data\")\n\n\nCondidence Interval of Odds Ratio of Titanic Survival Data\n\n\n\nOR\n2.5 %\n97.5 %\n\n\n\n\nsurvivedyes\n0.1736715\n0.1205732\n0.2477571\n\n\nsexmale\n0.7599345\n0.5310852\n1.0802226\n\n\nage\n0.9363771\n0.9263179\n0.9462185\n\n\n\n\n\nsurvivedyes: The odds ratio of 0.1737 indicates that passengers who survived are significantly less likely to belong to a higher passenger class. Specifically, survivors are approximately 82.63% less likely to be in a higher class than those who did not survive, as the odds ratio is less than 1. The 95% confidence interval ranging from 0.12057 to 0.2478 reinforces the statistical significance of this finding, confirming that this is a robust effect.\nsexmale: The odds ratio for males is 0.7599, suggesting that males are less likely to be in a higher passenger class compared to females. However, the confidence interval for this estimate ranges from 0.5311 to 1.0802, which includes 1, indicating that this result is not statistically significant. Thus, sex may not be a strong predictor of passenger class on the Titanic.\nage: The odds ratio for age is 0.9364, implying that for every additional year of age, the likelihood of being in a higher passenger class decreases by about 6.36%. This effect is statistically significant, as the confidence interval (0.9263 to 0.9462) does not include 1. This suggests a consistent trend where older passengers were less likely to be in higher classes.\n\n\nAssessing of Model Fit\nThis section provides a discussion on assessing the model using the same metrics utilized in the first example.\n\nmodel &lt;- clm(passengerClass ~ survived + sex + age, data = training_data)\n\nPseudo R-squared\n\nknitr::kable(nagelkerke(model)$Pseudo.R.squared.for.model.vs.null, \n             caption = \"Pseudo R-Squared\")\n\n\nPseudo R-Squared\n\n\n\nPseudo.R.squared\n\n\n\n\nMcFadden\n0.149588\n\n\nCox and Snell (ML)\n0.270207\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n\n\n\n\nThe table above shows the same three versions of different \\(R^2\\). McFadden’s R-squared at 0.149588 suggests a modest explanatory power. Cox and Snell’s and Nagelkerke’s values, at 0.270207 and 0.307667 respectively, provide higher estimates, suggesting a better model fit.\nLikelihood Ratio Test\n\nknitr::kable(nagelkerke(model)$Likelihood.ratio.test,\n             caption = \"Likelihood Ratio Test\")\n\n\nLikelihood Ratio Test\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\n\n-3\n-131.51\n263.02\n0\n\n\n\n\n\nSince the given p-value is 0 which is clearly less than 0.05, hence, the null hypothesis is rejected, that adding the predictors is better than the null model with no predictors at all.\nLipsitz Test\n\nlipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  passengerClass ~ survived + sex + age\nLR statistic = 16.42, df = 9, p-value = 0.05861\n\n\nSince the p-value is greater than 0.05, we do not reject the null hypothesis. The model is adequately fitting the ordinal data.\nAccuracy\n\npredicted_data &lt;- predict(fit, testing_data, type = \"class\")\nconfusionMatrix(testing_data$passengerClass, predicted_data)\n\n\nConfusion Matrix and Statistics by Class of Titanic Survival Data\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\nPrediction\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nFor the 1st class, the model correctly predicted 41 passengers as 1st class when the actual class of the passengers are 1st class. Moreover, there are 16 incorrectly predicted as 3rd class. For the 2nd class, the model correctly predicted only 1 passenger as being in the 2nd class. It misclassified 13 passengers who were actually in 1st class as being in 2nd class, and 39 passengers who were in 2nd class were mistakenly classified as being in 3rd class. This shows a significant misclassification error for 2nd class passengers. Lastly, for the 3rd class, the model correctly predicted 86 passengers as the actual third class, while misclassifying the 15 passengers as the 1st class. The misclassification error in the prediction for 2nd class may result in significant loss of accuracy in the prediction power of the model. The same can be applied for the incorrect prediction for the other classes.\nAccuracy\nThe overall model accuracy is 0.61. The accuracy is low because of the misclassification resulted in the confusion matrix. One of the possible contribution for this is that due to the disproportionate number for each category of the dependent variable as what is performed in the descriptive statistic earlier.\nSensitivity\nFor the first class, the sensitivity is 0.59, meaning the model correctly identified 59% of all actual 1st class passengers as 1st class. The sensitivity in class 2 is 1.0 implies the model perfectly identified all passengers who were actually in 2nd class, although from the confusion matrix, it appears there was an issue with only 1 passenger correctly identified. Lastly, the sensitivity is 0.61 for the third class indicating that 61% of actual 3rd class passengers were correctly predicted as 3rd class by the model.\nSpecificity\nThe specificity is 0.89 for the 1st class, which means the model correctly identified 89% of passengers who were not in 1st class.For the 2nd class, the specificity is 0.75, indicating that 75% of the passengers not belonging to 2nd class were accurately identified as not being 2nd class. Lastly, for the 3rd class, with a specificity of 0.79, the model correctly identified 79% of the non-3rd class passengers.\nNow that the model diagnostics are finished, the subsequent section involves refining the model by eliminating variables that are not statistically significant. This step will help determine if the model’s fit has improved. Additionally, the upcoming section will compare the changes in coefficients, intercepts, interpretations, and model diagnostics.\n\n\nRemoving Insignificant Variable\nThis section contains the comparison of the original ordinal logistic model of the TitanicSurvival data set with the remodeled version where the sex variable is removed. The full output of the modelling, inference, and assessment of the model of the remodeled version can be seen in the appendix.\n\n\nModeling\n\nComparison of Coefficients and Intercepts\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nValue\nStd. Error\nt value\np value\n\nValue\nStd. Error\nt value\np value\n\n\nsurvivedyes\n-1.751\n0.184\n-9.538\n0\n\n-1.592\n0.149\n-10.683\n0\n\n\nsexmale\n-0.275\n0.181\n-1.517\n0.129\n\n-\n-\n-\n-\n\n\nage\n-0.066\n0.005\n-12.131\n0\n\n-0.066\n0.005\n-12.154\n0\n\n\n1st|2nd\n-4.082\n0.276\n-14.774\n0\n\n-3.848\n0.226\n-17.011\n0\n\n\n2nd|3rd\n-2.688\n0.253\n-10.617\n0\n\n-2.456\n0.198\n-12.377\n0\n\n\n\nTable 25 compares coefficients from original and remodeled ordinal logistic regression models. In the remodeled model, ‘sexmale’ is removed due to its non-significant p-value. In the remodeled version, the coefficients slightly decreased, but continued to show significance. The intercepts for class transitions ‘1st|2nd’ and ‘2nd|3rd’ decreased, indicating clearer distinctions between classes in the remodeled model, which altogether suggests an enhanced model efficiency and interpretative clarity by excluding ‘sexmale’.\nConfidence Interval\n\nComparison of Confidence Interval of Logit\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nlogit\n2.5 %\n97.5 %\n\nlogit\n2.5 %\n97.5 %\n\n\nsurvivedyes\n-1.7506\n-2.11550\n-1.3953\n\n-1.5917\n-1.8866\n-1.3022\n\n\nsexmale\n-0.2745\n-0.6328\n0.0772\n\n-\n-\n-\n\n\nage\n-0.0657\n-0.0765\n-0.0553\n\n-0.0659\n-0.0767\n-0.0554\n\n\n\nThe log odds of the independent variables shows a decreased in value in the remodeled version as illustrated in table 23. There is no zero included in the confidence interval asserting statistically significance in the remodeled version. The comparison of the two models, albeit with slight changes in the values maintained its significance.\n\nComparison of Confidence Interval of Odds Ratio\n\n\n\nOriginal\n\nRemodeled\n\n\n\n\n\nOR\n2.5 %\n97.5 %\n\nOR\n2.5 %\n97.5 %\n\n\nsurvivedyes\n0.1737\n0.1206\n0.2478\n\n0.2036\n0.1516\n0.2719\n\n\nsexmale\n0.7599\n0.5311\n1.0802\n\n-\n-\n-\n\n\nage\n0.9364\n0.9263\n0.9462\n\n0.9362\n0.9262\n0.9461\n\n\n\nThe comparison of the odds ratio and confidence interval shows a slight change in the values between the two models. The two models maintained a significant impact.\nPseudo R-squared\n\nComparison of Pseudo R-squared\n\n\n\nPseudo R-Squared\n\n\n\n\n\nOriginal\n\nRemodeled\n\n\nMcFadden\n0.149588\n\n0.148262\n\n\nCox and Snell (ML)\n0.270207\n\n0.268165\n\n\nNagelkerke (Cragg and Uhler)\n0.307667\n\n0.305342\n\n\n\nComparing the Pseudo R-squared between the two models, a slight changes occured, particularly slight decrease can be seen in the remodeled version This is the case because the sex variable has an association with the dependent variable, hence, when it is removed in the remodeled version, the values of R2 decreased.\nLikelihood Ratio Test\n\nComparison on LRT\n\n\n\nDf.diff\nLogLik.diff\nChisq\np.value\n\n\n\n\nOriginal\n-3\n-131.51\n263.02\n0\n\n\nRemodeled\n-2\n-130.34\n260.69\n0\n\n\n\nThere is a decrease in the log likelihood difference, and the \\(\\chi^2\\) in the remodeled version since the sex variable is removed. Nevertheless, of the two LRTs, the null hypothesis is rejected, suggesting that adding the predictors is better than the null model with no predictors.\nLipsitz Test\n\nComparison on Lipsitz Test\n\n\n\nLR Statistic\ndf\np value\n\n\n\n\nOriginal\n16.42\n9\n0.05861\n\n\nRemodeled\n13.064\n9\n0.1597\n\n\n\nThere is an increase in the p-value in the remodeled version. Even so, the two models are adequately fitting the ordinal data. The predictions of the model are the same or closely the same as the actual data.\nAccuracy\n\nComparison of Confusion Matrix\n\n\n\n\nReference\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\n\nPrediction\n1st\n2nd\n3rd\n\nC1\nC2\nC3\nC1\nC2\nC3\n\n\n\n1st\n41\n0\n16\n\n\n\n\n\n\n\n\n\nOriginal\n2nd\n13\n1\n39\n0.61\n0.59\n1\n0.61\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\n1st\n42\n0\n15\n\n\n\n\n\n\n\n\n\nRemodeled\n2nd\n16\n0\n37\n0.59\n0.58\nNA\n0.62\n0.89\n0.75\n0.79\n\n\n\n3rd\n15\n0\n86\n\n\n\n\n\n\n\n\n\n\nThere are changes in the values in the confusion matrices, wherein the remodeled version fails to correctly predict the true positive of the 2nd class. This led to a decrease in the accuracy of the model from 0.61 to 0.59. Furthermore, this resulted in changes in Sensitivity.\n\nComparison on Residual Deviance and AIC\n\n\n\nResidual Deviance\nAIC\n\n\n\n\nOriginal\n1495.276\n1505.276\n\n\nRemodeled\n1497.609\n1505.609\n\n\n\nTable 32 compares the Residual Deviance and AIC between the original and remodeled ordinal logistic regression models. The Residual Deviance shows a slight increase from 1495.276 in the original model to 1497.609 in the remodeled version, indicating a marginal decrease in model fit as it slightly fails to capture the data variability as effectively as the original. The AIC remains nearly unchanged, shifting from 1505.276 to 1505.609. These metrics indicate that the removal of the predictor has not significantly improved the overall efficiency and effectiveness of the model in explaining the variability in the data. Performing bootstrap analysis assesses if there is significant difference in the AIC of original and remodeled and the Residual deviance.\nBootstrapping\nThe bootstrap process involves resampling the original dataset 1000 times and for each sample. In calculating the difference between AIC and Residual Deviance, the statistic of the original is subtracted from the remodeled. The code of bootstrapping can be seen in the Appendix.\n\nBootstrap Statistics\n\n\n\noriginal\nbias\nstd. error\n\n\n\n\nt1*\n-0.3329053\n-1.09505\n3.537359\n\n\nt2*\n-2.3329053\n-1.09505\n3.537359\n\n\n\nThe table above is the bootstrap analysis performed in R, where the \\(t1*\\) represents he difference in AIC, the \\(t2*\\) is for the difference in Residual Deviance. The negative bias in both the AIC and Residual Deviance differences suggests that the bootstrap replications tend to produce smaller differences than those observed in the original data. The standard errors imply that there is a significant spread in the estimates of these differences across the bootstrap sample.\nRemoving non-statistically significant variables from an ordinal logistic regression model, can sometimes lead to a worsening of the model’s performance. This phenomenon may seem counterintuitive since it is generally recommended to simplify models by eliminating variables that do not contribute significant improvements. According to Agresti (2010), he discussed that excluding variables may not show immediate statistical significance but could still be influential. This was supported by Harrel (2001), noting that non-significant variables might still play crucial roles in the context of confounding or interacting effects. In the study of Hosmer, et al. (2013) about “Applied Logistic Regression”, they give caution against the indiscriminate removal of variables based solely on their p-values without considering their roles in the model’s architecture.\n\nBootstrap Confidence Interval\n\n\n\nLevel\nBasic\n\n\n\n\n\\(t1*\\)\n95 %\n(-2.6513, 10.0923)\n\n\n\\(t2*\\)\n95 %\n(-4.651, 8.092)\n\n\n\nThe inclusion of zero in the confidence intervals suggests that the differences in AIC and Residual Deviance are not statistically significant. This means that, with respect to these metrics, the remodeled model does not differ significantly from the original model. Removing variables did not significantly improve or worsen the model’s fit.\n\n\n\nConclusion and Recommendation\nThis report utilized ordinal logistic regression to examine two datasets: the decision-making process regarding graduate school applications among college juniors and the socio-economic factors affecting survival on the Titanic. Through model evaluations and diagnostics, including Pseudo R-squared values, Likelihood Ratio Tests, and other fit assessments, valuable insights were gathered into the influence of various predictors on ordered categorical outcomes.\nFor the graduate school application study, results underscored the significant role parental education and GPA play in influencing students’ likelihood of applying to graduate school. The study emphasized how these factors quantitatively affect students’ decision-making processes across different likelihood categories.\nIn the Titanic dataset analysis, the findings showed clear socio-economic divides in survival rates. It demonstrates that passenger class and age significantly influenced survival likelihood. Key findings indicated that survivors were more likely to be from higher social classes, highlighting the social stratification’s impact on survival probabilities. Moreover, older passengers were less likely to be in higher classes, potentially influencing their survival chances. This analysis not only provided statistical backing to historical accounts but also offered a deeper understanding of how these factors interacted under extreme circumstances.\nBased on the findings from these analyses, the following recommendations are proposed:\n\nThe additional of variables that relates well to the dependent variable that could affect the decision-making process for potential graduate students. Examples would be psychological factors or financial considerations. For historical datasets like the Titanic, extending the analysis to include crew data and comparing it with other maritime disasters could provide broader insights.\nIt is recommended to refine data handling and model fitting techniques, such as addressing any imbalance in class distributions within datasets or employing more sophisticated methods for handling missing data and outliers to improve model accuracy and reliability.\n\n\n\nReferences\nAgresti, Alan. (2010). “Analysis of Ordinal Categorical Data”. Wiley Series in Probability and Statistics.\nArchibald, T., & Sloan, J. (2011). Titanic: The Real Story of the Construction of the World’s Most Famous Ship. Channel 4 Books.\nArfan, M., & Sherwani, R. (2017, January 1). Ordinal Logit and Multilevel Ordinal Logit Models: An Application on Wealth Index MICS-Survey Data. Pakistan Journal of Statistics & Operation Research, 13(1), 211-226. https://doi.org/10.18187/pjsor.v13i1.1801\nArı, E., & Yıldız, Z. (2014). Parallel Lines Assumption in Ordinal Logistic Regression And Analysis Approaches. International Interdisciplinary Journal of Scientific Research, 1(3), 8-23.\nAstin, A. W. (1984). Student involvement: A developmental theory for higher education. Journal of College Student Development, 25(4), 297-308.\nBeesley, L. (2011). The Loss of the SS. Titanic: Its Story and Its Lessons. Hesperides Press.\nBrant, R. (1990). Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression. Biometrics, 46(4), 1171–1178. https://doi.org/10.2307/2532457\nBorooah, V. K. (2002). Logit and probit: Ordered and multinomial models. Thousand Oaks, CA: Sage.\nBowen, W. G., & Bok, D. (1998). The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions. Princeton University Press.\nCox, D. R., and E. J. Snell. 1989. The Analysis of Binary Data, 2nd ed. London: Chapman and Hall.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEthington, C. A., & Smart, J. C. (1986). Persistence to graduate education. Research in Higher Education, 24(3), 287-303.\nFrey, B. S., Savage, D. A., & Torgler, B. (2010). Behavior under extreme conditions: The Titanic disaster. Journal of Economic Perspectives, 25(1), 209-222.\nHall, W. (2014). Titanic: The Unfolding Story as Told by the Daily Mirror. Pavilion Books.\nHardin, J., & Hilbe, J. (2007). Generalized linear models and extensions (2nd ed.). College Station, TX: Stata Press.\nHarrell, Frank E. Jr. (2001). “Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis”. Springer Series in Statistics.\nHosmer, D.W., Lemeshow, S., & Sturdivant, R.X. (2013). “Applied Logistic Regression”. Wiley Series in Probability and Statistics.\nIBM. (2023, September 19). https://www.ibm.com/docs/en/spss-statistics/saas?topic=model-pseudo-r-square\nStuart R. Lipsitz & Garrett M. Fitzmaurice & Geert Molenberghs, 1996. “Goodness‐Of‐Fit Tests for Ordinal Response Regression Models,” Journal of the Royal Statistical Society Series C, Royal Statistical Society, vol. 45(2), pages 175-190, June.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Thousand Oaks, CA: Sage.\nMcCullagh, P. (1980). Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952\nMcCullagh, P., & Nelder, J.A. (1989). Generalized linear models (2nd ed.). London: Chapman & Hall.\nMcFadden, D. 1974. Conditional logit analysis of qualitative choice behavior. In: Frontiers in Economics, P. Zarembka, eds. New York: Academic Press.\nMcNulty K. Handbook of Regression Modeling in People Analytics: With Examples in R and Python. 1st edition. Chapman and Hall/CRC; 2021.\nMitrani, A., (2019, December 6). Evaluating Categorical Models II: Sensitivity and Specificity. Towards Data Science. https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8#:~:text=Sensitivity %20is%20the%20metric%20that,negatives%20of%20each%20available%20category\nNagelkerke, N. J. D. 1991. A note on the general definition of the coefficient of determination. Biometrika, 78:3, 691-692.\nPerna, L. W., & Titus, M. A. (2005). The relationship between parental involvement as social capital and college enrollment: An examination of racial/ethnic group differences. Journal of Higher Education, 76(5), 485-518.\nSpigner, C. (2012). Age, social class and gender on the Titanic. Disaster Prevention and Management.\nTenny S, Hoffman MR. Odds Ratio. [Updated 2023 May 22]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2024 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK431098/#\nTinto, V. (1993). Leaving College: Rethinking the Causes and Cures of Student Attrition. University of Chicago Press."
  }
]